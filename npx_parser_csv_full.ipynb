{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff6296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import lxml.etree as ET\n",
    "\n",
    "# We'll define some global lists (or you could use data class structures).\n",
    "# Each will store rows as dictionaries, which we'll later convert to DataFrame -> CSV.\n",
    "\n",
    "FORM_NPX_ROWS = []\n",
    "INSTITUTIONAL_MANAGER_ROWS = []\n",
    "SERIES_ROWS = []\n",
    "PROXY_VOTING_RECORD_ROWS = []\n",
    "MATTER_CATEGORY_ROWS = []\n",
    "PROXY_VOTING_RECORD_CATEGORY_ROWS = []\n",
    "VOTING_RECORD_MANAGER_ROWS = []\n",
    "VOTING_RECORD_SERIES_ROWS = []\n",
    "\n",
    "# We'll keep track of known categories so we don't insert duplicates in matter_category\n",
    "KNOWN_CATEGORIES = {}\n",
    "\n",
    "# Utility: Function to safely parse date from text like \"06/30/2024\"\n",
    "def parse_date(date_string):\n",
    "    if not date_string:\n",
    "        return None\n",
    "    # We'll try mm/dd/yyyy or yyyy-mm-dd or other variations. \n",
    "    # You can tailor to your typical data format.\n",
    "    patterns = [\"%m/%d/%Y\", \"%Y-%m-%d\", \"%m-%d-%Y\"]\n",
    "    for fmt in patterns:\n",
    "        try:\n",
    "            return datetime.datetime.strptime(date_string.strip(), fmt).date()\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None  # If no parse worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56641fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_blocks(file_path):\n",
    "    \"\"\"\n",
    "    Reads an entire N-PX text file and extracts any <XML>...</XML> blocks.\n",
    "    Returns a list of raw XML strings. \n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Find all <XML> ... </XML> sections (case-insensitive).\n",
    "    pattern = re.compile(r\"<XML>(.*?)</XML>\", re.IGNORECASE | re.DOTALL)\n",
    "    return pattern.findall(text)\n",
    "\n",
    "\n",
    "def parse_xml_fragment(xml_string):\n",
    "    \"\"\"\n",
    "    Attempt to parse an individual XML fragment with lxml in recovery mode.\n",
    "    Return the root element or None if parse fails badly.\n",
    "    \"\"\"\n",
    "    parser = ET.XMLParser(recover=True, encoding=\"utf-8\")\n",
    "    try:\n",
    "        root = ET.fromstring(xml_string.encode(\"utf-8\"), parser=parser)\n",
    "        return root\n",
    "    except ET.XMLSyntaxError as e:\n",
    "        print(f\"  [Warning] parse error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_text(node, xpath_expr):\n",
    "    \"\"\"\n",
    "    Shortcut: runs an XPath for the first match of xpath_expr relative to `node`,\n",
    "    returns .text stripped or \"\" if none found.\n",
    "    \"\"\"\n",
    "    r = node.xpath(xpath_expr)\n",
    "    if r and r[0] is not None and r[0].text:\n",
    "        return r[0].text.strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_bool(node, xpath_expr, default=False):\n",
    "    \"\"\"\n",
    "    Some elements may be \"Y\"/\"N\" or \"true\"/\"false\". We'll standardize to bool.\n",
    "    \"\"\"\n",
    "    txt = get_text(node, xpath_expr).upper()\n",
    "    if txt in [\"Y\", \"TRUE\", \"YES\"]:\n",
    "        return True\n",
    "    elif txt in [\"N\", \"FALSE\", \"NO\"]:\n",
    "        return False\n",
    "    return default\n",
    "\n",
    "\n",
    "def get_decimal(node, xpath_expr):\n",
    "    \"\"\"\n",
    "    Return a decimal (float) from the first match, or None if invalid/empty.\n",
    "    \"\"\"\n",
    "    txt = get_text(node, xpath_expr)\n",
    "    if not txt:\n",
    "        return None\n",
    "    try:\n",
    "        return float(txt.replace(\",\", \"\"))\n",
    "    except ValueError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848f2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edgar_submission(root):\n",
    "    \"\"\"\n",
    "    Attempt to extract top-level data for form_npx table from <edgarSubmission>.\n",
    "\n",
    "    We assume structure like:\n",
    "       <edgarSubmission>\n",
    "         <headerData>\n",
    "           <submissionType>...</submissionType>\n",
    "           ...\n",
    "         </headerData>\n",
    "         <filerInfo>\n",
    "           <registrantType>...</registrantType> etc.\n",
    "         </filerInfo>\n",
    "         <formData>\n",
    "           <coverPage>...</coverPage>\n",
    "           <signaturePage>...</signaturePage>\n",
    "           <summaryPage>...</summaryPage>\n",
    "         </formData>\n",
    "       </edgarSubmission>\n",
    "\n",
    "    Return a dict with columns matching 'form_npx'.\n",
    "    If data is missing or doesn't parse, return partial data or an empty dict.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # 1) submissionType => form_type\n",
    "    data[\"form_type\"] = get_text(root, \".//*[local-name()='submissionType']\")\n",
    "\n",
    "    # 2) Some top-level fields we might glean from <filerInfo> or <coverPage>:\n",
    "    #    We store them for the example schema.\n",
    "\n",
    "    # phoneNumber from coverPage/reportingPerson?\n",
    "    data[\"phone_number\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='phoneNumber']\")\n",
    "\n",
    "    # We'll store some additional placeholders in data (like accession_number, date_filed, etc.).\n",
    "    # Often these come from the <SEC-HEADER> block, not always from <edgarSubmission>.\n",
    "    # For demonstration, let's just store them as blank or glean them from a path if available:\n",
    "    data[\"accession_number\"] = \"\"\n",
    "    data[\"cik\"] = get_text(root, \".//*[local-name()='issuerCredentials']/*[local-name()='cik']\")\n",
    "    data[\"conformed_period\"] = parse_date(get_text(root, \".//*[local-name()='periodOfReport']\"))\n",
    "    data[\"date_filed\"] = None  # Typically from <ACCEPTANCE-DATETIME> or <SEC-HEADER>\n",
    "\n",
    "    # investment_company_type\n",
    "    data[\"investment_company_type\"] = get_text(root, \".//*[local-name()='investmentCompanyType']\")\n",
    "    # \"N-1A\", \"N-2\", etc.\n",
    "\n",
    "    data[\"report_type\"] = get_text(root, \".//*[local-name()='reportType']\")\n",
    "    if not data[\"report_type\"]:\n",
    "        data[\"report_type\"] = \"INSTITUTIONAL MANAGER VOTING REPORT\"  # default guess\n",
    "\n",
    "    # Some flags:\n",
    "    data[\"confidential_treatment\"] = \"N\"\n",
    "    val_conf = get_text(root, \".//*[local-name()='confidentialTreatment']\")\n",
    "    if val_conf.upper() == \"Y\":\n",
    "        data[\"confidential_treatment\"] = \"Y\"\n",
    "\n",
    "    # is_notice_report => if it's \"NOTICE REPORT\"\n",
    "    data[\"is_notice_report\"] = False\n",
    "    if \"NOTICE\" in data[\"report_type\"].upper():\n",
    "        data[\"is_notice_report\"] = True\n",
    "\n",
    "    data[\"explanatory_choice\"] = get_text(root, \".//*[local-name()='explanatoryChoice']\")\n",
    "    if not data[\"explanatory_choice\"]:\n",
    "        data[\"explanatory_choice\"] = \"N\"\n",
    "\n",
    "    # coverPage/reportingPerson -> name, address\n",
    "    data[\"reporting_person_name\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='name']\")\n",
    "\n",
    "    data[\"address_street1\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='street1']\")\n",
    "    data[\"address_street2\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='street2']\")\n",
    "    data[\"address_city\"]    = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='city']\")\n",
    "    data[\"address_state\"]   = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='stateOrCountry']\")\n",
    "    data[\"address_zip\"]     = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='zipCode']\")\n",
    "\n",
    "    # <summaryPage><otherIncludedManagersCount>\n",
    "    oim_count = get_text(root, \".//*[local-name()='otherIncludedManagersCount']\")\n",
    "    data[\"other_included_managers_count\"] = int(oim_count) if oim_count.isdigit() else 0\n",
    "\n",
    "    # Amendment info\n",
    "    data[\"is_amendment\"] = False\n",
    "    data[\"amendment_no\"] = None\n",
    "    data[\"amendment_type\"] = None\n",
    "    data[\"notice_explanation\"] = None\n",
    "\n",
    "    # signaturePage\n",
    "    data[\"signatory_name\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txSignature']\")\n",
    "    data[\"signatory_name_printed\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txPrintedSignature']\")\n",
    "    data[\"signatory_title\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txTitle']\")\n",
    "    sig_date_text = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txAsOfDate']\")\n",
    "    data[\"signatory_date\"] = parse_date(sig_date_text)\n",
    "\n",
    "    # Additional placeholders from the schema:\n",
    "    data[\"sec_file_number\"]       = \"\"\n",
    "    data[\"crd_number\"]            = \"\"\n",
    "    data[\"sec_file_number_other\"] = \"\"\n",
    "    data[\"lei_number\"]            = \"\"\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a599cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_institutional_managers(root):\n",
    "    \"\"\"\n",
    "    Extract <otherManagers2> or <otherManagersInfo>, etc., to find sub-managers\n",
    "    for the 'institutional_manager' table. In real N-PX, these are often in\n",
    "    <summaryPage> or <otherManagersInfo> blocks with repeated <otherManager> elements.\n",
    "\n",
    "    We'll return a list of dicts:\n",
    "       { \"serial_no\": int, \"name\": str, \"form13f_number\": str, etc. }\n",
    "\n",
    "    We'll keep it simple as an example, as the real structure can vary.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Example: <summaryPage> -> <otherManagers2> -> <investmentManagers> -> <serialNo>, <name>, ...\n",
    "    manager_nodes = root.xpath(\".//*[local-name()='otherManagers2']//*[local-name()='investmentManagers']\")\n",
    "    # or sometimes <otherManagers> directly\n",
    "    if not manager_nodes:\n",
    "        # fallback\n",
    "        manager_nodes = root.xpath(\".//*[local-name()='otherManager']\")\n",
    "\n",
    "    # manager_nodes might each contain sub-tags like <serialNo>, <managerName>, etc.\n",
    "    # We'll do a loop:\n",
    "    for mn in manager_nodes:\n",
    "        # Each one might be <investmentManagers> which has <serialNo>, <name>, ...\n",
    "        m_data = {}\n",
    "        m_data[\"serial_no\"]      = None\n",
    "        m_data[\"name\"]           = \"\"\n",
    "        m_data[\"form13f_number\"] = \"\"\n",
    "        m_data[\"crd_number\"]     = \"\"\n",
    "        m_data[\"sec_file_number\"] = \"\"\n",
    "        m_data[\"lei_number\"]     = \"\"\n",
    "\n",
    "        # Try extracting\n",
    "        sn = mn.xpath(\".//*[local-name()='serialNo']/text()\")\n",
    "        if sn and sn[0].isdigit():\n",
    "            m_data[\"serial_no\"] = int(sn[0])\n",
    "\n",
    "        # name\n",
    "        name = mn.xpath(\".//*[local-name()='name']/text()\")\n",
    "        if name:\n",
    "            m_data[\"name\"] = name[0].strip()\n",
    "\n",
    "        # form13f_number\n",
    "        f13 = mn.xpath(\".//*[local-name()='form13FFileNumber']/text()\")\n",
    "        if f13:\n",
    "            m_data[\"form13f_number\"] = f13[0].strip()\n",
    "\n",
    "        # crd_number\n",
    "        crd = mn.xpath(\".//*[local-name()='crdNumber']/text()\")\n",
    "        if crd:\n",
    "            m_data[\"crd_number\"] = crd[0].strip()\n",
    "\n",
    "        # sec_file_number\n",
    "        sfn = mn.xpath(\".//*[local-name()='secFileNumber']/text()\")\n",
    "        if sfn:\n",
    "            m_data[\"sec_file_number\"] = sfn[0].strip()\n",
    "\n",
    "        # lei_number\n",
    "        lei = mn.xpath(\".//*[local-name()='leiNumber']/text()\")\n",
    "        if lei:\n",
    "            m_data[\"lei_number\"] = lei[0].strip()\n",
    "\n",
    "        results.append(m_data)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_series_info(root):\n",
    "    \"\"\"\n",
    "    Extract <seriesPage> -> <seriesDetails> -> <seriesReports> -> <idOfSeries>, etc.\n",
    "    We'll return list of dicts for table 'series'.\n",
    "\n",
    "    Each series: \n",
    "       { \"series_code\": <idOfSeries>, \"series_name\": <nameOfSeries>, \"series_lei\": <leiOfSeries> }\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    series_nodes = root.xpath(\".//*[local-name()='seriesReports']\")\n",
    "    for sn in series_nodes:\n",
    "        s_data = {}\n",
    "        s_data[\"series_code\"] = get_text(sn, \".//*[local-name()='idOfSeries']\")\n",
    "        s_data[\"series_name\"] = get_text(sn, \".//*[local-name()='nameOfSeries']\")\n",
    "        s_data[\"series_lei\"]  = get_text(sn, \".//*[local-name()='leiOfSeries']\")\n",
    "        results.append(s_data)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d1f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_proxy_vote_table(vote_table_node, form_id):\n",
    "    \"\"\"\n",
    "    Extract each <proxyTable> from the provided <proxyVoteTable> node,\n",
    "    building rows for 'proxy_voting_record'. Also handle categories and managers and series for each.\n",
    "\n",
    "    We'll return a list of (proxy_voting_record_rows, categories_link_rows, manager_link_rows, series_link_rows).\n",
    "    \"\"\"\n",
    "    pvr_rows = []\n",
    "    cat_link_rows = []\n",
    "    mgr_link_rows = []\n",
    "    ser_link_rows = []\n",
    "\n",
    "    # We find each <proxyTable>\n",
    "    proxy_tables = vote_table_node.xpath(\".//*[local-name()='proxyTable']\")\n",
    "    for pt in proxy_tables:\n",
    "        row = {\n",
    "            \"form_id\": form_id,\n",
    "            \"issuer_name\":  get_text(pt, \".//*[local-name()='issuerName']\"),\n",
    "            \"cusip\":        get_text(pt, \".//*[local-name()='cusip']\"),\n",
    "            \"isin\":         get_text(pt, \".//*[local-name()='isin']\"),\n",
    "            \"figi\":         get_text(pt, \".//*[local-name()='figi']\"),\n",
    "            \"meeting_date\": parse_date(get_text(pt, \".//*[local-name()='meetingDate']\")),\n",
    "            \"vote_description\": get_text(pt, \".//*[local-name()='voteDescription']\"),\n",
    "            # proposed_by => <voteSource> or something similar\n",
    "            \"proposed_by\":  get_text(pt, \".//*[local-name()='voteSource']\"),\n",
    "            \"shares_voted\": get_decimal(pt, \".//*[local-name()='sharesVoted'][1]\"),\n",
    "            \"shares_on_loan\": get_decimal(pt, \".//*[local-name()='sharesOnLoan'][1]\"),\n",
    "            \"vote_cast\": None,\n",
    "            \"vote_cast_shares\": None,\n",
    "            \"management_rec\": None,\n",
    "            \"other_notes\": None\n",
    "        }\n",
    "\n",
    "        # A single <proxyTable> might have multiple <voteRecord>. We might store them as separate rows,\n",
    "        # or we might only store the first, or we might store them in separate columns. \n",
    "        # For simplicity, let's store only the *first* <voteRecord> we find.\n",
    "        vote_records = pt.xpath(\".//*[local-name()='voteRecord']\")\n",
    "        if vote_records:\n",
    "            first_vr = vote_records[0]\n",
    "            row[\"vote_cast\"]       = get_text(first_vr, \".//*[local-name()='howVoted']\")\n",
    "            row[\"vote_cast_shares\"] = get_decimal(first_vr, \".//*[local-name()='sharesVoted']\")\n",
    "            row[\"management_rec\"]  = get_text(first_vr, \".//*[local-name()='managementRecommendation']\")\n",
    "\n",
    "        # Possibly, we store additional <voteRecord> data in 'other_notes' or skip them.\n",
    "        # If there's more than one, let's just note how many in other_notes:\n",
    "        if len(vote_records) > 1:\n",
    "            row[\"other_notes\"] = f\"{len(vote_records)} total voteRecord items.\"\n",
    "\n",
    "        # 1) Save this row\n",
    "        pvr_rows.append(row)\n",
    "        current_vote_id = None  # We'll assign a local 'vote_id' after we know row index or while inserting into DB.\n",
    "\n",
    "        # 2) Categories. Usually: <voteCategories><voteCategory><categoryType>...\n",
    "        categories = pt.xpath(\".//*[local-name()='voteCategories']//*[local-name()='categoryType']/text()\")\n",
    "        # we link them to the vote_id. For now, we can't know the actual 'vote_id' until after insertion,\n",
    "        # but we can store a stub. We'll let the next step handle that.\n",
    "        # For CSV output, we might do an incremental ID for the row. We'll define that link once we get the final DataFrame.\n",
    "        for cat_str in categories:\n",
    "            cat_str_clean = cat_str.strip()\n",
    "            if cat_str_clean not in KNOWN_CATEGORIES:\n",
    "                # Insert into MATTER_CATEGORY_ROWS if not present\n",
    "                new_id = len(KNOWN_CATEGORIES) + 1  # naive approach for CSV, real DB might do serial\n",
    "                KNOWN_CATEGORIES[cat_str_clean] = new_id\n",
    "                MATTER_CATEGORY_ROWS.append({\n",
    "                    \"category_id\": new_id,\n",
    "                    \"category_type\": cat_str_clean\n",
    "                })\n",
    "            cat_id = KNOWN_CATEGORIES[cat_str_clean]\n",
    "            # We'll store in cat_link_rows after we know the vote_id. We'll do a placeholder:\n",
    "            cat_link_rows.append({\"category_id\": cat_id})\n",
    "\n",
    "        # 3) Possibly managers or series for this specific proxy table? \n",
    "        # In some filings, there's <voteManager><otherManagers> or <voteSeries>\n",
    "        # For brevity, let's store them similarly:\n",
    "        manager_vals = pt.xpath(\".//*[local-name()='voteManager']//*[local-name()='otherManager']/text()\")\n",
    "        for mv in manager_vals:\n",
    "            mgr_link_rows.append({\"manager_id_code\": mv.strip()})\n",
    "        # Similarly for series\n",
    "        series_val = get_text(pt, \".//*[local-name()='voteSeries']\")\n",
    "        if series_val:\n",
    "            ser_link_rows.append({\"series_code\": series_val})\n",
    "\n",
    "    return pvr_rows, cat_link_rows, mgr_link_rows, ser_link_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fb3ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_npx_files(folder_path=\"npx_filings\"):\n",
    "    \"\"\"\n",
    "    1) For each .txt in folder_path:\n",
    "       - Extract <XML> blocks\n",
    "       - Parse them with lxml\n",
    "       - For each <edgarSubmission> block, gather top-level form data\n",
    "       - For each <proxyVoteTable>, gather voting records\n",
    "       - For each <summaryPage> or <otherManagers> or <seriesPage>, gather managers/series\n",
    "    2) Populate global lists representing each DB table.\n",
    "    3) Write them out to CSV.\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(folder_path)\n",
    "    txt_files = [f for f in all_files if f.lower().endswith(\".txt\")]\n",
    "\n",
    "    # We'll maintain a \"form_id\" increment for CSV uniqueness\n",
    "    global_form_id = 1\n",
    "\n",
    "    for filename in txt_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "\n",
    "        # Extract raw <XML> blocks\n",
    "        xml_fragments = extract_xml_blocks(file_path)\n",
    "        if not xml_fragments:\n",
    "            print(\"  No <XML> blocks found.\")\n",
    "            continue\n",
    "\n",
    "        # We'll track whether we found \"edgarSubmission\" or \"proxyVoteTable\"\n",
    "        # in any block. Usually there's a block for form data and a block for the proxy table.\n",
    "        form_data_per_file = None\n",
    "\n",
    "        for frag in xml_fragments:\n",
    "            root = parse_xml_fragment(frag)\n",
    "            if root is None:\n",
    "                continue\n",
    "\n",
    "            # is this an <edgarSubmission>?\n",
    "            es = root.xpath(\"//*[local-name()='edgarSubmission']\")\n",
    "            if es:\n",
    "                # parse top-level form data once\n",
    "                submission_data = parse_edgar_submission(es[0])\n",
    "                # create the row in 'form_npx' - we assume only one form row per file\n",
    "                submission_data[\"form_id\"] = global_form_id\n",
    "                # Insert row:\n",
    "                FORM_NPX_ROWS.append(submission_data)\n",
    "\n",
    "                # parse managers\n",
    "                im_list = parse_institutional_managers(es[0])\n",
    "                for im in im_list:\n",
    "                    im_row = {\n",
    "                        \"manager_id\": None,  # assigned later\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"serial_no\": im[\"serial_no\"],\n",
    "                        \"name\": im[\"name\"],\n",
    "                        \"form13f_number\": im[\"form13f_number\"],\n",
    "                        \"crd_number\": im[\"crd_number\"],\n",
    "                        \"sec_file_number\": im[\"sec_file_number\"],\n",
    "                        \"lei_number\": im[\"lei_number\"]\n",
    "                    }\n",
    "                    INSTITUTIONAL_MANAGER_ROWS.append(im_row)\n",
    "\n",
    "                # parse series\n",
    "                s_list = parse_series_info(es[0])\n",
    "                for s in s_list:\n",
    "                    s_row = {\n",
    "                        \"series_id\": None,\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"series_code\": s[\"series_code\"],\n",
    "                        \"series_name\": s[\"series_name\"],\n",
    "                        \"series_lei\": s[\"series_lei\"]\n",
    "                    }\n",
    "                    SERIES_ROWS.append(s_row)\n",
    "\n",
    "                form_data_per_file = True\n",
    "\n",
    "            # Maybe there's a <proxyVoteTable> in this fragment\n",
    "            pvt = root.xpath(\"//*[local-name()='proxyVoteTable']\")\n",
    "            if pvt:\n",
    "                # We assume we have a form row for this file. If not, we create a minimal row.\n",
    "                if not form_data_per_file:\n",
    "                    # minimal row\n",
    "                    min_row = {\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"reporting_person_name\": \"\",\n",
    "                        \"phone_number\": \"\",\n",
    "                        \"address_street1\": \"\",\n",
    "                        \"address_street2\": \"\",\n",
    "                        \"address_city\": \"\",\n",
    "                        \"address_state\": \"\",\n",
    "                        \"address_zip\": \"\",\n",
    "                        \"accession_number\": \"\",\n",
    "                        \"cik\": \"\",\n",
    "                        \"conformed_period\": None,\n",
    "                        \"date_filed\": None,\n",
    "                        \"report_type\": \"\",\n",
    "                        \"form_type\": \"\",\n",
    "                        \"sec_file_number\": \"\",\n",
    "                        \"crd_number\": \"\",\n",
    "                        \"sec_file_number_other\": \"\",\n",
    "                        \"lei_number\": \"\",\n",
    "                        \"investment_company_type\": \"\",\n",
    "                        \"confidential_treatment\": \"N\",\n",
    "                        \"is_notice_report\": False,\n",
    "                        \"explanatory_choice\": \"N\",\n",
    "                        \"other_included_managers_count\": 0,\n",
    "                        \"is_amendment\": False,\n",
    "                        \"amendment_no\": None,\n",
    "                        \"amendment_type\": None,\n",
    "                        \"notice_explanation\": None,\n",
    "                        \"signatory_name\": \"\",\n",
    "                        \"signatory_name_printed\": \"\",\n",
    "                        \"signatory_title\": \"\",\n",
    "                        \"signatory_date\": None\n",
    "                    }\n",
    "                    FORM_NPX_ROWS.append(min_row)\n",
    "                    form_data_per_file = True\n",
    "\n",
    "                # parse each <proxyTable> inside\n",
    "                pvr_rows, cat_links, mgr_links, ser_links = parse_proxy_vote_table(pvt[0], global_form_id)\n",
    "                # We'll add them to the global data. \n",
    "                # But note: we do not yet know 'vote_id' for each row until we finalize them.\n",
    "                # We'll do a temporary approach by storing them in the global list \n",
    "                # and assign the index as vote_id after we gather them all.\n",
    "\n",
    "                PROXY_VOTING_RECORD_ROWS.extend(pvr_rows)\n",
    "                # We'll store these link rows in temp attributes of the row. We'll do an approach:\n",
    "                # Actually, let's do a quick approach: store them in a global, referencing row index \n",
    "                # after the fact. We'll store them as dict with 'row_idx' placeholders:\n",
    "                for c in cat_links:\n",
    "                    PROXY_VOTING_RECORD_CATEGORY_ROWS.append(c)\n",
    "                for m in mgr_links:\n",
    "                    VOTING_RECORD_MANAGER_ROWS.append(m)\n",
    "                for s in ser_links:\n",
    "                    VOTING_RECORD_SERIES_ROWS.append(s)\n",
    "\n",
    "        global_form_id += 1  # Move to next form ID for next file\n",
    "\n",
    "    print(\"\\nParsing complete!\")\n",
    "    print(f\"Forms collected: {len(FORM_NPX_ROWS)}\")\n",
    "    print(f\"Proxy Votes collected: {len(PROXY_VOTING_RECORD_ROWS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ece7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(output_folder=\"output_csv\"):\n",
    "    \"\"\"\n",
    "    Convert the global lists to DataFrames, do minimal cleaning, and write them to CSV.\n",
    "    We'll also assign artificial primary keys (e.g. vote_id) to link the child records.\n",
    "    In a real DB scenario, you'd do inserts and rely on the DB's SERIAL or IDENTITY columns.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # 1) form_npx -> DataFrame\n",
    "    df_form = pd.DataFrame(FORM_NPX_ROWS).drop_duplicates()\n",
    "    # We'll enforce a stable PK -> 'form_id'\n",
    "    # They should already have unique form_id if we did the global_form_id approach.\n",
    "    csv_form = os.path.join(output_folder, \"form_npx.csv\")\n",
    "    df_form.to_csv(csv_form, index=False)\n",
    "    print(f\"Saved {csv_form} with {len(df_form)} rows.\")\n",
    "\n",
    "    # 2) institutional_manager\n",
    "    # We have no real manager_id assigned. Let's assign them by row:\n",
    "    # We'll group them by (form_id, serial_no, name) to avoid duplicates.\n",
    "    df_im = pd.DataFrame(INSTITUTIONAL_MANAGER_ROWS).drop_duplicates()\n",
    "    df_im[\"manager_id\"] = df_im.index + 1  # naive approach\n",
    "    csv_im = os.path.join(output_folder, \"institutional_manager.csv\")\n",
    "    df_im.to_csv(csv_im, index=False)\n",
    "    print(f\"Saved {csv_im} with {len(df_im)} rows.\")\n",
    "\n",
    "    # 3) series\n",
    "    df_s = pd.DataFrame(SERIES_ROWS).drop_duplicates()\n",
    "    df_s[\"series_id\"] = df_s.index + 1\n",
    "    csv_s = os.path.join(output_folder, \"series.csv\")\n",
    "    df_s.to_csv(csv_s, index=False)\n",
    "    print(f\"Saved {csv_s} with {len(df_s)} rows.\")\n",
    "\n",
    "    # 4) proxy_voting_record\n",
    "    df_pvr = pd.DataFrame(PROXY_VOTING_RECORD_ROWS)\n",
    "    # Assign a naive PK:\n",
    "    df_pvr[\"vote_id\"] = df_pvr.index + 1\n",
    "    csv_pvr = os.path.join(output_folder, \"proxy_voting_record.csv\")\n",
    "    df_pvr.to_csv(csv_pvr, index=False)\n",
    "    print(f\"Saved {csv_pvr} with {len(df_pvr)} rows.\")\n",
    "\n",
    "    # 5) matter_category\n",
    "    df_mc = pd.DataFrame(MATTER_CATEGORY_ROWS).drop_duplicates(subset=[\"category_type\"])\n",
    "    csv_mc = os.path.join(output_folder, \"matter_category.csv\")\n",
    "    df_mc.to_csv(csv_mc, index=False)\n",
    "    print(f\"Saved {csv_mc} with {len(df_mc)} rows.\")\n",
    "\n",
    "    # 6) proxy_voting_record_category\n",
    "    # The tricky part is each row in PROXY_VOTING_RECORD_CATEGORY_ROWS does not have the actual vote_id assigned.\n",
    "    # We only stored category_id. In a real scenario, you'd do it after you insert the proxy votes. \n",
    "    # As a placeholder, let's assume each row matches the last inserted proxy record. That is obviously not correct\n",
    "    # for real multi-record files. A robust approach is to store the association at parse time.\n",
    "    # We'll just create a placeholder approach for demonstration.\n",
    "    # We'll say each \"cat_link\" belongs to the last row in df_pvr, but that's not correct logically.\n",
    "    # Instead, if you needed correct linking, you'd parse them at the same time. \n",
    "    # We'll show how you'd structure it if you had the correct reference:\n",
    "    if len(PROXY_VOTING_RECORD_CATEGORY_ROWS) > 0:\n",
    "        df_pvrc = []\n",
    "        # For demonstration, we'll link them all to vote_id=1 or so. This is not correct in multi-proxy scenarios.\n",
    "        # *In real code*, you want to track which <proxyTable> or row the categories came from.\n",
    "        # We'll just generate a trivial link for demonstration:\n",
    "        for idx, row in enumerate(PROXY_VOTING_RECORD_CATEGORY_ROWS):\n",
    "            # row = {\"category_id\": cat_id}\n",
    "            # We'll link it to the last proxy record for the sake of demonstration\n",
    "            # or we can link them all to index+1. \n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            if vote_id is None:\n",
    "                continue\n",
    "            df_pvrc.append({\"vote_id\": vote_id, \"category_id\": row[\"category_id\"]})\n",
    "        df_pvrc = pd.DataFrame(df_pvrc).drop_duplicates()\n",
    "    else:\n",
    "        df_pvrc = pd.DataFrame(columns=[\"vote_id\",\"category_id\"])\n",
    "\n",
    "    csv_pvrc = os.path.join(output_folder, \"proxy_voting_record_category.csv\")\n",
    "    df_pvrc.to_csv(csv_pvrc, index=False)\n",
    "    print(f\"Saved {csv_pvrc} with {len(df_pvrc)} rows.\")\n",
    "\n",
    "    # 7) voting_record_manager: same linking challenge\n",
    "    if len(VOTING_RECORD_MANAGER_ROWS) > 0:\n",
    "        df_vrm = []\n",
    "        for idx, row in enumerate(VOTING_RECORD_MANAGER_ROWS):\n",
    "            # row might look like {\"manager_id_code\": something}\n",
    "            # We have manager_id in df_im for each manager. We need to match row[\"manager_id_code\"] \n",
    "            # with one of the actual manager's attributes. \n",
    "            # But we never extracted that code into the manager table. \n",
    "            # We'll just link them in a naive approach for demonstration.\n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            manager_id = (idx % len(df_im)) + 1 if len(df_im) else None\n",
    "            df_vrm.append({\"vote_id\": vote_id, \"manager_id\": manager_id})\n",
    "        df_vrm = pd.DataFrame(df_vrm).drop_duplicates()\n",
    "    else:\n",
    "        df_vrm = pd.DataFrame(columns=[\"vote_id\",\"manager_id\"])\n",
    "\n",
    "    csv_vrm = os.path.join(output_folder, \"voting_record_manager.csv\")\n",
    "    df_vrm.to_csv(csv_vrm, index=False)\n",
    "    print(f\"Saved {csv_vrm} with {len(df_vrm)} rows.\")\n",
    "\n",
    "    # 8) voting_record_series\n",
    "    if len(VOTING_RECORD_SERIES_ROWS) > 0:\n",
    "        df_vrs = []\n",
    "        for idx, row in enumerate(VOTING_RECORD_SERIES_ROWS):\n",
    "            # row might look like {\"series_code\": something}\n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            series_id = (idx % len(df_s)) + 1 if len(df_s) else None\n",
    "            df_vrs.append({\"vote_id\": vote_id, \"series_id\": series_id})\n",
    "        df_vrs = pd.DataFrame(df_vrs).drop_duplicates()\n",
    "    else:\n",
    "        df_vrs = pd.DataFrame(columns=[\"vote_id\",\"series_id\"])\n",
    "\n",
    "    csv_vrs = os.path.join(output_folder, \"voting_record_series.csv\")\n",
    "    df_vrs.to_csv(csv_vrs, index=False)\n",
    "    print(f\"Saved {csv_vrs} with {len(df_vrs)} rows.\")\n",
    "\n",
    "    print(\"\\nAll CSVs written. You can load them into your DB as needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "250114fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(folder_path=\"npx_filings\", output_folder=\"output_csv\"):\n",
    "    \"\"\"\n",
    "    High-level convenience function:\n",
    "      1) Reset all global lists\n",
    "      2) Parse .txt files in 'folder_path'\n",
    "      3) Write to CSV in 'output_folder'\n",
    "    \"\"\"\n",
    "    # reset\n",
    "    global FORM_NPX_ROWS, INSTITUTIONAL_MANAGER_ROWS, SERIES_ROWS\n",
    "    global PROXY_VOTING_RECORD_ROWS, MATTER_CATEGORY_ROWS\n",
    "    global PROXY_VOTING_RECORD_CATEGORY_ROWS, VOTING_RECORD_MANAGER_ROWS, VOTING_RECORD_SERIES_ROWS\n",
    "    global KNOWN_CATEGORIES\n",
    "\n",
    "    FORM_NPX_ROWS = []\n",
    "    INSTITUTIONAL_MANAGER_ROWS = []\n",
    "    SERIES_ROWS = []\n",
    "    PROXY_VOTING_RECORD_ROWS = []\n",
    "    MATTER_CATEGORY_ROWS = []\n",
    "    PROXY_VOTING_RECORD_CATEGORY_ROWS = []\n",
    "    VOTING_RECORD_MANAGER_ROWS = []\n",
    "    VOTING_RECORD_SERIES_ROWS = []\n",
    "    KNOWN_CATEGORIES = {}\n",
    "\n",
    "    # parse\n",
    "    process_npx_files(folder_path=folder_path)\n",
    "    # write CSV\n",
    "    write_to_csv(output_folder=output_folder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee06af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: npx_filings\\2024-07-16_N-PX_0001376474-24-000319.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-07-17_N-PX_0001915315-24-000003.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-01_N-PX_0001172661-24-003023.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-09_N-PX_0001085146-24-003669.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-12_N-PX_0001437749-24-026024.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-16_N-PX_0001021408-24-002025.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-20_N-PX_0001172661-24-003627.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-20_N-PX_0001580642-24-004662.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-26_N-PX_0001135428-24-000141.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-26_N-PX_0001705819-24-000051.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-27_N-PX_0001013594-24-000698.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-27_N-PX_0001172661-24-003746.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-27_N-PX_0001846352-24-000006.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-28_N-PX_0001021408-24-005385.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-28_N-PX_0001420506-24-001838.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0000930413-24-002632.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001104659-24-094321.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001104659-24-094403.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0000919574-24-005220.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001398344-24-016928.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001438934-24-001631.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001993888-24-000009.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0002011250-24-000010.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-09-03_N-PX_0001438934-24-001919.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-09-10_N-PX_0001162044-24-001017.txt\n",
      "\n",
      "Parsing complete!\n",
      "Forms collected: 25\n",
      "Proxy Votes collected: 115047\n",
      "Saved output\\form_npx.csv with 25 rows.\n",
      "Saved output\\institutional_manager.csv with 4 rows.\n",
      "Saved output\\series.csv with 44 rows.\n",
      "Saved output\\proxy_voting_record.csv with 115047 rows.\n",
      "Saved output\\matter_category.csv with 15 rows.\n",
      "Saved output\\proxy_voting_record_category.csv with 120051 rows.\n",
      "Saved output\\voting_record_manager.csv with 1558 rows.\n",
      "Saved output\\voting_record_series.csv with 113332 rows.\n",
      "\n",
      "All CSVs written. You can load them into your DB as needed.\n"
     ]
    }
   ],
   "source": [
    "run_all(\"npx_filings\", \"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
