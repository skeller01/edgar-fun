{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff6296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import lxml.etree as ET\n",
    "\n",
    "# We'll define some global lists (or you could use data class structures).\n",
    "# Each will store rows as dictionaries, which we'll later convert to DataFrame -> CSV.\n",
    "\n",
    "FORM_NPX_ROWS = []\n",
    "INSTITUTIONAL_MANAGER_ROWS = []\n",
    "SERIES_ROWS = []\n",
    "PROXY_VOTING_RECORD_ROWS = []\n",
    "MATTER_CATEGORY_ROWS = []\n",
    "PROXY_VOTING_RECORD_CATEGORY_ROWS = []\n",
    "VOTING_RECORD_MANAGER_ROWS = []\n",
    "VOTING_RECORD_SERIES_ROWS = []\n",
    "\n",
    "# We'll keep track of known categories so we don't insert duplicates in matter_category\n",
    "KNOWN_CATEGORIES = {}\n",
    "\n",
    "# Utility: Function to safely parse date from text like \"06/30/2024\"\n",
    "def parse_date(date_string):\n",
    "    if not date_string:\n",
    "        return None\n",
    "    # We'll try mm/dd/yyyy or yyyy-mm-dd or other variations. \n",
    "    # You can tailor to your typical data format.\n",
    "    patterns = [\"%m/%d/%Y\", \"%Y-%m-%d\", \"%m-%d-%Y\"]\n",
    "    for fmt in patterns:\n",
    "        try:\n",
    "            return datetime.datetime.strptime(date_string.strip(), fmt).date()\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None  # If no parse worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56641fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_blocks(file_path):\n",
    "    \"\"\"\n",
    "    Reads an entire N-PX text file and extracts any <XML>...</XML> blocks.\n",
    "    Returns a list of raw XML strings. \n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Find all <XML> ... </XML> sections (case-insensitive).\n",
    "    pattern = re.compile(r\"<XML>(.*?)</XML>\", re.IGNORECASE | re.DOTALL)\n",
    "    return pattern.findall(text)\n",
    "\n",
    "\n",
    "def parse_xml_fragment(xml_string):\n",
    "    \"\"\"\n",
    "    Attempt to parse an individual XML fragment with lxml in recovery mode.\n",
    "    Return the root element or None if parse fails badly.\n",
    "    \"\"\"\n",
    "    parser = ET.XMLParser(recover=True, encoding=\"utf-8\")\n",
    "    try:\n",
    "        root = ET.fromstring(xml_string.encode(\"utf-8\"), parser=parser)\n",
    "        return root\n",
    "    except ET.XMLSyntaxError as e:\n",
    "        print(f\"  [Warning] parse error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_text(node, xpath_expr):\n",
    "    \"\"\"\n",
    "    Shortcut: runs an XPath for the first match of xpath_expr relative to `node`,\n",
    "    returns .text stripped or \"\" if none found.\n",
    "    \"\"\"\n",
    "    r = node.xpath(xpath_expr)\n",
    "    if r and r[0] is not None and r[0].text:\n",
    "        return r[0].text.strip()\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_bool(node, xpath_expr, default=False):\n",
    "    \"\"\"\n",
    "    Some elements may be \"Y\"/\"N\" or \"true\"/\"false\". We'll standardize to bool.\n",
    "    \"\"\"\n",
    "    txt = get_text(node, xpath_expr).upper()\n",
    "    if txt in [\"Y\", \"TRUE\", \"YES\"]:\n",
    "        return True\n",
    "    elif txt in [\"N\", \"FALSE\", \"NO\"]:\n",
    "        return False\n",
    "    return default\n",
    "\n",
    "\n",
    "def get_decimal(node, xpath_expr):\n",
    "    \"\"\"\n",
    "    Return a decimal (float) from the first match, or None if invalid/empty.\n",
    "    \"\"\"\n",
    "    txt = get_text(node, xpath_expr)\n",
    "    if not txt:\n",
    "        return None\n",
    "    try:\n",
    "        return float(txt.replace(\",\", \"\"))\n",
    "    except ValueError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848f2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edgar_submission(root):\n",
    "    \"\"\"\n",
    "    Attempt to extract top-level data for form_npx table from <edgarSubmission>.\n",
    "\n",
    "    We assume structure like:\n",
    "       <edgarSubmission>\n",
    "         <headerData>\n",
    "           <submissionType>...</submissionType>\n",
    "           ...\n",
    "         </headerData>\n",
    "         <filerInfo>\n",
    "           <registrantType>...</registrantType> etc.\n",
    "         </filerInfo>\n",
    "         <formData>\n",
    "           <coverPage>...</coverPage>\n",
    "           <signaturePage>...</signaturePage>\n",
    "           <summaryPage>...</summaryPage>\n",
    "         </formData>\n",
    "       </edgarSubmission>\n",
    "\n",
    "    Return a dict with columns matching 'form_npx'.\n",
    "    If data is missing or doesn't parse, return partial data or an empty dict.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # 1) submissionType => form_type\n",
    "    data[\"form_type\"] = get_text(root, \".//*[local-name()='submissionType']\")\n",
    "\n",
    "    # 2) Some top-level fields we might glean from <filerInfo> or <coverPage>:\n",
    "    #    We store them for the example schema.\n",
    "\n",
    "    # phoneNumber from coverPage/reportingPerson?\n",
    "    data[\"phone_number\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='phoneNumber']\")\n",
    "\n",
    "    # We'll store some additional placeholders in data (like accession_number, date_filed, etc.).\n",
    "    # Often these come from the <SEC-HEADER> block, not always from <edgarSubmission>.\n",
    "    # For demonstration, let's just store them as blank or glean them from a path if available:\n",
    "    data[\"accession_number\"] = \"\"\n",
    "    data[\"cik\"] = get_text(root, \".//*[local-name()='issuerCredentials']/*[local-name()='cik']\")\n",
    "    data[\"conformed_period\"] = parse_date(get_text(root, \".//*[local-name()='periodOfReport']\"))\n",
    "    data[\"date_filed\"] = None  # Typically from <ACCEPTANCE-DATETIME> or <SEC-HEADER>\n",
    "\n",
    "    # investment_company_type\n",
    "    data[\"investment_company_type\"] = get_text(root, \".//*[local-name()='investmentCompanyType']\")\n",
    "    # \"N-1A\", \"N-2\", etc.\n",
    "\n",
    "    data[\"report_type\"] = get_text(root, \".//*[local-name()='reportType']\")\n",
    "    if not data[\"report_type\"]:\n",
    "        data[\"report_type\"] = \"INSTITUTIONAL MANAGER VOTING REPORT\"  # default guess\n",
    "\n",
    "    # Some flags:\n",
    "    data[\"confidential_treatment\"] = \"N\"\n",
    "    val_conf = get_text(root, \".//*[local-name()='confidentialTreatment']\")\n",
    "    if val_conf.upper() == \"Y\":\n",
    "        data[\"confidential_treatment\"] = \"Y\"\n",
    "\n",
    "    # is_notice_report => if it's \"NOTICE REPORT\"\n",
    "    data[\"is_notice_report\"] = False\n",
    "    if \"NOTICE\" in data[\"report_type\"].upper():\n",
    "        data[\"is_notice_report\"] = True\n",
    "\n",
    "    data[\"explanatory_choice\"] = get_text(root, \".//*[local-name()='explanatoryChoice']\")\n",
    "    if not data[\"explanatory_choice\"]:\n",
    "        data[\"explanatory_choice\"] = \"N\"\n",
    "\n",
    "    # coverPage/reportingPerson -> name, address\n",
    "    data[\"reporting_person_name\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='name']\")\n",
    "\n",
    "    data[\"address_street1\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='street1']\")\n",
    "    data[\"address_street2\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='street2']\")\n",
    "    data[\"address_city\"]    = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='city']\")\n",
    "    data[\"address_state\"]   = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='stateOrCountry']\")\n",
    "    data[\"address_zip\"]     = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='zipCode']\")\n",
    "\n",
    "    # <summaryPage><otherIncludedManagersCount>\n",
    "    oim_count = get_text(root, \".//*[local-name()='otherIncludedManagersCount']\")\n",
    "    data[\"other_included_managers_count\"] = int(oim_count) if oim_count.isdigit() else 0\n",
    "\n",
    "    # Amendment info\n",
    "    data[\"is_amendment\"] = False\n",
    "    data[\"amendment_no\"] = None\n",
    "    data[\"amendment_type\"] = None\n",
    "    data[\"notice_explanation\"] = None\n",
    "\n",
    "    # signaturePage\n",
    "    data[\"signatory_name\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txSignature']\")\n",
    "    data[\"signatory_name_printed\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txPrintedSignature']\")\n",
    "    data[\"signatory_title\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txTitle']\")\n",
    "    sig_date_text = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txAsOfDate']\")\n",
    "    data[\"signatory_date\"] = parse_date(sig_date_text)\n",
    "\n",
    "    # Additional placeholders from the schema:\n",
    "    data[\"sec_file_number\"]       = \"\"\n",
    "    data[\"crd_number\"]            = \"\"\n",
    "    data[\"sec_file_number_other\"] = \"\"\n",
    "    data[\"lei_number\"]            = \"\"\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a599cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_institutional_managers(root):\n",
    "    \"\"\"\n",
    "    Extract <otherManagers2> or <otherManagersInfo>, etc., to find sub-managers\n",
    "    for the 'institutional_manager' table. In real N-PX, these are often in\n",
    "    <summaryPage> or <otherManagersInfo> blocks with repeated <otherManager> elements.\n",
    "\n",
    "    We'll return a list of dicts:\n",
    "       { \"serial_no\": int, \"name\": str, \"form13f_number\": str, etc. }\n",
    "\n",
    "    We'll keep it simple as an example, as the real structure can vary.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Example: <summaryPage> -> <otherManagers2> -> <investmentManagers> -> <serialNo>, <name>, ...\n",
    "    manager_nodes = root.xpath(\".//*[local-name()='otherManagers2']//*[local-name()='investmentManagers']\")\n",
    "    # or sometimes <otherManagers> directly\n",
    "    if not manager_nodes:\n",
    "        # fallback\n",
    "        manager_nodes = root.xpath(\".//*[local-name()='otherManager']\")\n",
    "\n",
    "    # manager_nodes might each contain sub-tags like <serialNo>, <managerName>, etc.\n",
    "    # We'll do a loop:\n",
    "    for mn in manager_nodes:\n",
    "        # Each one might be <investmentManagers> which has <serialNo>, <name>, ...\n",
    "        m_data = {}\n",
    "        m_data[\"serial_no\"]      = None\n",
    "        m_data[\"name\"]           = \"\"\n",
    "        m_data[\"form13f_number\"] = \"\"\n",
    "        m_data[\"crd_number\"]     = \"\"\n",
    "        m_data[\"sec_file_number\"] = \"\"\n",
    "        m_data[\"lei_number\"]     = \"\"\n",
    "\n",
    "        # Try extracting\n",
    "        sn = mn.xpath(\".//*[local-name()='serialNo']/text()\")\n",
    "        if sn and sn[0].isdigit():\n",
    "            m_data[\"serial_no\"] = int(sn[0])\n",
    "\n",
    "        # name\n",
    "        name = mn.xpath(\".//*[local-name()='name']/text()\")\n",
    "        if name:\n",
    "            m_data[\"name\"] = name[0].strip()\n",
    "\n",
    "        # form13f_number\n",
    "        f13 = mn.xpath(\".//*[local-name()='form13FFileNumber']/text()\")\n",
    "        if f13:\n",
    "            m_data[\"form13f_number\"] = f13[0].strip()\n",
    "\n",
    "        # crd_number\n",
    "        crd = mn.xpath(\".//*[local-name()='crdNumber']/text()\")\n",
    "        if crd:\n",
    "            m_data[\"crd_number\"] = crd[0].strip()\n",
    "\n",
    "        # sec_file_number\n",
    "        sfn = mn.xpath(\".//*[local-name()='secFileNumber']/text()\")\n",
    "        if sfn:\n",
    "            m_data[\"sec_file_number\"] = sfn[0].strip()\n",
    "\n",
    "        # lei_number\n",
    "        lei = mn.xpath(\".//*[local-name()='leiNumber']/text()\")\n",
    "        if lei:\n",
    "            m_data[\"lei_number\"] = lei[0].strip()\n",
    "\n",
    "        results.append(m_data)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_series_info(root):\n",
    "    \"\"\"\n",
    "    Extract <seriesPage> -> <seriesDetails> -> <seriesReports> -> <idOfSeries>, etc.\n",
    "    We'll return list of dicts for table 'series'.\n",
    "\n",
    "    Each series: \n",
    "       { \"series_code\": <idOfSeries>, \"series_name\": <nameOfSeries>, \"series_lei\": <leiOfSeries> }\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    series_nodes = root.xpath(\".//*[local-name()='seriesReports']\")\n",
    "    for sn in series_nodes:\n",
    "        s_data = {}\n",
    "        s_data[\"series_code\"] = get_text(sn, \".//*[local-name()='idOfSeries']\")\n",
    "        s_data[\"series_name\"] = get_text(sn, \".//*[local-name()='nameOfSeries']\")\n",
    "        s_data[\"series_lei\"]  = get_text(sn, \".//*[local-name()='leiOfSeries']\")\n",
    "        results.append(s_data)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d1f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_proxy_vote_table(vote_table_node, form_id):\n",
    "    \"\"\"\n",
    "    Extract each <proxyTable> from the provided <proxyVoteTable> node,\n",
    "    building rows for 'proxy_voting_record'. Also handle categories and managers and series for each.\n",
    "\n",
    "    We'll return a list of (proxy_voting_record_rows, categories_link_rows, manager_link_rows, series_link_rows).\n",
    "    \"\"\"\n",
    "    pvr_rows = []\n",
    "    cat_link_rows = []\n",
    "    mgr_link_rows = []\n",
    "    ser_link_rows = []\n",
    "\n",
    "    # We find each <proxyTable>\n",
    "    proxy_tables = vote_table_node.xpath(\".//*[local-name()='proxyTable']\")\n",
    "    for pt in proxy_tables:\n",
    "        row = {\n",
    "            \"form_id\": form_id,\n",
    "            \"issuer_name\":  get_text(pt, \".//*[local-name()='issuerName']\"),\n",
    "            \"cusip\":        get_text(pt, \".//*[local-name()='cusip']\"),\n",
    "            \"isin\":         get_text(pt, \".//*[local-name()='isin']\"),\n",
    "            \"figi\":         get_text(pt, \".//*[local-name()='figi']\"),\n",
    "            \"meeting_date\": parse_date(get_text(pt, \".//*[local-name()='meetingDate']\")),\n",
    "            \"vote_description\": get_text(pt, \".//*[local-name()='voteDescription']\"),\n",
    "            # proposed_by => <voteSource> or something similar\n",
    "            \"proposed_by\":  get_text(pt, \".//*[local-name()='voteSource']\"),\n",
    "            \"shares_voted\": get_decimal(pt, \".//*[local-name()='sharesVoted'][1]\"),\n",
    "            \"shares_on_loan\": get_decimal(pt, \".//*[local-name()='sharesOnLoan'][1]\"),\n",
    "            \"vote_cast\": None,\n",
    "            \"vote_cast_shares\": None,\n",
    "            \"management_rec\": None,\n",
    "            \"other_notes\": None\n",
    "        }\n",
    "\n",
    "        # A single <proxyTable> might have multiple <voteRecord>. We might store them as separate rows,\n",
    "        # or we might only store the first, or we might store them in separate columns. \n",
    "        # For simplicity, let's store only the *first* <voteRecord> we find.\n",
    "        vote_records = pt.xpath(\".//*[local-name()='voteRecord']\")\n",
    "        if vote_records:\n",
    "            first_vr = vote_records[0]\n",
    "            row[\"vote_cast\"]       = get_text(first_vr, \".//*[local-name()='howVoted']\")\n",
    "            row[\"vote_cast_shares\"] = get_decimal(first_vr, \".//*[local-name()='sharesVoted']\")\n",
    "            row[\"management_rec\"]  = get_text(first_vr, \".//*[local-name()='managementRecommendation']\")\n",
    "\n",
    "        # Possibly, we store additional <voteRecord> data in 'other_notes' or skip them.\n",
    "        # If there's more than one, let's just note how many in other_notes:\n",
    "        if len(vote_records) > 1:\n",
    "            row[\"other_notes\"] = f\"{len(vote_records)} total voteRecord items.\"\n",
    "\n",
    "        # 1) Save this row\n",
    "        pvr_rows.append(row)\n",
    "        current_vote_id = None  # We'll assign a local 'vote_id' after we know row index or while inserting into DB.\n",
    "\n",
    "        # 2) Categories. Usually: <voteCategories><voteCategory><categoryType>...\n",
    "        categories = pt.xpath(\".//*[local-name()='voteCategories']//*[local-name()='categoryType']/text()\")\n",
    "        # we link them to the vote_id. For now, we can't know the actual 'vote_id' until after insertion,\n",
    "        # but we can store a stub. We'll let the next step handle that.\n",
    "        # For CSV output, we might do an incremental ID for the row. We'll define that link once we get the final DataFrame.\n",
    "        for cat_str in categories:\n",
    "            cat_str_clean = cat_str.strip()\n",
    "            if cat_str_clean not in KNOWN_CATEGORIES:\n",
    "                # Insert into MATTER_CATEGORY_ROWS if not present\n",
    "                new_id = len(KNOWN_CATEGORIES) + 1  # naive approach for CSV, real DB might do serial\n",
    "                KNOWN_CATEGORIES[cat_str_clean] = new_id\n",
    "                MATTER_CATEGORY_ROWS.append({\n",
    "                    \"category_id\": new_id,\n",
    "                    \"category_type\": cat_str_clean\n",
    "                })\n",
    "            cat_id = KNOWN_CATEGORIES[cat_str_clean]\n",
    "            # We'll store in cat_link_rows after we know the vote_id. We'll do a placeholder:\n",
    "            cat_link_rows.append({\"category_id\": cat_id})\n",
    "\n",
    "        # 3) Possibly managers or series for this specific proxy table? \n",
    "        # In some filings, there's <voteManager><otherManagers> or <voteSeries>\n",
    "        # For brevity, let's store them similarly:\n",
    "        manager_vals = pt.xpath(\".//*[local-name()='voteManager']//*[local-name()='otherManager']/text()\")\n",
    "        for mv in manager_vals:\n",
    "            mgr_link_rows.append({\"manager_id_code\": mv.strip()})\n",
    "        # Similarly for series\n",
    "        series_val = get_text(pt, \".//*[local-name()='voteSeries']\")\n",
    "        if series_val:\n",
    "            ser_link_rows.append({\"series_code\": series_val})\n",
    "\n",
    "    return pvr_rows, cat_link_rows, mgr_link_rows, ser_link_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb3ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_npx_files(folder_path=\"npx_filings\"):\n",
    "    \"\"\"\n",
    "    1) For each .txt in folder_path:\n",
    "       - Extract <XML> blocks\n",
    "       - Parse them with lxml\n",
    "       - For each <edgarSubmission> block, gather top-level form data\n",
    "       - For each <proxyVoteTable>, gather voting records\n",
    "       - For each <summaryPage> or <otherManagers> or <seriesPage>, gather managers/series\n",
    "    2) Populate global lists representing each DB table.\n",
    "    3) Write them out to CSV.\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(folder_path)\n",
    "    txt_files = [f for f in all_files if f.lower().endswith(\".txt\")]\n",
    "\n",
    "    # We'll maintain a \"form_id\" increment for CSV uniqueness\n",
    "    global_form_id = 1\n",
    "\n",
    "    for filename in txt_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"\\nProcessing file: {file_path}\")\n",
    "\n",
    "        # Extract raw <XML> blocks\n",
    "        xml_fragments = extract_xml_blocks(file_path)\n",
    "        if not xml_fragments:\n",
    "            print(\"  No <XML> blocks found.\")\n",
    "            continue\n",
    "\n",
    "        # We'll track whether we found \"edgarSubmission\" or \"proxyVoteTable\"\n",
    "        # in any block. Usually there's a block for form data and a block for the proxy table.\n",
    "        form_data_per_file = None\n",
    "\n",
    "        for frag in xml_fragments:\n",
    "            root = parse_xml_fragment(frag)\n",
    "            if root is None:\n",
    "                continue\n",
    "\n",
    "            # is this an <edgarSubmission>?\n",
    "            es = root.xpath(\"//*[local-name()='edgarSubmission']\")\n",
    "            if es:\n",
    "                # parse top-level form data once\n",
    "                submission_data = parse_edgar_submission(es[0])\n",
    "                # create the row in 'form_npx' - we assume only one form row per file\n",
    "                submission_data[\"form_id\"] = global_form_id\n",
    "                # Insert row:\n",
    "                FORM_NPX_ROWS.append(submission_data)\n",
    "\n",
    "                # parse managers\n",
    "                im_list = parse_institutional_managers(es[0])\n",
    "                for im in im_list:\n",
    "                    im_row = {\n",
    "                        \"manager_id\": None,  # assigned later\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"serial_no\": im[\"serial_no\"],\n",
    "                        \"name\": im[\"name\"],\n",
    "                        \"form13f_number\": im[\"form13f_number\"],\n",
    "                        \"crd_number\": im[\"crd_number\"],\n",
    "                        \"sec_file_number\": im[\"sec_file_number\"],\n",
    "                        \"lei_number\": im[\"lei_number\"]\n",
    "                    }\n",
    "                    INSTITUTIONAL_MANAGER_ROWS.append(im_row)\n",
    "\n",
    "                # parse series\n",
    "                s_list = parse_series_info(es[0])\n",
    "                for s in s_list:\n",
    "                    s_row = {\n",
    "                        \"series_id\": None,\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"series_code\": s[\"series_code\"],\n",
    "                        \"series_name\": s[\"series_name\"],\n",
    "                        \"series_lei\": s[\"series_lei\"]\n",
    "                    }\n",
    "                    SERIES_ROWS.append(s_row)\n",
    "\n",
    "                form_data_per_file = True\n",
    "\n",
    "            # Maybe there's a <proxyVoteTable> in this fragment\n",
    "            pvt = root.xpath(\"//*[local-name()='proxyVoteTable']\")\n",
    "            if pvt:\n",
    "                # We assume we have a form row for this file. If not, we create a minimal row.\n",
    "                if not form_data_per_file:\n",
    "                    # minimal row\n",
    "                    min_row = {\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"reporting_person_name\": \"\",\n",
    "                        \"phone_number\": \"\",\n",
    "                        \"address_street1\": \"\",\n",
    "                        \"address_street2\": \"\",\n",
    "                        \"address_city\": \"\",\n",
    "                        \"address_state\": \"\",\n",
    "                        \"address_zip\": \"\",\n",
    "                        \"accession_number\": \"\",\n",
    "                        \"cik\": \"\",\n",
    "                        \"conformed_period\": None,\n",
    "                        \"date_filed\": None,\n",
    "                        \"report_type\": \"\",\n",
    "                        \"form_type\": \"\",\n",
    "                        \"sec_file_number\": \"\",\n",
    "                        \"crd_number\": \"\",\n",
    "                        \"sec_file_number_other\": \"\",\n",
    "                        \"lei_number\": \"\",\n",
    "                        \"investment_company_type\": \"\",\n",
    "                        \"confidential_treatment\": \"N\",\n",
    "                        \"is_notice_report\": False,\n",
    "                        \"explanatory_choice\": \"N\",\n",
    "                        \"other_included_managers_count\": 0,\n",
    "                        \"is_amendment\": False,\n",
    "                        \"amendment_no\": None,\n",
    "                        \"amendment_type\": None,\n",
    "                        \"notice_explanation\": None,\n",
    "                        \"signatory_name\": \"\",\n",
    "                        \"signatory_name_printed\": \"\",\n",
    "                        \"signatory_title\": \"\",\n",
    "                        \"signatory_date\": None\n",
    "                    }\n",
    "                    FORM_NPX_ROWS.append(min_row)\n",
    "                    form_data_per_file = True\n",
    "\n",
    "                # parse each <proxyTable> inside\n",
    "                pvr_rows, cat_links, mgr_links, ser_links = parse_proxy_vote_table(pvt[0], global_form_id)\n",
    "                # We'll add them to the global data. \n",
    "                # But note: we do not yet know 'vote_id' for each row until we finalize them.\n",
    "                # We'll do a temporary approach by storing them in the global list \n",
    "                # and assign the index as vote_id after we gather them all.\n",
    "\n",
    "                PROXY_VOTING_RECORD_ROWS.extend(pvr_rows)\n",
    "                # We'll store these link rows in temp attributes of the row. We'll do an approach:\n",
    "                # Actually, let's do a quick approach: store them in a global, referencing row index \n",
    "                # after the fact. We'll store them as dict with 'row_idx' placeholders:\n",
    "                for c in cat_links:\n",
    "                    PROXY_VOTING_RECORD_CATEGORY_ROWS.append(c)\n",
    "                for m in mgr_links:\n",
    "                    VOTING_RECORD_MANAGER_ROWS.append(m)\n",
    "                for s in ser_links:\n",
    "                    VOTING_RECORD_SERIES_ROWS.append(s)\n",
    "\n",
    "        global_form_id += 1  # Move to next form ID for next file\n",
    "\n",
    "    print(\"\\nParsing complete!\")\n",
    "    print(f\"Forms collected: {len(FORM_NPX_ROWS)}\")\n",
    "    print(f\"Proxy Votes collected: {len(PROXY_VOTING_RECORD_ROWS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ece7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(output_folder=\"output_csv\"):\n",
    "    \"\"\"\n",
    "    Convert the global lists to DataFrames, do minimal cleaning, and write them to CSV.\n",
    "    We'll also assign artificial primary keys (e.g. vote_id) to link the child records.\n",
    "    In a real DB scenario, you'd do inserts and rely on the DB's SERIAL or IDENTITY columns.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # 1) form_npx -> DataFrame\n",
    "    df_form = pd.DataFrame(FORM_NPX_ROWS).drop_duplicates()\n",
    "    # We'll enforce a stable PK -> 'form_id'\n",
    "    # They should already have unique form_id if we did the global_form_id approach.\n",
    "    csv_form = os.path.join(output_folder, \"form_npx.csv\")\n",
    "    df_form.to_csv(csv_form, index=False)\n",
    "    print(f\"Saved {csv_form} with {len(df_form)} rows.\")\n",
    "\n",
    "    # 2) institutional_manager\n",
    "    # We have no real manager_id assigned. Let's assign them by row:\n",
    "    # We'll group them by (form_id, serial_no, name) to avoid duplicates.\n",
    "    df_im = pd.DataFrame(INSTITUTIONAL_MANAGER_ROWS).drop_duplicates()\n",
    "    df_im[\"manager_id\"] = df_im.index + 1  # naive approach\n",
    "    csv_im = os.path.join(output_folder, \"institutional_manager.csv\")\n",
    "    df_im.to_csv(csv_im, index=False)\n",
    "    print(f\"Saved {csv_im} with {len(df_im)} rows.\")\n",
    "\n",
    "    # 3) series\n",
    "    df_s = pd.DataFrame(SERIES_ROWS).drop_duplicates()\n",
    "    df_s[\"series_id\"] = df_s.index + 1\n",
    "    csv_s = os.path.join(output_folder, \"series.csv\")\n",
    "    df_s.to_csv(csv_s, index=False)\n",
    "    print(f\"Saved {csv_s} with {len(df_s)} rows.\")\n",
    "\n",
    "    # 4) proxy_voting_record\n",
    "    df_pvr = pd.DataFrame(PROXY_VOTING_RECORD_ROWS)\n",
    "    # Assign a naive PK:\n",
    "    df_pvr[\"vote_id\"] = df_pvr.index + 1\n",
    "    csv_pvr = os.path.join(output_folder, \"proxy_voting_record.csv\")\n",
    "    df_pvr.to_csv(csv_pvr, index=False)\n",
    "    print(f\"Saved {csv_pvr} with {len(df_pvr)} rows.\")\n",
    "\n",
    "    # 5) matter_category\n",
    "    df_mc = pd.DataFrame(MATTER_CATEGORY_ROWS).drop_duplicates(subset=[\"category_type\"])\n",
    "    csv_mc = os.path.join(output_folder, \"matter_category.csv\")\n",
    "    df_mc.to_csv(csv_mc, index=False)\n",
    "    print(f\"Saved {csv_mc} with {len(df_mc)} rows.\")\n",
    "\n",
    "    # 6) proxy_voting_record_category\n",
    "    # The tricky part is each row in PROXY_VOTING_RECORD_CATEGORY_ROWS does not have the actual vote_id assigned.\n",
    "    # We only stored category_id. In a real scenario, you'd do it after you insert the proxy votes. \n",
    "    # As a placeholder, let's assume each row matches the last inserted proxy record. That is obviously not correct\n",
    "    # for real multi-record files. A robust approach is to store the association at parse time.\n",
    "    # We'll just create a placeholder approach for demonstration.\n",
    "    # We'll say each \"cat_link\" belongs to the last row in df_pvr, but that's not correct logically.\n",
    "    # Instead, if you needed correct linking, you'd parse them at the same time. \n",
    "    # We'll show how you'd structure it if you had the correct reference:\n",
    "    if len(PROXY_VOTING_RECORD_CATEGORY_ROWS) > 0:\n",
    "        df_pvrc = []\n",
    "        # For demonstration, we'll link them all to vote_id=1 or so. This is not correct in multi-proxy scenarios.\n",
    "        # *In real code*, you want to track which <proxyTable> or row the categories came from.\n",
    "        # We'll just generate a trivial link for demonstration:\n",
    "        for idx, row in enumerate(PROXY_VOTING_RECORD_CATEGORY_ROWS):\n",
    "            # row = {\"category_id\": cat_id}\n",
    "            # We'll link it to the last proxy record for the sake of demonstration\n",
    "            # or we can link them all to index+1. \n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            if vote_id is None:\n",
    "                continue\n",
    "            df_pvrc.append({\"vote_id\": vote_id, \"category_id\": row[\"category_id\"]})\n",
    "        df_pvrc = pd.DataFrame(df_pvrc).drop_duplicates()\n",
    "    else:\n",
    "        df_pvrc = pd.DataFrame(columns=[\"vote_id\",\"category_id\"])\n",
    "\n",
    "    csv_pvrc = os.path.join(output_folder, \"proxy_voting_record_category.csv\")\n",
    "    df_pvrc.to_csv(csv_pvrc, index=False)\n",
    "    print(f\"Saved {csv_pvrc} with {len(df_pvrc)} rows.\")\n",
    "\n",
    "    # 7) voting_record_manager: same linking challenge\n",
    "    if len(VOTING_RECORD_MANAGER_ROWS) > 0:\n",
    "        df_vrm = []\n",
    "        for idx, row in enumerate(VOTING_RECORD_MANAGER_ROWS):\n",
    "            # row might look like {\"manager_id_code\": something}\n",
    "            # We have manager_id in df_im for each manager. We need to match row[\"manager_id_code\"] \n",
    "            # with one of the actual manager's attributes. \n",
    "            # But we never extracted that code into the manager table. \n",
    "            # We'll just link them in a naive approach for demonstration.\n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            manager_id = (idx % len(df_im)) + 1 if len(df_im) else None\n",
    "            df_vrm.append({\"vote_id\": vote_id, \"manager_id\": manager_id})\n",
    "        df_vrm = pd.DataFrame(df_vrm).drop_duplicates()\n",
    "    else:\n",
    "        df_vrm = pd.DataFrame(columns=[\"vote_id\",\"manager_id\"])\n",
    "\n",
    "    csv_vrm = os.path.join(output_folder, \"voting_record_manager.csv\")\n",
    "    df_vrm.to_csv(csv_vrm, index=False)\n",
    "    print(f\"Saved {csv_vrm} with {len(df_vrm)} rows.\")\n",
    "\n",
    "    # 8) voting_record_series\n",
    "    if len(VOTING_RECORD_SERIES_ROWS) > 0:\n",
    "        df_vrs = []\n",
    "        for idx, row in enumerate(VOTING_RECORD_SERIES_ROWS):\n",
    "            # row might look like {\"series_code\": something}\n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            series_id = (idx % len(df_s)) + 1 if len(df_s) else None\n",
    "            df_vrs.append({\"vote_id\": vote_id, \"series_id\": series_id})\n",
    "        df_vrs = pd.DataFrame(df_vrs).drop_duplicates()\n",
    "    else:\n",
    "        df_vrs = pd.DataFrame(columns=[\"vote_id\",\"series_id\"])\n",
    "\n",
    "    csv_vrs = os.path.join(output_folder, \"voting_record_series.csv\")\n",
    "    df_vrs.to_csv(csv_vrs, index=False)\n",
    "    print(f\"Saved {csv_vrs} with {len(df_vrs)} rows.\")\n",
    "\n",
    "    print(\"\\nAll CSVs written. You can load them into your DB as needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250114fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(folder_path=\"npx_filings\", output_folder=\"output_csv\"):\n",
    "    \"\"\"\n",
    "    High-level convenience function:\n",
    "      1) Reset all global lists\n",
    "      2) Parse .txt files in 'folder_path'\n",
    "      3) Write to CSV in 'output_folder'\n",
    "    \"\"\"\n",
    "    # reset\n",
    "    global FORM_NPX_ROWS, INSTITUTIONAL_MANAGER_ROWS, SERIES_ROWS\n",
    "    global PROXY_VOTING_RECORD_ROWS, MATTER_CATEGORY_ROWS\n",
    "    global PROXY_VOTING_RECORD_CATEGORY_ROWS, VOTING_RECORD_MANAGER_ROWS, VOTING_RECORD_SERIES_ROWS\n",
    "    global KNOWN_CATEGORIES\n",
    "\n",
    "    FORM_NPX_ROWS = []\n",
    "    INSTITUTIONAL_MANAGER_ROWS = []\n",
    "    SERIES_ROWS = []\n",
    "    PROXY_VOTING_RECORD_ROWS = []\n",
    "    MATTER_CATEGORY_ROWS = []\n",
    "    PROXY_VOTING_RECORD_CATEGORY_ROWS = []\n",
    "    VOTING_RECORD_MANAGER_ROWS = []\n",
    "    VOTING_RECORD_SERIES_ROWS = []\n",
    "    KNOWN_CATEGORIES = {}\n",
    "\n",
    "    # parse\n",
    "    process_npx_files(folder_path=folder_path)\n",
    "    # write CSV\n",
    "    write_to_csv(output_folder=output_folder)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee06af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: npx_filings\\2024-07-16_N-PX_0001376474-24-000319.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-07-17_N-PX_0001915315-24-000003.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-07-23_N-PX_0000720064-24-000002.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-07-30_N-PX_0001843110-24-000005.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-01_N-PX_0001172661-24-003023.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-05_N-PX_0001951757-24-000677.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-06_N-PX_0001951757-24-000708.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-09_N-PX_0001085146-24-003669.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-09_N-PX_0001775530-24-000005.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-12_N-PX_0001437749-24-026024.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-16_N-PX_0001021408-24-002025.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-20_N-PX_0001172661-24-003627.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-20_N-PX_0001580642-24-004662.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-21_N-PX_0001085146-24-004090.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-22_N-PX_0001104659-24-091897.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-22_N-PX_0001973783-24-000005.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-26_N-PX_0000312348-24-000032.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-26_N-PX_0001085146-24-004205.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-26_N-PX_0001135428-24-000141.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-26_N-PX_0001705819-24-000051.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-27_N-PX_0001013594-24-000698.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-27_N-PX_0001082215-24-000009.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-27_N-PX_0001172661-24-003746.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-27_N-PX_0001846352-24-000006.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-28_N-PX_0001021408-24-005385.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-28_N-PX_0001420506-24-001838.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-28_N-PX_0001438934-24-001035.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0000005272-24-000093.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0000930413-24-002632.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001021408-24-005720.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001104659-24-094321.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001104659-24-094403.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001206774-24-000871.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001438934-24-001195.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001591546-24-000013.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-29_N-PX_0001633864-24-000005.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0000919574-24-005220.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001021408-24-006715.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001398344-24-016928.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001438934-24-001537.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001438934-24-001631.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001941040-24-000410.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0001993888-24-000009.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-08-30_N-PX_0002011250-24-000010.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-09-03_N-PX_0001104659-24-095771.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-09-03_N-PX_0001104659-24-095806.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-09-03_N-PX_0001438934-24-001919.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-09-10_N-PX_0001162044-24-001017.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-09-13_N-PX_0001172661-24-004082.txt\n",
      "\n",
      "Processing file: npx_filings\\2024-10-02_N-PX_0002011188-24-000008.txt\n",
      "\n",
      "Parsing complete!\n",
      "Forms collected: 50\n",
      "Proxy Votes collected: 163116\n",
      "Saved output\\form_npx.csv with 50 rows.\n",
      "Saved output\\institutional_manager.csv with 15 rows.\n",
      "Saved output\\series.csv with 89 rows.\n",
      "Saved output\\proxy_voting_record.csv with 163116 rows.\n",
      "Saved output\\matter_category.csv with 15 rows.\n",
      "Saved output\\proxy_voting_record_category.csv with 168562 rows.\n",
      "Saved output\\voting_record_manager.csv with 47142 rows.\n",
      "Saved output\\voting_record_series.csv with 156638 rows.\n",
      "\n",
      "All CSVs written. You can load them into your DB as needed.\n"
     ]
    }
   ],
   "source": [
    "run_all(\"npx_filings\", \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63d58b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import lxml.etree as ET\n",
    "\n",
    "# Global data lists that we store table rows in:\n",
    "FORM_NPX_ROWS = []\n",
    "INSTITUTIONAL_MANAGER_ROWS = []\n",
    "SERIES_ROWS = []\n",
    "PROXY_VOTING_RECORD_ROWS = []\n",
    "MATTER_CATEGORY_ROWS = []\n",
    "PROXY_VOTING_RECORD_CATEGORY_ROWS = []\n",
    "VOTING_RECORD_MANAGER_ROWS = []\n",
    "VOTING_RECORD_SERIES_ROWS = []\n",
    "\n",
    "# Keep track of known categories to avoid duplicate insertion\n",
    "KNOWN_CATEGORIES = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bbfdd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_string):\n",
    "    \"\"\"\n",
    "    Safely parse date from text like '06/30/2024' or '2024-06-30' or 'YYYYMMDD'.\n",
    "    The official EDGAR N-PX spec wants date as MM-DD-YYYY if the element is included,\n",
    "    but real filers can vary. \n",
    "    \"\"\"\n",
    "    if not date_string:\n",
    "        return None\n",
    "    patterns = [\"%m/%d/%Y\", \"%Y-%m-%d\", \"%m-%d-%Y\", \"%Y%m%d\"]\n",
    "    for fmt in patterns:\n",
    "        try:\n",
    "            return datetime.datetime.strptime(date_string.strip(), fmt).date()\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def get_text(node, xpath_expr):\n",
    "    \"\"\"\n",
    "    Shortcut: returns .text of the first match or '' if none found.\n",
    "    \"\"\"\n",
    "    result = node.xpath(xpath_expr)\n",
    "    if result and result[0] is not None and result[0].text:\n",
    "        return result[0].text.strip()\n",
    "    return \"\"\n",
    "\n",
    "def get_bool(node, xpath_expr, default=False):\n",
    "    \"\"\"\n",
    "    Some elements may be \"Y\"/\"N\", \"true\"/\"false\". We'll unify them to Python bool.\n",
    "    \"\"\"\n",
    "    txt = get_text(node, xpath_expr).upper()\n",
    "    if txt in [\"Y\", \"YES\", \"TRUE\", \"1\"]:\n",
    "        return True\n",
    "    elif txt in [\"N\", \"NO\", \"FALSE\", \"0\"]:\n",
    "        return False\n",
    "    return default\n",
    "\n",
    "def get_decimal(node, xpath_expr):\n",
    "    \"\"\"\n",
    "    Return a float from first matched element text, or None if empty/invalid.\n",
    "    \"\"\"\n",
    "    txt = get_text(node, xpath_expr)\n",
    "    if not txt:\n",
    "        return None\n",
    "    try:\n",
    "        return float(txt.replace(\",\", \"\"))\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def extract_sec_header_info(file_path):\n",
    "    \"\"\"\n",
    "    Many filers put \"ACCESSION NUMBER:\" and \"FILED AS OF DATE:\" lines in <SEC-HEADER>.\n",
    "    We'll parse them out with regex. \n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    info = {\n",
    "        \"accession_number\": \"\",\n",
    "        \"date_filed\": None\n",
    "    }\n",
    "\n",
    "    # Accession\n",
    "    match_acc = re.search(r\"ACCESSION\\s+NUMBER:\\s*([^\\r\\n]+)\", raw, re.IGNORECASE)\n",
    "    if match_acc:\n",
    "        info[\"accession_number\"] = match_acc.group(1).strip()\n",
    "\n",
    "    # FILED AS OF DATE\n",
    "    match_filed = re.search(r\"FILED\\s+AS\\s+OF\\s+DATE:\\s*(\\d{8})\", raw, re.IGNORECASE)\n",
    "    if match_filed:\n",
    "        info[\"date_filed\"] = parse_date(match_filed.group(1).strip())\n",
    "\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f929a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edgar_submission(root):\n",
    "    \"\"\"\n",
    "    Extract top-level data from <edgarSubmission> based on the official schema (Section 3.4).\n",
    "    We'll attempt to fill the columns in the form_npx table.\n",
    "\n",
    "    We handle these sub-nodes:\n",
    "    - <headerData> -> <submissionType>, etc. \n",
    "    - <filerInfo> -> <registrantType>, <liveTestFlag>, <filer> -> <issuerCredentials>, ...\n",
    "    - <formData> -> <coverPage>, <amendmentInfo>, <reportingPerson>, <reportInfo>, <explanatoryInformation>, ...\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    # 1) submissionType => form_type \n",
    "    #    (N-PX or N-PX/A)\n",
    "    data[\"form_type\"] = get_text(root, \".//*[local-name()='submissionType']\")\n",
    "\n",
    "    # 2) Optional fields from <filerInfo>\n",
    "    #    e.g., <registrantType> => \"IM\" or \"RMIC\", <liveTestFlag> => \"LIVE\" or \"TEST\"\n",
    "    data[\"registrant_type\"] = get_text(root, \".//*[local-name()='registrantType']\")\n",
    "    data[\"live_test_flag\"] = get_text(root, \".//*[local-name()='liveTestFlag']\")\n",
    "\n",
    "    # <filer> -> <issuerCredentials> -> <cik>\n",
    "    data[\"cik\"] = get_text(root, \".//*[local-name()='issuerCredentials']/*[local-name()='cik']\")\n",
    "\n",
    "    # We won't parse <ccc> because it's masked by EDGAR\n",
    "\n",
    "    # 3) phone_number from <reportingPerson> or <contactPhoneNumber> from <contact>?\n",
    "    #    Official spec says <reportingPerson><phoneNumber> is mandatory. \n",
    "    data[\"phone_number\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='phoneNumber']\")\n",
    "\n",
    "    # 4) <investmentCompanyType> => \"N-1A\", \"N-2\", etc.\n",
    "    data[\"investment_company_type\"] = get_text(root, \".//*[local-name()='investmentCompanyType']\")\n",
    "\n",
    "    # 5) <periodOfReport>\n",
    "    data[\"conformed_period\"] = parse_date(get_text(root, \".//*[local-name()='periodOfReport']\"))\n",
    "\n",
    "    # 6) <coverPage><yearOrQuarter> => \"YEAR\" or \"QUARTER\"\n",
    "    data[\"year_or_quarter\"] = get_text(root, \".//*[local-name()='coverPage']/*[local-name()='yearOrQuarter']\")\n",
    "\n",
    "    # <coverPage><reportCalendarYear> \n",
    "    rcy = get_text(root, \".//*[local-name()='coverPage']/*[local-name()='reportCalendarYear']\")\n",
    "    data[\"report_calendar_year\"] = rcy if rcy.isdigit() else \"\"\n",
    "\n",
    "    # <coverPage><reportQuarterYear> (less common)\n",
    "    rqy = get_text(root, \".//*[local-name()='coverPage']/*[local-name()='reportQuarterYear']\")\n",
    "    data[\"report_quarter_year\"] = rqy if rqy.isdigit() else \"\"\n",
    "\n",
    "    # 7) <reportInfo> -> <reportType>, <confidentialTreatment> => \"Y\"/\"N\"\n",
    "    data[\"report_type\"] = get_text(root, \".//*[local-name()='reportInfo']/*[local-name()='reportType']\")\n",
    "    if not data[\"report_type\"]:\n",
    "        # fallback if not present\n",
    "        data[\"report_type\"] = \"INSTITUTIONAL MANAGER VOTING REPORT\"\n",
    "    conf_treat = get_text(root, \".//*[local-name()='reportInfo']/*[local-name()='confidentialTreatment']\")\n",
    "    data[\"confidential_treatment\"] = \"Y\" if conf_treat.upper() == \"Y\" else \"N\"\n",
    "\n",
    "    # <reportInfo> might contain <noticeExplanation> \n",
    "    data[\"notice_explanation\"] = get_text(root, \".//*[local-name()='reportInfo']/*[local-name()='explanationType']\")\n",
    "\n",
    "    # 8) \"is_notice_report\" => if the report_type includes \"NOTICE\"\n",
    "    data[\"is_notice_report\"] = \"NOTICE\" in data[\"report_type\"].upper()\n",
    "\n",
    "    # 9) <coverPage><amendmentInfo> -> isAmendment, amendmentNo, amendmentType, reasonForNonConfidentiality, ...\n",
    "    # According to the spec, \"amendmentInfo\" is mandatory for N-PX/A\n",
    "    # We parse it carefully:\n",
    "    isAmd = get_text(root, \".//*[local-name()='amendmentInfo']/*[local-name()='isAmendment']\")\n",
    "    data[\"is_amendment\"] = (isAmd.upper() == \"Y\")\n",
    "    amd_no = get_text(root, \".//*[local-name()='amendmentInfo']/*[local-name()='amendmentNo']\")\n",
    "    data[\"amendment_no\"] = int(amd_no) if amd_no.isdigit() else None\n",
    "\n",
    "    amd_type = get_text(root, \".//*[local-name()='amendmentInfo']/*[local-name()='amendmentType']\")\n",
    "    data[\"amendment_type\"] = amd_type if amd_type else None\n",
    "\n",
    "    # reasonForNonConfidentiality? <amendmentInfo><reasonForNonConfidentiality>\n",
    "    rfn = get_text(root, \".//*[local-name()='amendmentInfo']/*[local-name()='reasonForNonConfidentiality']\")\n",
    "    data[\"reason_for_non_confidentiality\"] = rfn if rfn else None\n",
    "\n",
    "    # 10) <reportingCrdNumber>, <reportingSecFileNumber>, <leiNumber> from <reportInfo> \n",
    "    data[\"reporting_crd_number\"] = get_text(root, \".//*[local-name()='reportingCrdNumber']\")\n",
    "    data[\"reporting_sec_file_number\"] = get_text(root, \".//*[local-name()='reportingSecFileNumber']\")\n",
    "    data[\"lei_number\"] = get_text(root, \".//*[local-name()='leiNumber']\")\n",
    "\n",
    "    # 11) <fileNumber> => \"sec_file_number\" in your DB schema \n",
    "    # Possibly found in the same <reportInfo> or <coverPage>. \n",
    "    data[\"sec_file_number\"] = get_text(root, \".//*[local-name()='fileNumber']\")\n",
    "\n",
    "    # 12) <explanatoryInformation> -> <explanatoryChoice> => 'Y'/'N', <explanatoryNotes> => big text?\n",
    "    data[\"explanatory_choice\"] = get_text(root, \".//*[local-name()='explanatoryInformation']/*[local-name()='explanatoryChoice']\")\n",
    "    if not data[\"explanatory_choice\"]:\n",
    "        data[\"explanatory_choice\"] = \"N\"\n",
    "    expl_notes = get_text(root, \".//*[local-name()='explanatoryInformation']/*[local-name()='explanatoryNotes']\")\n",
    "    data[\"explanatory_notes\"] = expl_notes if expl_notes else None\n",
    "\n",
    "    # 13) <summaryPage><otherIncludedManagersCount> => integer\n",
    "    oimc = get_text(root, \".//*[local-name()='summaryPage']/*[local-name()='otherIncludedManagersCount']\")\n",
    "    data[\"other_included_managers_count\"] = int(oimc) if oimc.isdigit() else 0\n",
    "\n",
    "    # 14) <reportingPerson> => name, phone, address (country especially)\n",
    "    data[\"reporting_person_name\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='name']\")\n",
    "    # phone_number we already set above\n",
    "    data[\"address_street1\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='street1']\")\n",
    "    data[\"address_street2\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='street2']\")\n",
    "    data[\"address_city\"]    = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='city']\")\n",
    "    data[\"address_state\"]   = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='stateOrCountry']\")\n",
    "    data[\"address_country\"] = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='stateOrCountry']\")  # or <country> if available\n",
    "    data[\"address_zip\"]     = get_text(root, \".//*[local-name()='reportingPerson']/*[local-name()='address']/*[local-name()='zipCode']\")\n",
    "\n",
    "    # 15) Signature Page\n",
    "    data[\"signatory_name\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txSignature']\")\n",
    "    data[\"signatory_name_printed\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txPrintedSignature']\")\n",
    "    data[\"signatory_title\"] = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txTitle']\")\n",
    "    sig_date = get_text(root, \".//*[local-name()='signaturePage']/*[local-name()='txAsOfDate']\")\n",
    "    data[\"signatory_date\"] = parse_date(sig_date)\n",
    "\n",
    "    # We'll fill in \"accession_number\" + \"date_filed\" from the <SEC-HEADER> outside. \n",
    "    data[\"accession_number\"] = \"\"\n",
    "    data[\"date_filed\"] = None\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d633ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_institutional_managers(root):\n",
    "    \"\"\"\n",
    "    Extract <summaryPage> -> <otherManagers2> -> <investmentManagers>,\n",
    "    or <otherManagersInfo>, or <otherManagers> block,\n",
    "    for the 'institutional_manager' table.\n",
    "\n",
    "    We'll gather:\n",
    "      serial_no, name, form13f_number, crd_number, sec_file_number, lei_number\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    manager_nodes = root.xpath(\".//*[local-name()='otherManagers2']//*[local-name()='investmentManagers']\")\n",
    "    if not manager_nodes:\n",
    "        manager_nodes = root.xpath(\".//*[local-name()='otherManager']\")\n",
    "\n",
    "    for mn in manager_nodes:\n",
    "        m_data = {\n",
    "            \"serial_no\": None,\n",
    "            \"name\": \"\",\n",
    "            \"form13f_number\": \"\",\n",
    "            \"crd_number\": \"\",\n",
    "            \"sec_file_number\": \"\",\n",
    "            \"lei_number\": \"\"\n",
    "        }\n",
    "\n",
    "        # serialNo\n",
    "        sn = mn.xpath(\".//*[local-name()='serialNo']/text()\")\n",
    "        if sn and sn[0].isdigit():\n",
    "            m_data[\"serial_no\"] = int(sn[0])\n",
    "\n",
    "        # name\n",
    "        nm = mn.xpath(\".//*[local-name()='name']/text()\")\n",
    "        if nm:\n",
    "            m_data[\"name\"] = nm[0].strip()\n",
    "\n",
    "        # form13FFileNumber\n",
    "        f13 = mn.xpath(\".//*[local-name()='form13FFileNumber']/text()\")\n",
    "        if f13:\n",
    "            m_data[\"form13f_number\"] = f13[0].strip()\n",
    "\n",
    "        # crdNumber\n",
    "        crd = mn.xpath(\".//*[local-name()='crdNumber']/text()\")\n",
    "        if crd:\n",
    "            m_data[\"crd_number\"] = crd[0].strip()\n",
    "\n",
    "        # secFileNumber\n",
    "        sfn = mn.xpath(\".//*[local-name()='secFileNumber']/text()\")\n",
    "        if sfn:\n",
    "            m_data[\"sec_file_number\"] = sfn[0].strip()\n",
    "\n",
    "        # leiNumber\n",
    "        lei = mn.xpath(\".//*[local-name()='leiNumber']/text()\")\n",
    "        if lei:\n",
    "            m_data[\"lei_number\"] = lei[0].strip()\n",
    "\n",
    "        results.append(m_data)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_series_info(root):\n",
    "    \"\"\"\n",
    "    <seriesPage> -> <seriesDetails> -> <seriesReports> -> <idOfSeries>, <nameOfSeries>, <leiOfSeries>\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    series_nodes = root.xpath(\".//*[local-name()='seriesReports']\")\n",
    "    for sn in series_nodes:\n",
    "        s_data = {}\n",
    "        s_data[\"series_code\"] = get_text(sn, \".//*[local-name()='idOfSeries']\")\n",
    "        s_data[\"series_name\"] = get_text(sn, \".//*[local-name()='nameOfSeries']\")\n",
    "        s_data[\"series_lei\"]  = get_text(sn, \".//*[local-name()='leiOfSeries']\")\n",
    "        results.append(s_data)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f13eb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_proxy_vote_table(vote_table_node, form_id):\n",
    "    \"\"\"\n",
    "    Extract each <proxyTable> from <proxyVoteTable>, returning rows for 'proxy_voting_record'\n",
    "    plus category links, manager links, and series links.\n",
    "\n",
    "    Official spec says mandatory elements in <proxyTable>:\n",
    "      <issuerName>, <meetingDate>, <voteDescription>, <voteCategories>, <sharesVoted>, <sharesOnLoan>.\n",
    "    Optional: <cusip>, <isin>, <figi>, <voteSource> (MATTER_DISCLOSURE_TYPE), ...\n",
    "    \"\"\"\n",
    "    pvr_rows = []\n",
    "    cat_link_rows = []\n",
    "    mgr_link_rows = []\n",
    "    ser_link_rows = []\n",
    "\n",
    "    proxy_tables = vote_table_node.xpath(\".//*[local-name()='proxyTable']\")\n",
    "    for pt in proxy_tables:\n",
    "        row = {\n",
    "            \"form_id\": form_id,\n",
    "            \"issuer_name\":  get_text(pt, \".//*[local-name()='issuerName']\"),\n",
    "            \"cusip\":        get_text(pt, \".//*[local-name()='cusip']\"),\n",
    "            \"isin\":         get_text(pt, \".//*[local-name()='isin']\"),\n",
    "            \"figi\":         get_text(pt, \".//*[local-name()='figi']\"),\n",
    "            \"meeting_date\": parse_date(get_text(pt, \".//*[local-name()='meetingDate']\")),\n",
    "            \"vote_description\": get_text(pt, \".//*[local-name()='voteDescription']\"),\n",
    "            \"proposed_by\":  get_text(pt, \".//*[local-name()='voteSource']\"),  # or <otherVoteDescription> if needed\n",
    "            \"shares_voted\": get_decimal(pt, \".//*[local-name()='sharesVoted'][1]\"),\n",
    "            \"shares_on_loan\": get_decimal(pt, \".//*[local-name()='sharesOnLoan'][1]\"),\n",
    "            \"vote_cast\": None,\n",
    "            \"vote_cast_shares\": None,\n",
    "            \"management_rec\": None,\n",
    "            \"other_notes\": None\n",
    "        }\n",
    "\n",
    "        # <vote><voteRecord> => howVoted (VOTE_TYPE), sharesVoted, managementRecommendation (MANAGEMENT_TYPE)\n",
    "        vote_records = pt.xpath(\".//*[local-name()='voteRecord']\")\n",
    "        if vote_records:\n",
    "            first_vr = vote_records[0]\n",
    "            row[\"vote_cast\"]       = get_text(first_vr, \".//*[local-name()='howVoted']\")\n",
    "            row[\"vote_cast_shares\"] = get_decimal(first_vr, \".//*[local-name()='sharesVoted']\")\n",
    "            row[\"management_rec\"]  = get_text(first_vr, \".//*[local-name()='managementRecommendation']\")\n",
    "\n",
    "        if len(vote_records) > 1:\n",
    "            row[\"other_notes\"] = f\"{len(vote_records)} total voteRecord items.\"\n",
    "\n",
    "        pvr_rows.append(row)\n",
    "\n",
    "        # <voteCategories> => <voteCategory> => <categoryType> => see 4.10 VOTE_CATEGORY\n",
    "        categories = pt.xpath(\".//*[local-name()='voteCategories']//*[local-name()='categoryType']/text()\")\n",
    "        for cat_str in categories:\n",
    "            cat_str_clean = cat_str.strip()\n",
    "            if cat_str_clean not in KNOWN_CATEGORIES:\n",
    "                new_id = len(KNOWN_CATEGORIES) + 1\n",
    "                KNOWN_CATEGORIES[cat_str_clean] = new_id\n",
    "                MATTER_CATEGORY_ROWS.append({\"category_id\": new_id, \"category_type\": cat_str_clean})\n",
    "            cat_id = KNOWN_CATEGORIES[cat_str_clean]\n",
    "            cat_link_rows.append({\"category_id\": cat_id})\n",
    "\n",
    "        # <voteManager><otherManagers><otherManager> => manager_id_code?\n",
    "        manager_vals = pt.xpath(\".//*[local-name()='voteManager']//*[local-name()='otherManager']/text()\")\n",
    "        for mv in manager_vals:\n",
    "            mgr_link_rows.append({\"manager_id_code\": mv.strip()})\n",
    "\n",
    "        # <voteSeries> => possibly links a series code\n",
    "        vs = get_text(pt, \".//*[local-name()='voteSeries']\")\n",
    "        if vs:\n",
    "            ser_link_rows.append({\"series_code\": vs})\n",
    "\n",
    "    return pvr_rows, cat_link_rows, mgr_link_rows, ser_link_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "449a5543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_blocks(file_path):\n",
    "    \"\"\"\n",
    "    Return a list of <XML> ... </XML> strings from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        text = f.read()\n",
    "    pattern = re.compile(r\"<XML>(.*?)</XML>\", re.IGNORECASE | re.DOTALL)\n",
    "    return pattern.findall(text)\n",
    "\n",
    "def parse_xml_fragment(xml_string):\n",
    "    \"\"\"\n",
    "    Attempt to parse an XML fragment with lxml.etree in recovery mode.\n",
    "    \"\"\"\n",
    "    parser = ET.XMLParser(recover=True, encoding=\"utf-8\")\n",
    "    try:\n",
    "        root = ET.fromstring(xml_string.encode(\"utf-8\"), parser=parser)\n",
    "        return root\n",
    "    except ET.XMLSyntaxError as e:\n",
    "        print(f\"  [Warning] parse error: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_npx_files(folder_path=\"npx_filings\"):\n",
    "    \"\"\"\n",
    "    1) For each .txt, parse <SEC-HEADER> for accession_number & date_filed\n",
    "    2) Extract <XML> blocks\n",
    "    3) For each block, if <edgarSubmission>, parse form-level data\n",
    "       and institutional managers, series info\n",
    "    4) If <proxyVoteTable>, parse proxy voting records\n",
    "    \"\"\"\n",
    "\n",
    "    all_files = os.listdir(folder_path)\n",
    "    txt_files = [f for f in all_files if f.lower().endswith(\".txt\")]\n",
    "\n",
    "    global_form_id = 1\n",
    "\n",
    "    for fname in txt_files:\n",
    "        file_path = os.path.join(folder_path, fname)\n",
    "        print(f\"\\nProcessing: {file_path}\")\n",
    "\n",
    "        # 1) parse <SEC-HEADER>\n",
    "        header_info = extract_sec_header_info(file_path)\n",
    "\n",
    "        # 2) extract <XML> blocks\n",
    "        xml_fragments = extract_xml_blocks(file_path)\n",
    "        if not xml_fragments:\n",
    "            print(\"  No <XML> blocks found.\")\n",
    "            continue\n",
    "\n",
    "        found_form_data = False\n",
    "\n",
    "        for frag in xml_fragments:\n",
    "            root = parse_xml_fragment(frag)\n",
    "            if root is None:\n",
    "                continue\n",
    "\n",
    "            # find <edgarSubmission>\n",
    "            es_nodes = root.xpath(\"//*[local-name()='edgarSubmission']\")\n",
    "            if es_nodes:\n",
    "                es = es_nodes[0]\n",
    "                submission_data = parse_edgar_submission(es)\n",
    "                # fill from <SEC-HEADER>\n",
    "                submission_data[\"accession_number\"] = header_info[\"accession_number\"]\n",
    "                submission_data[\"date_filed\"] = header_info[\"date_filed\"]\n",
    "                submission_data[\"form_id\"] = global_form_id\n",
    "\n",
    "                FORM_NPX_ROWS.append(submission_data)\n",
    "\n",
    "                # parse managers\n",
    "                im_list = parse_institutional_managers(es)\n",
    "                for im in im_list:\n",
    "                    row_im = {\n",
    "                        \"manager_id\": None,\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"serial_no\": im[\"serial_no\"],\n",
    "                        \"name\": im[\"name\"],\n",
    "                        \"form13f_number\": im[\"form13f_number\"],\n",
    "                        \"crd_number\": im[\"crd_number\"],\n",
    "                        \"sec_file_number\": im[\"sec_file_number\"],\n",
    "                        \"lei_number\": im[\"lei_number\"]\n",
    "                    }\n",
    "                    INSTITUTIONAL_MANAGER_ROWS.append(row_im)\n",
    "\n",
    "                # parse series\n",
    "                s_list = parse_series_info(es)\n",
    "                for s in s_list:\n",
    "                    row_s = {\n",
    "                        \"series_id\": None,\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"series_code\": s[\"series_code\"],\n",
    "                        \"series_name\": s[\"series_name\"],\n",
    "                        \"series_lei\": s[\"series_lei\"]\n",
    "                    }\n",
    "                    SERIES_ROWS.append(row_s)\n",
    "\n",
    "                found_form_data = True\n",
    "\n",
    "            # find <proxyVoteTable>\n",
    "            pvt = root.xpath(\"//*[local-name()='proxyVoteTable']\")\n",
    "            if pvt:\n",
    "                if not found_form_data:\n",
    "                    # If there's no <edgarSubmission>, create a minimal row in FORM_NPX\n",
    "                    # (some filers might only have a proxy table?)\n",
    "                    min_form = {\n",
    "                        \"form_id\": global_form_id,\n",
    "                        \"form_type\": \"\",\n",
    "                        \"registrant_type\": \"\",\n",
    "                        \"live_test_flag\": \"\",\n",
    "                        \"cik\": \"\",\n",
    "                        \"phone_number\": \"\",\n",
    "                        \"investment_company_type\": \"\",\n",
    "                        \"conformed_period\": None,\n",
    "                        \"year_or_quarter\": \"\",\n",
    "                        \"report_calendar_year\": \"\",\n",
    "                        \"report_quarter_year\": \"\",\n",
    "                        \"report_type\": \"\",\n",
    "                        \"confidential_treatment\": \"N\",\n",
    "                        \"notice_explanation\": \"\",\n",
    "                        \"is_notice_report\": False,\n",
    "                        \"is_amendment\": False,\n",
    "                        \"amendment_no\": None,\n",
    "                        \"amendment_type\": None,\n",
    "                        \"reason_for_non_confidentiality\": None,\n",
    "                        \"reporting_crd_number\": \"\",\n",
    "                        \"reporting_sec_file_number\": \"\",\n",
    "                        \"lei_number\": \"\",\n",
    "                        \"sec_file_number\": \"\",\n",
    "                        \"explanatory_choice\": \"N\",\n",
    "                        \"explanatory_notes\": \"\",\n",
    "                        \"other_included_managers_count\": 0,\n",
    "                        \"reporting_person_name\": \"\",\n",
    "                        \"address_street1\": \"\",\n",
    "                        \"address_street2\": \"\",\n",
    "                        \"address_city\": \"\",\n",
    "                        \"address_state\": \"\",\n",
    "                        \"address_country\": \"\",\n",
    "                        \"address_zip\": \"\",\n",
    "                        \"signatory_name\": \"\",\n",
    "                        \"signatory_name_printed\": \"\",\n",
    "                        \"signatory_title\": \"\",\n",
    "                        \"signatory_date\": None,\n",
    "                        \"accession_number\": header_info[\"accession_number\"],\n",
    "                        \"date_filed\": header_info[\"date_filed\"]\n",
    "                    }\n",
    "                    FORM_NPX_ROWS.append(min_form)\n",
    "                    found_form_data = True\n",
    "\n",
    "                # parse the actual proxy tables\n",
    "                sub_pvr, cat_links, mgr_links, ser_links = parse_proxy_vote_table(pvt[0], global_form_id)\n",
    "                PROXY_VOTING_RECORD_ROWS.extend(sub_pvr)\n",
    "                PROXY_VOTING_RECORD_CATEGORY_ROWS.extend(cat_links)\n",
    "                VOTING_RECORD_MANAGER_ROWS.extend(mgr_links)\n",
    "                VOTING_RECORD_SERIES_ROWS.extend(ser_links)\n",
    "\n",
    "        global_form_id += 1\n",
    "\n",
    "    print(\"\\nFinished parsing N-PX. Forms:\", len(FORM_NPX_ROWS),\n",
    "          \"Proxy Voting Rows:\", len(PROXY_VOTING_RECORD_ROWS))\n",
    "\n",
    "def write_to_csv(output_folder=\"output_csv\"):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # 1) form_npx\n",
    "    df_form = pd.DataFrame(FORM_NPX_ROWS).drop_duplicates()\n",
    "    csv_form = os.path.join(output_folder, \"form_npx.csv\")\n",
    "    df_form.to_csv(csv_form, index=False)\n",
    "    print(f\"Saved form_npx.csv: {len(df_form)} rows.\")\n",
    "\n",
    "    # 2) institutional_manager\n",
    "    df_im = pd.DataFrame(INSTITUTIONAL_MANAGER_ROWS).drop_duplicates()\n",
    "    df_im[\"manager_id\"] = df_im.index + 1\n",
    "    csv_im = os.path.join(output_folder, \"institutional_manager.csv\")\n",
    "    df_im.to_csv(csv_im, index=False)\n",
    "    print(f\"Saved institutional_manager.csv: {len(df_im)} rows.\")\n",
    "\n",
    "    # 3) series\n",
    "    df_s = pd.DataFrame(SERIES_ROWS).drop_duplicates()\n",
    "    df_s[\"series_id\"] = df_s.index + 1\n",
    "    csv_s = os.path.join(output_folder, \"series.csv\")\n",
    "    df_s.to_csv(csv_s, index=False)\n",
    "    print(f\"Saved series.csv: {len(df_s)} rows.\")\n",
    "\n",
    "    # 4) proxy_voting_record\n",
    "    df_pvr = pd.DataFrame(PROXY_VOTING_RECORD_ROWS)\n",
    "    df_pvr[\"vote_id\"] = df_pvr.index + 1\n",
    "    csv_pvr = os.path.join(output_folder, \"proxy_voting_record.csv\")\n",
    "    df_pvr.to_csv(csv_pvr, index=False)\n",
    "    print(f\"Saved proxy_voting_record.csv: {len(df_pvr)} rows.\")\n",
    "\n",
    "    # 5) matter_category\n",
    "    df_mc = pd.DataFrame(MATTER_CATEGORY_ROWS).drop_duplicates(subset=[\"category_type\"])\n",
    "    csv_mc = os.path.join(output_folder, \"matter_category.csv\")\n",
    "    df_mc.to_csv(csv_mc, index=False)\n",
    "    print(f\"Saved matter_category.csv: {len(df_mc)} rows.\")\n",
    "\n",
    "    # 6) proxy_voting_record_category\n",
    "    if PROXY_VOTING_RECORD_CATEGORY_ROWS:\n",
    "        df_pvrc = []\n",
    "        # For demonstration, we link each category row to a naive vote_id\n",
    "        # In production, you'd store the index or something that knows which row it belongs to.\n",
    "        # We'll do a round-robin approach:\n",
    "        for idx, cat_row in enumerate(PROXY_VOTING_RECORD_CATEGORY_ROWS):\n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            if vote_id:\n",
    "                df_pvrc.append({\"vote_id\": vote_id, \"category_id\": cat_row[\"category_id\"]})\n",
    "        df_pvrc = pd.DataFrame(df_pvrc).drop_duplicates()\n",
    "    else:\n",
    "        df_pvrc = pd.DataFrame(columns=[\"vote_id\", \"category_id\"])\n",
    "\n",
    "    csv_pvrc = os.path.join(output_folder, \"proxy_voting_record_category.csv\")\n",
    "    df_pvrc.to_csv(csv_pvrc, index=False)\n",
    "    print(f\"Saved proxy_voting_record_category.csv: {len(df_pvrc)} rows.\")\n",
    "\n",
    "    # 7) voting_record_manager\n",
    "    if VOTING_RECORD_MANAGER_ROWS:\n",
    "        df_vrm = []\n",
    "        for idx, mgr_row in enumerate(VOTING_RECORD_MANAGER_ROWS):\n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            manager_id = (idx % len(df_im)) + 1 if len(df_im) else None\n",
    "            df_vrm.append({\"vote_id\": vote_id, \"manager_id\": manager_id})\n",
    "        df_vrm = pd.DataFrame(df_vrm).drop_duplicates()\n",
    "    else:\n",
    "        df_vrm = pd.DataFrame(columns=[\"vote_id\",\"manager_id\"])\n",
    "\n",
    "    csv_vrm = os.path.join(output_folder, \"voting_record_manager.csv\")\n",
    "    df_vrm.to_csv(csv_vrm, index=False)\n",
    "    print(f\"Saved voting_record_manager.csv: {len(df_vrm)} rows.\")\n",
    "\n",
    "    # 8) voting_record_series\n",
    "    if VOTING_RECORD_SERIES_ROWS:\n",
    "        df_vrs = []\n",
    "        for idx, srow in enumerate(VOTING_RECORD_SERIES_ROWS):\n",
    "            vote_id = (idx % len(df_pvr)) + 1 if len(df_pvr) else None\n",
    "            series_id = (idx % len(df_s)) + 1 if len(df_s) else None\n",
    "            df_vrs.append({\"vote_id\": vote_id, \"series_id\": series_id})\n",
    "        df_vrs = pd.DataFrame(df_vrs).drop_duplicates()\n",
    "    else:\n",
    "        df_vrs = pd.DataFrame(columns=[\"vote_id\",\"series_id\"])\n",
    "\n",
    "    csv_vrs = os.path.join(output_folder, \"voting_record_series.csv\")\n",
    "    df_vrs.to_csv(csv_vrs, index=False)\n",
    "    print(f\"Saved voting_record_series.csv: {len(df_vrs)} rows.\")\n",
    "\n",
    "    print(\"\\nAll CSVs have been written. Check your output folder to confirm.\")\n",
    "\n",
    "\n",
    "def run_all(folder_path=\"npx_filings\", output_folder=\"output_csv\"):\n",
    "    \"\"\"\n",
    "    High-level convenience function to:\n",
    "      1) Reset global data \n",
    "      2) Parse the N-PX .txt files\n",
    "      3) Write results to CSV\n",
    "    \"\"\"\n",
    "    global FORM_NPX_ROWS, INSTITUTIONAL_MANAGER_ROWS, SERIES_ROWS\n",
    "    global PROXY_VOTING_RECORD_ROWS, MATTER_CATEGORY_ROWS\n",
    "    global PROXY_VOTING_RECORD_CATEGORY_ROWS, VOTING_RECORD_MANAGER_ROWS, VOTING_RECORD_SERIES_ROWS\n",
    "    global KNOWN_CATEGORIES\n",
    "\n",
    "    # Reset\n",
    "    FORM_NPX_ROWS = []\n",
    "    INSTITUTIONAL_MANAGER_ROWS = []\n",
    "    SERIES_ROWS = []\n",
    "    PROXY_VOTING_RECORD_ROWS = []\n",
    "    MATTER_CATEGORY_ROWS = []\n",
    "    PROXY_VOTING_RECORD_CATEGORY_ROWS = []\n",
    "    VOTING_RECORD_MANAGER_ROWS = []\n",
    "    VOTING_RECORD_SERIES_ROWS = []\n",
    "    KNOWN_CATEGORIES = {}\n",
    "\n",
    "    # Parse\n",
    "    process_npx_files(folder_path)\n",
    "    # Write\n",
    "    write_to_csv(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4b8e10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: npx_filings\\2024-07-01_N-PX_0001536924-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-03_N-PX_0001896711-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-12_N-PX_0001214659-24-012298.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-12_N-PX_0001912297-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-15_N-PX_0001172661-24-002773.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-16_N-PX_0001085146-24-003046.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-16_N-PX_0001376474-24-000319.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-16_N-PX_0001754960-24-000252.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-17_N-PX_0001915315-24-000003.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-23_N-PX_0000720064-24-000002.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-23_N-PX_0000810305-24-000010.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-24_N-PX_0001172661-24-002891.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-26_N-PX_0001021408-24-000694.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-26_N-PX_0001172661-24-002946.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-30_N-PX_0001044929-24-000010.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-30_N-PX_0001843110-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-31_N-PX_0001722641-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-07-31_N-PX_0001927175-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-01_N-PX_0001172661-24-003023.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-01_N-PX_0001398344-24-013486.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-02_N-PX_0001085146-24-003462.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-02_N-PX_0001757128-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-05_N-PX_0001141802-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-05_N-PX_0001580642-24-004158.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-05_N-PX_0001951757-24-000676.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-05_N-PX_0001951757-24-000677.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-06_N-PX_0001951757-24-000708.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-07_N-PX_0001438934-24-000095.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-08_N-PX_0001213900-24-066590.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-08_N-PX_0001730293-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-08_N-PX_0001731497-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-09_N-PX_0001085146-24-003669.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-09_N-PX_0001104659-24-087663.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-09_N-PX_0001104659-24-087832.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-09_N-PX_0001775530-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-09_N-PX_0001840501-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-12_N-PX_0001437749-24-026024.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-12_N-PX_0001438934-24-000146.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-14_N-PX_0001725547-24-003928.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-14_N-PX_0001765380-24-000343.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-14_N-PX_0001822531-24-000008.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-15_N-PX_0001540235-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-15_N-PX_0001692227-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-16_N-PX_0001021408-24-002019.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-16_N-PX_0001021408-24-002025.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-16_N-PX_0001563690-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-20_N-PX_0001172661-24-003627.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-20_N-PX_0001540944-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-20_N-PX_0001580642-24-004662.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001085146-24-004090.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001330463-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001606507-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001725547-24-004122.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001951757-24-000869.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001951757-24-000881.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001951757-24-000890.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0001991301-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-21_N-PX_0002030060-24-000009.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-22_N-PX_0001080107-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-22_N-PX_0001104659-24-091897.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-22_N-PX_0001123292-24-000230.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-22_N-PX_0001580642-24-004748.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-22_N-PX_0001973783-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-23_N-PX_0001021408-24-003145.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-23_N-PX_0001021408-24-003735.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-23_N-PX_0001085146-24-004199.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-23_N-PX_0001256071-24-000011.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-23_N-PX_0001454984-24-000010.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-23_N-PX_0001674787-24-000001.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-23_N-PX_0001842572-24-000008.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0000312348-24-000032.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0000919574-24-004866.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001085146-24-004205.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001085146-24-004218.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001135428-24-000141.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001172661-24-003707.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001591556-24-000018.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001667731-24-000565.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001705819-24-000051.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-26_N-PX_0001845743-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0000897378-24-000011.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001013594-24-000698.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001021408-24-004692.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001021408-24-004738.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001021408-24-004844.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001021408-24-005076.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001082215-24-000009.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001085227-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001104659-24-093237.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001104659-24-093369.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001104659-24-093463.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001172661-24-003746.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001398344-24-015621.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001650303-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001697766-24-000001.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001798367-24-000015.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0001846352-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-27_N-PX_0002010644-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0000067590-24-000085.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0000356787-24-000056.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0000752798-24-000007.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0000881432-24-000013.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0000930413-24-002506.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001021408-24-005236.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001021408-24-005385.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001085146-24-004276.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001178913-24-002827.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001420506-24-001838.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001438934-24-001035.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001518235-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001523847-24-000003.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001623632-24-001418.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001623632-24-001501.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-28_N-PX_0001795594-24-000008.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000005272-24-000093.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000021175-24-000072.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000902664-24-005310.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000919574-24-005089.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000928816-24-001291.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000930413-24-002605.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000930413-24-002632.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000930413-24-002638.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0000947871-24-000733.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001021408-24-005720.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001021408-24-005793.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001021408-24-006005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001050463-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001084683-24-000008.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001085146-24-004409.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001104659-24-094321.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001104659-24-094324.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001104659-24-094403.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001104659-24-094461.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001104659-24-094473.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001104659-24-094716.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001206774-24-000864.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001206774-24-000867.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001206774-24-000871.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001323645-24-000011.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001398344-24-016813.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001438934-24-001091.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001438934-24-001195.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001580642-24-004948.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001591546-24-000013.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001633864-24-000005.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-29_N-PX_0001636441-24-000006.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0000919574-24-005165.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0000919574-24-005220.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001021408-24-006360.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001021408-24-006390.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001021408-24-006488.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001021408-24-006614.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001021408-24-006617.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001021408-24-006673.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001021408-24-006715.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001104659-24-095123.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001104659-24-095427.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001104659-24-095629.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001140361-24-039526.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001140361-24-039549.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001172661-24-003873.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001172661-24-003926.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001213900-24-074309.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001398344-24-016928.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001398344-24-016945.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001398344-24-016999.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001420506-24-001947.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001438934-24-001521.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001438934-24-001537.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001438934-24-001631.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001438934-24-001656.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001438934-24-001731.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001539497-24-001830.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001648711-24-000019.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001829126-24-006015.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001908938-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001941040-24-000410.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001964897-24-000002.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0001993888-24-000009.txt\n",
      "\n",
      "Processing: npx_filings\\2024-08-30_N-PX_0002011250-24-000010.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0000919574-24-005338.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001021408-24-007076.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001021408-24-007161.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001104659-24-095771.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001104659-24-095806.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001104659-24-095858.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001104659-24-096106.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001420506-24-002020.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001438934-24-001768.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001438934-24-001913.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001438934-24-001919.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0001493152-24-034718.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-03_N-PX_0002000324-24-002628.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-10_N-PX_0001162044-24-001017.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-10_N-PX_0001438934-24-001974.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-13_N-PX_0001172661-24-004082.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-25_N-PX_0001398344-24-018155.txt\n",
      "\n",
      "Processing: npx_filings\\2024-09-30_N-PX_0001567738-24-000009.txt\n",
      "\n",
      "Processing: npx_filings\\2024-10-01_N-PX_0001736648-24-000004.txt\n",
      "\n",
      "Processing: npx_filings\\2024-10-02_N-PX_0002011188-24-000008.txt\n",
      "\n",
      "Finished parsing N-PX. Forms: 200 Proxy Voting Rows: 318383\n",
      "Saved form_npx.csv: 200 rows.\n",
      "Saved institutional_manager.csv: 42 rows.\n",
      "Saved series.csv: 204 rows.\n",
      "Saved proxy_voting_record.csv: 318383 rows.\n",
      "Saved matter_category.csv: 16 rows.\n",
      "Saved proxy_voting_record_category.csv: 330998 rows.\n",
      "Saved voting_record_manager.csv: 59364 rows.\n",
      "Saved voting_record_series.csv: 248035 rows.\n",
      "\n",
      "All CSVs have been written. Check your output folder to confirm.\n"
     ]
    }
   ],
   "source": [
    "run_all(\"npx_filings\", \"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
