{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa231e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "#  N‑PX filing loader – SageMaker / S3 / RDS edition (April 2025)\n",
    "# ---------------------------------------------------------------\n",
    "\"\"\"\n",
    "Runs inside an **Amazon SageMaker** notebook (or Studio) that sits in a\n",
    "VPC.  Filings live in an S3 bucket _in another VPC_, and results are\n",
    "written to a PostgreSQL RDS instance (also usually in that VPC or peered\n",
    "one).\n",
    "\n",
    "👷 **What this file does**\n",
    "1.  Streams each `*.txt` filing directly from S3 (zero local disk I/O)\n",
    "   via `s3fs`.\n",
    "2.  Parses, batches, and inserts into RDS exactly as before – we reuse\n",
    "   all helper logic.\n",
    "3.  Detects at runtime whether we are reading from **local disk**\n",
    "   (`FILINGS_DIR`) or an S3 bucket (env var `S3_BUCKET`).  Same business\n",
    "   code works in either context.\n",
    "\n",
    "👮 **Infra prerequisites** (one‑time):\n",
    "- The SageMaker execution‑role must have `s3:GetObject` permission on the\n",
    "  bucket/prefix that holds the filings.\n",
    "- The notebook must run in a subnet that can reach the RDS endpoint\n",
    "  (same VPC, VPC‑peered, or via AWS PrivateLink).  Security group allows\n",
    "  outbound 5432.\n",
    "- RDS has a SG rule allowing inbound 5432 from the notebook’s security\n",
    "  group.\n",
    "- Environment variables `PGHOST PGPORT PGUSER PGPASSWORD PGDATABASE`\n",
    "  should be either stored as **Secrets Manager** secrets retrieved by the\n",
    "  notebook or pre‑set via the lifecycle config.  For demo they are still\n",
    "  prompted if missing.\n",
    "\n",
    "Install dependencies in a SageMaker conda cell:\n",
    "```bash\n",
    "pip install s3fs psycopg2-binary tqdm python-dotenv lxml\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1 · Imports & configuration\n",
    "# ---------------------------------------------------------------\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import datetime as dt\n",
    "import getpass\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "\n",
    "import boto3\n",
    "import s3fs                     # <‑‑ new\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values, DictCursor\n",
    "from lxml import etree as ET\n",
    "from tqdm.auto import tqdm      # auto picks notebook/console\n",
    "from decimal import Decimal\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- logging --------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    ")\n",
    "\n",
    "# --- environment ----------------------------------------------------------\n",
    "FILINGS_DIR = Path(\"npx_filings\")          # ignored when S3 is used\n",
    "S3_BUCKET  = os.getenv(\"S3_BUCKET\")        # if set ➟ use S3\n",
    "S3_PREFIX  = os.getenv(\"S3_PREFIX\", \"\")   # optional key prefix\n",
    "\n",
    "# --- SEC regex helpers ----------------------------------------------------\n",
    "DATE_FMTS    = (\"%Y-%m-%d\", \"%m/%d/%Y\", \"%m-%d-%Y\", \"%Y%m%d\")\n",
    "DEC_CLEAN_RE = re.compile(r\"[^\\d\\.\\-]\")\n",
    "SEC_HEADER_RE = re.compile(\n",
    "    r\"ACCESSION\\s+NUMBER:\\s*(?P<acc>[^\\r\\n]+).*?\" \\\n",
    "    r\"FILED\\s+AS\\s+OF\\s+DATE:\\s*(?P<filed>\\d{8})\",\n",
    "    re.I | re.S,\n",
    ")\n",
    "XML_BLOCK_RE = re.compile(r\"(<\\?xml.*?</edgarSubmission>)\", re.I | re.S)\n",
    "XML_PARSER   = ET.XMLParser(recover=True, encoding=\"utf-8\", huge_tree=True)\n",
    "\n",
    "# --- credentials ----------------------------------------------------------\n",
    "load_dotenv()\n",
    "for var in (\"PGHOST\", \"PGPORT\", \"PGUSER\", \"PGPASSWORD\", \"PGDATABASE\"):\n",
    "    if not os.getenv(var):\n",
    "        os.environ[var] = (\n",
    "            getpass.getpass(f\"{var}: \") if var == \"PGPASSWORD\" else input(f\"{var}: \")\n",
    "        )\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2 · Helper functions (unchanged except Decimal + S3)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "@functools.lru_cache(maxsize=1024)\n",
    "def parse_date(s: str | None):\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    for fmt in DATE_FMTS:\n",
    "        try:\n",
    "            return dt.datetime.strptime(s, fmt).date()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    logging.warning(\"Un‑parsable date: %s\", s)\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_decimal(s: str | None):\n",
    "    if not s:\n",
    "        return None\n",
    "    txt = DEC_CLEAN_RE.sub(\"\", s)\n",
    "    try:\n",
    "        return Decimal(txt) if txt else None\n",
    "    except ArithmeticError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get(node, xp, sl=None):\n",
    "    res = node.xpath(xp)\n",
    "    if not res:\n",
    "        return \"\"\n",
    "    txt = res[0] if isinstance(res[0], str) else res[0].text or \"\"\n",
    "    txt = txt.strip()\n",
    "    return txt[:sl] if sl else txt\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3 · SEC header & XML extraction (text‑based)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def extract_sec_header_from_text(txt: str):\n",
    "    head = txt[:4000]\n",
    "    m = SEC_HEADER_RE.search(head)\n",
    "    if not m:\n",
    "        return {\"accession_number\": \"\", \"date_filed\": None}\n",
    "    return {\n",
    "        \"accession_number\": m.group(\"acc\").strip(),\n",
    "        \"date_filed\": dt.datetime.strptime(m.group(\"filed\"), \"%Y%m%d\").date(),\n",
    "    }\n",
    "\n",
    "\n",
    "def xml_blocks_from_text(txt: str):\n",
    "    return XML_BLOCK_RE.findall(txt)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4 · All parsing & DB helpers (identical to previous patch)\n",
    "# ---------------------------------------------------------------\n",
    "#   – for brevity not repeated here; see earlier code –\n",
    "#   keep: parse_edgar_submission, parse_institutional_managers,\n",
    "#         parse_series_info, parse_proxy_tables, pg_conn, upsert_category\n",
    "\n",
    "# (Insert the same definitions from the prior patched file.)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5 · Loader that consumes **raw text** (works for S3 or disk)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "VOTE_COLS = [\n",
    "    \"form_id\", \"issuer_name\", \"cusip\", \"isin\", \"figi\",\n",
    "    \"meeting_date\", \"vote_description\", \"proposed_by\",\n",
    "    \"shares_voted\", \"shares_on_loan\",\n",
    "    \"vote_cast\", \"vote_cast_shares\", \"management_rec\",\n",
    "    \"other_notes\",\n",
    "]\n",
    "\n",
    "def load_filing_text(filename: str, txt: str, conn, cat_cache):\n",
    "    hdr   = extract_sec_header_from_text(txt)\n",
    "    frags = xml_blocks_from_text(txt)\n",
    "    if not frags:\n",
    "        logging.warning(\"%s – no <edgarSubmission> blocks\", filename)\n",
    "        return\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        form_map, mgr_map, ser_map = {}, defaultdict(dict), defaultdict(dict)\n",
    "        for frag in frags:\n",
    "            root = ET.fromstring(frag.encode(), parser=XML_PARSER)\n",
    "            #  --- IDENTICAL inner logic as before (forms/managers/series/votes) ---\n",
    "            #  Copy the body of load_filing() from the patched version, but\n",
    "            #  replace 'path.name' with 'filename' in log messages.\n",
    "            # ------------------------------------------------------------------\n",
    "            #  (omitted here to keep this snippet concise.)\n",
    "            # ------------------------------------------------------------------\n",
    "        logging.info(\"%s – committed\", filename)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 6 · Iterators over local disk or S3\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def iter_local_files() -> tuple[str, str]:\n",
    "    for path in sorted(FILINGS_DIR.glob(\"*.txt\")):\n",
    "        yield path.name, path.read_text(\"utf-8\", \"replace\")\n",
    "\n",
    "\n",
    "def iter_s3_files(bucket: str, prefix: str) -> tuple[str, str]:\n",
    "    fs = s3fs.S3FileSystem(anon=False)     # IAM role supplies creds\n",
    "    keys = fs.ls(f\"{bucket}/{prefix}\")\n",
    "    for key in tqdm(keys, desc=\"S3 keys\"):\n",
    "        # key is like 'bucket/prefix/file.txt'; we need only the object key\n",
    "        obj_key = \"/\".join(key.split(\"/\")[1:]) if key.startswith(bucket) else key\n",
    "        with fs.open(f\"{bucket}/{obj_key}\", \"r\") as f:\n",
    "            yield obj_key.split(\"/\")[-1], f.read()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 7 · Orchestrator – chooses S3 vs local and streams to RDS\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def run_loader():\n",
    "    if S3_BUCKET:\n",
    "        logging.info(\"Running in S3 mode: bucket=%s prefix='%s'\", S3_BUCKET, S3_PREFIX)\n",
    "        file_iter = iter_s3_files(S3_BUCKET, S3_PREFIX)\n",
    "    else:\n",
    "        file_iter = iter_local_files()\n",
    "\n",
    "    with pg_conn() as conn:\n",
    "        cat_cache = {}\n",
    "        for fname, txt in tqdm(list(file_iter), desc=\"Filings\"):\n",
    "            try:\n",
    "                load_filing_text(fname, txt, conn, cat_cache)\n",
    "                conn.commit()\n",
    "            except Exception:\n",
    "                logging.exception(\"‼️  %s failed – rolled back\", fname)\n",
    "                conn.rollback()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_loader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3103d10",
   "metadata": {},
   "source": [
    "You only need to “splice in” whatever the **other notebook does that our loader doesn’t already do**.\\\n",
    "From what you’ve said, that extra value is *discovering which S3 objects are actually N‑PX filings* among a lot of miscellaneous stuff.\\\n",
    "Everything else—parsing, batching, writing to RDS—is already in the patched loader.\n",
    "\n",
    "Below is a simple roadmap plus a 10‑line code stub that shows exactly where to hook the discovery logic in.\\\n",
    "(You can keep both notebooks separate: just `import` the discovery function, or paste it into the small area marked ▸.)\n",
    "\n",
    "---\n",
    "\n",
    "## 1 · Identify the one function you want\n",
    "\n",
    "Open the other notebook and find (or create) a helper that returns the keys you need, e.g.:\n",
    "\n",
    "```python\n",
    "def list_npx_keys(bucket: str, prefix: str) -> list[str]:\n",
    "    \\\"\\\"\\\"Return *only* the S3 keys that are text N‑PX filings.\\\"\\\"\\\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "Typical filters inside might be:\n",
    "\n",
    "- key ends with `.txt`\n",
    "- key path includes `/npx/`\n",
    "- object metadata tag `sec-form = N-PX`\n",
    "\n",
    "---\n",
    "\n",
    "## 2 · Import (or copy) that function into the loader\n",
    "\n",
    "### Option A – import the notebook as a module\n",
    "\n",
    "- In the other notebook add:\n",
    "\n",
    "  ```python\n",
    "  # at bottom\n",
    "  if __name__ == \"__main__\":\n",
    "      pass\n",
    "  ```\n",
    "\n",
    "- Save it as `npx_s3_discovery.py` in the same directory.\n",
    "\n",
    "- In our loader add:\n",
    "\n",
    "  ```python\n",
    "  from npx_s3_discovery import list_npx_keys\n",
    "  ```\n",
    "\n",
    "### Option B – copy the few lines directly\n",
    "\n",
    "Paste the function into **Section 6** of the loader (right before the iterators).\n",
    "\n",
    "---\n",
    "\n",
    "## 3 · Replace the simple iterator\n",
    "\n",
    "Open the canvas loader and swap the body of `iter_s3_files()` for this:\n",
    "\n",
    "```python\n",
    "def iter_s3_files(bucket: str, prefix: str) -> tuple[str, str]:\n",
    "    \\\"\\\"\\\"Yield (filename, file‑text) only for true N‑PX filings.\\\"\\\"\\\"\n",
    "    fs = s3fs.S3FileSystem(anon=False)\n",
    "    keys = list_npx_keys(bucket, prefix)          # ▸ your smarter filter\n",
    "    for key in tqdm(keys, desc=\\\"N‑PX keys\\\"):\n",
    "        with fs.open(f\\\"{bucket}/{key}\\\", \\\"r\\\") as f:\n",
    "            yield key.split(\\\"/\\\")[-1], f.read()\n",
    "```\n",
    "\n",
    "That is literally the only edit you need—everything else stays the same.\n",
    "\n",
    "---\n",
    "\n",
    "## 4 · Run it\n",
    "\n",
    "```python\n",
    "%env S3_BUCKET=my-sec-bucket\n",
    "%env S3_PREFIX=raw/sec-filings/\n",
    "run_loader()           # progress bar should now show only true N‑PX files\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Quick checklist\n",
    "\n",
    "| Step                  | What to confirm                                                                                |\n",
    "| --------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| Import works          | `from npx_s3_discovery import list_npx_keys` runs with no `ModuleNotFoundError`.               |\n",
    "| Function returns keys | `print(list_npx_keys(bucket, prefix)[:3])` shows paths ending in `.txt`.                       |\n",
    "| Loader picks them up  | The progress bar label changed from **“S3 keys”** to **“N‑PX keys”** and counts fewer objects. |\n",
    "\n",
    "After that, the loader’s existing parsing/batching logic finishes the job and writes to RDS as before—no other integration required.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
