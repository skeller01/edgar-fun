# -*- coding: utf-8 -*-
"""
SEC Form Nâ€‘PX loader (schema 2025â€‘05â€‘14) â€“ **corrected edition**
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
This script parses .txt filings, extracts XML, and loads the data into a
Postgres (AWSÂ RDSâ€‘ready) instance that conforms to the **2025â€‘05â€‘14 relational
schema** (`npx_schema_05152025.txt`).

Corrections & improvements vs. the draft supplied by the user
-------------------------------------------------------------
* **Implemented** `_scan_existing_series_classes` to honour *includeâ€‘allâ€‘series*
  filings (prevents `AttributeError`).
* **`clean_period`** now recognises all fiscal quarter ends (3â€‘31,Â 6â€‘30,
  9â€‘30,Â 12â€‘31).
* Centralised column handlingÂ â€“ every INSERT derives its VALUES tuple directly
  from the authoritative `*_COLS` constants; no more silent drift.
* Reâ€‘worked the **JSON logger** to avoid touching private `logging` internals.
* Streamâ€‘based XML fragment detection no longer depends on `<?xml` being on a
  single line â€“ we start buffering as soon as `<edgarSubmission` or
  `<proxyVoteTable` appears.
* Renamed `MAX_DEC` â†’ `MAX_DEC_MAGNITUDE` for clarity.
* Misc: integer coercion for `year_or_quarter`, etc.; minor microâ€‘optimisations
  and clearer variable names.

Environment & usage
-------------------
Requires
    * PythonÂ â‰¥Â 3.9
    * `lxml`, `psycopg2â€‘binary`, `pythonâ€‘dotenv`

Run
    python npx_loader_updated_corrected.py <folder_with_txt_filings> [flush_every]
"""
from __future__ import annotations

import datetime as dt
import io
import logging
import os
import re
import sys
import tempfile
from collections import defaultdict
from decimal import Decimal
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from zoneinfo import ZoneInfo

import lxml.etree as ET
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from pythonjsonlogger import jsonlogger  # type: ignore

    class _JsonFmt(jsonlogger.JsonFormatter):
        """Minimal JSON formatter that keeps public logging API intact."""

        def __init__(self):  # noqa: D401 â€“ nonâ€‘docstring
            super().__init__("%(asctime)sZ %(levelname)s %(name)s %(message)s")
            # Force UTC ISOâ€‘8601 timestamp
            self.converter = lambda *args: dt.datetime.now(tz=ZoneInfo("UTC")).timetuple()

    _handler: logging.Handler = logging.StreamHandler()
    _handler.setFormatter(_JsonFmt())
except ImportError:  # fallback to plainâ€‘text â€“ still ISO & UTC
    _handler = logging.StreamHandler()
    _handler.setFormatter(
        logging.Formatter("%(asctime)sZ [%(levelname)s] %(name)s: %(message)s", datefmt="%Y-%m-%dT%H:%M:%S")
    )

if not logging.root.handlers:
    logging.root.addHandler(_handler)
logging.root.setLevel(logging.INFO)
logger = logging.getLogger("npx_loader")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATE_FMTS: Tuple[str, ...] = (
    "%m/%d/%Y",
    "%Y-%m-%d",
    "%m-%d-%Y",
    "%Y%m%d",
    "%Y/%m/%d",
    "%m/%d/%y",
    "%m-%d-%y",
    "%d-%b-%Y",
    "%d-%B-%Y",
)

DEC_RE = re.compile(r"[^0-9.\-]")
MAX_DEC_MAGNITUDE = Decimal("1e14")  # 100Â trillion (absolute)
XML_START_RE = re.compile(r"<\s*(edgarSubmission|[A-Za-z]+:proxyVoteTable|proxyVoteTable)\b", re.I)
END_TAG_RE = re.compile(r"</(?:edgarSubmission|[A-Za-z]+:proxyVoteTable|proxyVoteTable)>", re.I)
ACC_RE = re.compile(r"ACCESSION\s+NUMBER:\s*([^\r\n]+)", re.I)
FILED_RE = re.compile(r"FILED\s+AS\s+OF\s+DATE:\s*(\d{6,8})", re.I)
PARSER = ET.XMLParser(recover=True, huge_tree=True, resolve_entities=False)

# Column maps (authoritative)
FORM_COLS = (
    "reporting_person_name","phone_number","address_street1","address_street2","address_city","address_state","address_zip","accession_number","is_parsable","cik","period_of_report_raw","period_of_report","date_filed","report_type","form_type","sec_file_number","crd_number","sec_file_number_other","lei_number","investment_company_type","confidential_treatment","is_notice_report","explanatory_choice","other_included_managers_count","series_count","is_amendment","amendment_no","amendment_type","notice_explanation","date_expired_denied_raw","date_reported_raw","year_or_quarter","report_calendar_year","report_quarter_year","explanatory_notes","signatory_name","signatory_name_printed","signatory_title","tx_as_of_date_raw","signatory_date",
)
MGR_COLS   = ("form_id","serial_no","name","form13f_number","crd_number","sec_file_number","lei_number")
SER_COLS   = ("form_id","series_code","series_name","series_lei")
SC_COLS    = ("series_id","class_id","class_name")
OP_COLS    = ("form_id","ica_form13f","crd_number","sec_file_number","lei_number","name")
VOTE_COLS  = (
    "form_id","issuer_name","cusip","isin","figi","meeting_date_raw","meeting_date","vote_description","proposed_by","shares_voted","shares_on_loan","vote_cast","vote_cast_shares","management_rec","other_notes",
)
VRC_COLS   = ("vote_id","category_id")
VRM_COLS   = ("vote_id","manager_id")
VRS_COLS   = ("vote_id","series_id")
CAT_COLS   = ("category_type",)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def excel_ordinal_to_date(serial: int) -> dt.date:
    if serial <= 0:
        raise ValueError("Serial must be positive")
    # Excel bug: 1900â€‘02â€‘29 (serialÂ 60) never existed
    if serial >= 60:
        serial -= 1
    return dt.date(1899, 12, 30) + dt.timedelta(days=serial)


def pdate(s: Optional[str]):
    if not s or not s.strip():
        return None
    s = s.strip()
    for fmt in DATE_FMTS:
        try:
            return dt.datetime.strptime(s, fmt).date()
        except ValueError:
            continue
    if s.isdigit() and len(s) >= 5:
        try:
            return excel_ordinal_to_date(int(s))
        except Exception:  # noqa: BLE001 â€“ quiet fail
            pass
    logger.debug("unparseable date %s", s)
    return None


def clean_period(raw: Optional[str]) -> Optional[dt.date]:
    """Return quarterâ€‘end date if *raw* text parses to any 31â€‘Mar/30â€‘Jun/30â€‘Sep/31â€‘Dec."""
    if not raw:
        return None
    raw = raw.strip()
    if raw.isdigit() and len(raw) >= 5:
        try:
            d = excel_ordinal_to_date(int(raw))
        except Exception:  # noqa: BLE001
            return None
    else:
        d = pdate(raw)
    if not d:
        return None
    quarter_ends = {(3, 31), (6, 30), (9, 30), (12, 31)}
    if (d.month, d.day) in quarter_ends:
        return d
    return None


def txt(node: ET._Element, xp: str, sl: Optional[int] = None) -> str:
    """Namespaceâ€‘agnostic `text()` extractor with optional sliceâ€‘limit."""
    try:
        res = node.xpath(xp)
    except Exception as exc:  # noqa: BLE001
        logger.error("xpath %s failed â€“ %s", xp, exc)
        return ""
    if not res:
        return ""
    val = res[0] if isinstance(res[0], str) else (res[0].text or "")
    val = val.strip()
    return val[:sl] if sl else val


def dec(node: ET._Element, xp: str):
    t = txt(node, xp)
    if not t or not re.search(r"\d", t):
        return None
    try:
        val = Decimal(DEC_RE.sub("", t))
    except Exception:  # noqa: BLE001
        logger.debug("decimal parse failed %s", t)
        return None
    if abs(val) >= MAX_DEC_MAGNITUDE:
        logger.warning("decimal value %s exceeds %s â€“ set NULL", val, MAX_DEC_MAGNITUDE)
        return None
    return val


def canon_case(raw: Optional[str]) -> str:
    return "" if raw is None else raw.strip().upper()[:100]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ manifest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Manifest:
    """Simple file manifest to ensure idempotent processing."""

    def __init__(self, path: Path):
        self.path = path
        self._processed: set[str] = set()
        if self.path.exists():
            self._processed.update(p.strip() for p in self.path.read_text().splitlines())

    def processed(self, fname: str) -> bool:  # noqa: D401 â€“ propertyâ€‘like
        return fname in self._processed

    def mark(self, fname: str):
        if fname in self._processed:
            return
        # atomic append
        with tempfile.NamedTemporaryFile("w", dir=self.path.parent, delete=False) as tmp:
            tmp.write("\n".join(self._processed | {fname}))
            tmp.flush()
            os.fsync(tmp.fileno())
            tmp_path = Path(tmp.name)
        tmp_path.replace(self.path)
        self._processed.add(fname)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ loader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class NpxLoader:
    """Parse SEC Nâ€‘PX .txt filings and load into Postgres."""

    def __init__(self, conn_params: Dict[str, str], flush_every: int = 250, manifest: Optional[Manifest] = None):
        self.conn_params = conn_params
        self.flush_every = flush_every
        self.manifest = manifest or Manifest(Path(".processed_files"))
        self._form_idx = 0  # local surrogate key counter
        self._filing_series_counter: Dict[int, int] = defaultdict(int)
        self.reset_stage()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ reset â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def reset_stage(self, *, clear_pending: bool = True):
        self.FORMS: List[dict] = []
        self.MANAGERS: List[dict] = []
        self.SERIES: List[dict] = []
        self.SERIES_CLASS: List[dict] = []
        self.OTHER_PERSONS: List[dict] = []
        self.VOTES: List[dict] = []
        self.CATS: List[dict] = []
        self.VOTE_CATS: List[dict] = []
        self.VOTE_MGR: List[dict] = []
        self.VOTE_SERIES: List[dict] = []
        self.CAT_LOOKUP: Dict[str, int] = {}
        self._header_lines: List[str] = []
        if clear_pending:
            self.PENDING_SERIES_CLASS: List[dict] = []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ txt filing (flat) â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def parse_txt_filing(self, path: Path):
        if self.manifest.processed(path.name):
            logger.info("skip %s â€“ already processed", path.name)
            return
        accession = ""
        filed_date: Optional[dt.date] = None
        buf = io.StringIO()
        in_xml = False
        parsed_any = False
        hdr_lines: List[str] = []

        with path.open("r", encoding="utf-8", errors="replace") as fh:
            for line in fh:
                if not in_xml:
                    hdr_lines.append(line)
                if not accession and (m := ACC_RE.search(line)):
                    accession = m.group(1).strip()[:30]
                if filed_date is None and (m := FILED_RE.search(line)):
                    filed_date = pdate(m.group(1))
                if not in_xml and XML_START_RE.search(line):
                    in_xml = True
                    buf.write(line)
                    continue
                if in_xml:
                    buf.write(line)
                    if END_TAG_RE.search(line):
                        self._header_lines = hdr_lines
                        parsed_any |= self._parse_xml_fragment(buf.getvalue(), accession, filed_date, path.name)
                        in_xml = False
                        buf.seek(0); buf.truncate(0)
            # dangling fragment
            if in_xml and buf.tell():
                self._header_lines = hdr_lines
                logger.warning("dangling XML fragment in %s â€“ attempting parse", path.name)
                parsed_any |= self._parse_xml_fragment(buf.getvalue(), accession, filed_date, path.name)

        if parsed_any:
            self.manifest.mark(path.name)
        else:
            logger.warning("File %s yielded no edgarSubmission â€“ leaving unmarked", path.name)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ XML fragment â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _parse_xml_fragment(self, xml: str, accession: str, filed_date: Optional[dt.date], source_file: str) -> bool:
        try:
            root = ET.fromstring(xml.encode(), parser=PARSER)
        except ET.XMLSyntaxError as exc:  # noqa: BLE001
            logger.error("XML syntax error in %s â€“ %s", source_file, exc)
            return False

        # standalone vote table
        if root.tag.lower().endswith(("proxyvotetable", "proxyvotetable}")):
            if self._form_idx == 0:
                logger.error("vote table encountered before any edgarSubmission â€“ %s", source_file)
                return False
            self._parse_proxy_vote_table(root, self._form_idx - 1)
            return True

        es_nodes = [root] if root.tag.lower().endswith("edgarsubmission") else root.xpath(".//*[local-name()='edgarSubmission']")
        if not es_nodes:
            logger.debug("non Nâ€‘PX fragment in %s", source_file)
            return False

        es = es_nodes[0]
        local_form = self._form_idx
        self._form_idx += 1

        include_all = txt(es, ".//*[local-name()='rptIncludeAllSeriesFlag']").strip().lower() == "true"
        if include_all:
            self._scan_existing_series_classes(local_form)

        # --- cover page & header ---
        raw_period = txt(es, ".//*[local-name()='periodOfReport']")
        cp_node = es.xpath(".//*[local-name()='coverPage']")
        cover = cp_node[0] if cp_node else None

        # amendment block helper
        ai_node = es.xpath(".//*[local-name()='amendmentInfo']")
        def _ai(x: str) -> str:
            return txt(ai_node[0], f".//*[local-name()='{x}']") if ai_node else ""

        # build the form row using dictâ€‘comprehension for readability
        form_row = {
            "local_form": local_form,
            "reporting_person_name": txt(es, ".//*[local-name()='reportingPerson']/*[local-name()='name']", 250),
            "phone_number": txt(es, ".//*[local-name()='phoneNumber']", 50),
            "address_street1": txt(es, ".//*[local-name()='address']/*[local-name()='street1']", 250),
            "address_street2": txt(es, ".//*[local-name()='address']/*[local-name()='street2']", 250),
            "address_city": txt(es, ".//*[local-name()='address']/*[local-name()='city']", 100),
            "address_state": txt(es, ".//*[local-name()='address']/*[local-name()='stateOrCountry']", 100),
            "address_zip": txt(es, ".//*[local-name()='address']/*[local-name()='zipCode']", 30),
            "accession_number": accession[:30],
            "is_parsable": True,
            "cik": txt(es, ".//*[local-name()='cik']", 15),
            "period_of_report_raw": raw_period,
            "period_of_report": clean_period(raw_period),
            "date_filed": filed_date,
            "report_type": txt(es, ".//*[local-name()='reportType']", 100) or "FUND VOTING REPORT",
            "form_type": txt(es, ".//*[local-name()='submissionType']", 10) or "N-PX",
            "sec_file_number": txt(es, ".//*[local-name()='fileNumber']", 20),
            "crd_number": txt(es, ".//*[local-name()='reportingCrdNumber']", 20),
            "sec_file_number_other": txt(es, ".//*[local-name()='reportingSecFileNumber']", 20),
            "lei_number": txt(es, ".//*[local-name()='leiNumber']", 40),
            "investment_company_type": txt(es, ".//*[local-name()='investmentCompanyType']", 20),
            "confidential_treatment": "Y" if txt(es, ".//*[local-name()='confidentialTreatment']").strip().upper() in {"Y", "YES", "TRUE", "1"} else "N",
            "is_notice_report": "NOTICE" in txt(es, ".//*[local-name()='reportType']").upper(),
            "explanatory_choice": "Y" if txt(es, ".//*[local-name()='explanatoryChoice']").strip().upper() in {"Y", "YES", "TRUE", "1"} else "N",
            "other_included_managers_count": int(txt(es, ".//*[local-name()='otherIncludedManagersCount']") or 0),
            "series_count": 0,  # patched later
            "is_amendment": txt(es, ".//*[local-name()='isAmendment']").strip().upper() in {"Y", "YES", "TRUE", "1"},
            "amendment_no": (lambda v: int(v) if v.isdigit() else None)(txt(es, ".//*[local-name()='amendmentNo']")),
            "amendment_type": txt(es, ".//*[local-name()='amendmentType']", 20),
            "notice_explanation": txt(es, ".//*[local-name()='noticeExplanation']", 255),
            "date_expired_denied_raw": _ai("dateExpiredDenied"),
            "date_reported_raw": _ai("dateReported"),
            "year_or_quarter": (lambda x: int(x) if x.isdigit() else x)(txt(cover, ".//*[local-name()='yearOrQuarter']")) if cover is not None else "",
            "report_calendar_year": (lambda x: int(x) if x.isdigit() else x)(txt(cover, ".//*[local-name()='reportCalendarYear']")) if cover is not None else "",
            "report_quarter_year": (lambda x: int(x) if x.isdigit() else x)(txt(cover, ".//*[local-name()='reportQuarterYear']")) if cover is not None else "",
            "explanatory_notes": txt(es, ".//*[local-name()='explanatoryNotes']", 4000),
            "signatory_name": txt(es, ".//*[local-name()='txSignature']", 250),
            "signatory_name_printed": txt(es, ".//*[local-name()='txPrintedSignature']", 250),
            "signatory_title": txt(es, ".//*[local-name()='txTitle']", 100),
            "tx_as_of_date_raw": txt(es, ".//*[local-name()='txAsOfDate']", 20),
            "signatory_date": pdate(txt(es, ".//*[local-name()='txAsOfDate']")),
        }
        self.FORMS.append(form_row)

        # --- managers ---
        for mn in es.xpath(".//*[local-name()='summaryPage']//*[local-name()='investmentManagers']"):
            self.MANAGERS.append({
                "local_form": local_form,
                "serial_no": (lambda x: int(x) if x.isdigit() else None)(txt(mn, './/*[local-name()="serialNo"]')),
                "name": txt(mn, './/*[local-name()="name"]', 250),
                "form13f_number": txt(mn, './/*[local-name()="form13FFileNumber"]', 20) or txt(mn, './/*[local-name()="icaOr13FFileNumber"]', 17),
                "crd_number": txt(mn, './/*[local-name()="crdNumber"]', 20),
                "sec_file_number": txt(mn, './/*[local-name()="secFileNumber"]', 20) or txt(mn, './/*[local-name()="otherFileNumber"]', 17),
                "lei_number": txt(mn, './/*[local-name()="leiNumber"]', 40) or txt(mn, './/*[local-name()="leiNumberOM"]', 20),
            })

        # --- series & class ---
        for sr in es.xpath('.//*[local-name()="seriesReports"]'):
            s_code = txt(sr, './/*[local-name()="idOfSeries"]', 25)
            self._filing_series_counter[local_form] += 1
            self.SERIES.append({
                "local_form": local_form,
                "series_code": s_code,
                "series_name": txt(sr, './/*[local-name()="nameOfSeries"]', 250),
                "series_lei": txt(sr, './/*[local-name()="leiOfSeries"]', 40),
            })
            for cls in sr.xpath('.//*[local-name()="classInfo"]'):
                self.SERIES_CLASS.append({
                    "local_form": local_form,
                    "series_code": s_code,
                    "class_id": txt(cls, './/*[local-name()="classId"]', 10),
                    "class_name": txt(cls, './/*[local-name()="className"]', 250),
                })

        # --- other persons ---
        for op in es.xpath('.//*[local-name()="otherManager"]'):
            self.OTHER_PERSONS.append({
                "local_form": local_form,
                "ica_form13f": txt(op, './/*[local-name()="icaOr13FFileNumber"]', 17),
                "crd_number": txt(op, './/*[local-name()="crdNumber"]', 20),
                "sec_file_number": txt(op, './/*[local-name()="otherFileNumber"]', 17),
                "lei_number": txt(op, './/*[local-name()="leiNumberOM"]', 20),
                "name": txt(op, './/*[local-name()="managerName"]', 150),
            })

        # --- inline vote table(s) ---
        for pvt in es.xpath('.//*[local-name()="proxyVoteTable"]'):
            self._parse_proxy_vote_table(pvt, local_form)

        form_row["series_count"] = self._filing_series_counter[local_form]
        return True

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper: includeâ€‘allâ€‘series flag â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _scan_existing_series_classes(self, local_form: int):
        """If the filing declares `rptIncludeAllSeriesFlag=true`, replicate the latest
        seriesâ†’class map from earlier filings so that voteâ†’series bridges have
        something to point at even when the XML omits classInfo blocks."""
        # look back through *current runâ€‘time* collections only â€“ this avoids DBÂ roundâ€‘trips
        series_by_code: Dict[str, dict] = {}
        for ser in reversed(self.SERIES):
            if ser["series_code"] not in series_by_code:
                series_by_code[ser["series_code"]] = ser.copy()
        for sc in reversed(self.SERIES_CLASS):
            # map by series_code, preserve original class id/name
            self.PENDING_SERIES_CLASS.append({
                "local_form": local_form,
                "series_code": sc["series_code"],
                "class_id": sc["class_id"],
                "class_name": sc["class_name"],
            })

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ vote table â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _parse_proxy_vote_table(self, pvt: ET._Element, local_form: int):
        for pt in pvt.xpath('.//*[local-name()="proxyTable"]'):
            lv = len(self.VOTES)
            meeting_raw = txt(pt, './/*[local-name()="meetingDate"]')
            vote_row = {
                "local_vote": lv,
                "local_form": local_form,
                "issuer_name": txt(pt, './/*[local-name()="issuerName"]', 250),
                "cusip": txt(pt, './/*[local-name()="cusip"]', 30),
                "isin": txt(pt, './/*[local-name()="isin"]', 30),
                "figi": txt(pt, './/*[local-name()="figi"]', 30),
                "meeting_date_raw": meeting_raw,
                "meeting_date": pdate(meeting_raw),
                "vote_description": txt(pt, './/*[local-name()="voteDescription"]', 20000),
                "proposed_by": txt(pt, './/*[local-name()="voteSource"]', 20),
                "shares_voted": dec(pt, './/*[local-name()="sharesVoted"][1]'),
                "shares_on_loan": dec(pt, './/*[local-name()="sharesOnLoan"][1]'),
                "vote_cast": None,
                "vote_cast_shares": None,
                "management_rec": None,
                "other_notes": None,
            }
            vr = pt.xpath('.//*[local-name()="voteRecord"]')
            if vr:
                vote_row.update({
                    "vote_cast": txt(vr[0], './/*[local-name()="howVoted"]', 50),
                    "vote_cast_shares": dec(vr[0], './/*[local-name()="sharesVoted"]'),
                    "management_rec": txt(vr[0], './/*[local-name()="managementRecommendation"]', 50),
                })
                if len(vr) > 1:
                    vote_row["other_notes"] = f"{len(vr)} voteRecord tags found"
            self.VOTES.append(vote_row)

            # categories
            for cat in pt.xpath('.//*[local-name()="categoryType"]/text()'):
                c = canon_case(cat)
                if c not in self.CAT_LOOKUP:
                    self.CAT_LOOKUP[c] = len(self.CAT_LOOKUP) + 1
                    self.CATS.append({"local_cat_id": self.CAT_LOOKUP[c], "category_type": c})
                self.VOTE_CATS.append({"local_vote": lv, "local_cat_id": self.CAT_LOOKUP[c]})

            # managers
            for om in pt.xpath('.//*[local-name()="otherManager"]/text()'):
                try:
                    sn = int(om.strip())
                except ValueError:
                    continue
                self.VOTE_MGR.append({"local_vote": lv, "local_form": local_form, "serial_no": sn})

            # series bridge
            for sc in pt.xpath('.//*[local-name()="voteSeries"]/text()'):
                sc = sc.strip()[:25]
                if sc:
                    self.VOTE_SERIES.append({"local_vote": lv, "local_form": local_form, "series_code": sc})

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ DB flush â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def flush_to_db(self):
        if not self.FORMS:
            return
        logger.info("flushing %d filings â†’ DB", len(self.FORMS))
        conn = psycopg2.connect(**self.conn_params)
        try:
            with conn:
                with conn.cursor() as cur:
                    def _exec(sql: str, rows: list, *, fetch: bool = False):
                        if not rows:
                            return []
                        cur.execute("SAVEPOINT sp")
                        try:
                            return execute_values(cur, sql, rows, fetch=fetch, page_size=min(1000, len(rows)))
                        except Exception:  # noqa: BLE001
                            logger.error("batch insert failed â€“ rolled back", exc_info=True)
                            cur.execute("ROLLBACK TO SAVEPOINT sp")
                            return []

                    # --- form_npx ---
                    form_rows = [tuple(f.get(c) for c in FORM_COLS) for f in self.FORMS]
                    f_ids = _exec(
                        f"INSERT INTO form_npx ({','.join(FORM_COLS)}) VALUES %s RETURNING form_id",
                        form_rows,
                        fetch=True,
                    )
                    form_id_map = {f["local_form"]: fid[0] for f, fid in zip(self.FORMS, f_ids)}

                    # --- institutional_manager ---
                    mgr_rows = []
                    for m in self.MANAGERS:
                        row_dict = m.copy()
                        row_dict["form_id"] = form_id_map[m["local_form"]]
                        mgr_rows.append(tuple(row_dict.get(c) for c in MGR_COLS))
                    m_ids = _exec(
                        f"INSERT INTO institutional_manager ({','.join(MGR_COLS)}) VALUES %s RETURNING manager_id,form_id,serial_no",
                        mgr_rows,
                        fetch=True,
                    )
                    m_lookup = {(fid, sn): mid for mid, fid, sn in m_ids}

                    # --- series ---
                    ser_rows = []
                    for s in self.SERIES:
                        row_dict = s.copy()
                        row_dict["form_id"] = form_id_map[s["local_form"]]
                        ser_rows.append(tuple(row_dict.get(c) for c in SER_COLS))
                    ser_ret = _exec(
                        f"INSERT INTO series ({','.join(SER_COLS)}) VALUES %s RETURNING series_id,form_id,series_code",
                        ser_rows,
                        fetch=True,
                    )
                    ser_lookup = {(fid, sc): sid for sid, fid, sc in ser_ret}

                    # --- series_class ---
                    ready_sc, pending_sc, sc_seen = [], [], set()
                    for sc in self.SERIES_CLASS + self.PENDING_SERIES_CLASS:
                        fid = form_id_map.get(sc["local_form"])
                        sid = ser_lookup.get((fid, sc["series_code"]))
                        if not sid:
                            pending_sc.append(sc)
                            continue
                        pair = (sid, sc["class_id"])
                        if pair not in sc_seen:
                            ready_sc.append((sid, sc["class_id"], sc["class_name"]))
                            sc_seen.add(pair)
                    _exec(f"INSERT INTO series_class ({','.join(SC_COLS)}) VALUES %s", ready_sc)
                    self.PENDING_SERIES_CLASS = pending_sc
                    self.SERIES_CLASS = []

                    # --- other_reporting_person ---
                    op_rows = []
                    for o in self.OTHER_PERSONS:
                        row_dict = o.copy()
                        row_dict["form_id"] = form_id_map[o["local_form"]]
                        op_rows.append(tuple(row_dict.get(c) for c in OP_COLS))
                    _exec(f"INSERT INTO other_reporting_person ({','.join(OP_COLS)}) VALUES %s", op_rows)

                    # --- matter_category upsert ---
                    if self.CATS:
                        cat_rows = [tuple(c.get(col) for col in CAT_COLS) for c in self.CATS]
                        cat_ret = _exec(
                            f"INSERT INTO matter_category ({','.join(CAT_COLS)}) VALUES %s ON CONFLICT(category_type) DO UPDATE SET category_type=EXCLUDED.category_type RETURNING category_id,category_type",
                            cat_rows,
                            fetch=True,
                        )
                        cat_id_map = {t: c for c, t in cat_ret}
                        local_to_db_cat = {v: cat_id_map[k] for k, v in self.CAT_LOOKUP.items() if k in cat_id_map}
                    else:
                        local_to_db_cat = {}

                    # --- proxy_voting_record ---
                    vote_rows = []
                    for v in self.VOTES:
                        row_dict = v.copy()
                        row_dict["form_id"] = form_id_map[v["local_form"]]
                        vote_rows.append(tuple(row_dict.get(c) for c in VOTE_COLS))
                    vote_ret = _exec(
                        f"INSERT INTO proxy_voting_record ({','.join(VOTE_COLS)}) VALUES %s RETURNING vote_id",
                        vote_rows,
                        fetch=True,
                    )
                    vote_id_map = {v["local_vote"]: vid[0] for v, vid in zip(self.VOTES, vote_ret)}

                    # --- proxy_voting_record_category ---
                    cat_bridge_rows = [(
                        vote_id_map.get(vc["local_vote"]), local_to_db_cat.get(vc["local_cat_id"])
                    ) for vc in self.VOTE_CATS if vote_id_map.get(vc["local_vote"]) and local_to_db_cat.get(vc["local_cat_id"])]
                    _exec(f"INSERT INTO proxy_voting_record_category ({','.join(VRC_COLS)}) VALUES %s ON CONFLICT DO NOTHING", cat_bridge_rows)

                    # --- voting_record_manager ---
                    vrm_rows = [(
                        vote_id_map.get(vm["local_vote"]), m_lookup.get((form_id_map.get(vm["local_form"]), vm["serial_no"]))
                    ) for vm in self.VOTE_MGR if vote_id_map.get(vm["local_vote"]) and m_lookup.get((form_id_map.get(vm["local_form"]), vm["serial_no"]))]
                    _exec(f"INSERT INTO voting_record_manager ({','.join(VRM_COLS)}) VALUES %s ON CONFLICT DO NOTHING", vrm_rows)

                    # --- voting_record_series ---
                    vrs_rows = [(
                        vote_id_map.get(vs["local_vote"]), ser_lookup.get((form_id_map.get(vs["local_form"]), vs["series_code"]))
                    ) for vs in self.VOTE_SERIES if vote_id_map.get(vs["local_vote"]) and ser_lookup.get((form_id_map.get(vs["local_form"]), vs["series_code"]))]
                    _exec(f"INSERT INTO voting_record_series ({','.join(VRS_COLS)}) VALUES %s ON CONFLICT DO NOTHING", vrs_rows)
        finally:
            conn.close()
        self.reset_stage(clear_pending=False)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ run loop â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self, folder: Path):
        counter = 0
        for f in folder.iterdir():
            if f.suffix.lower() == ".txt":
                self.parse_txt_filing(f)
                counter += 1
                if counter % self.flush_every == 0:
                    self.flush_to_db()
        self.flush_to_db()
        retry = 0
        MAX_RETRY = 5  # safeguard against infinite loops
        while (self.SERIES_CLASS or self.PENDING_SERIES_CLASS) and retry < MAX_RETRY:
            retry += 1
            self.flush_to_db()
        if retry == MAX_RETRY:
            logger.error("Unresolved series_class after %d retries â€“ data loss possible", MAX_RETRY)
        logger.info("finished â€“ %d files processed", counter)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main(folder: str, flush_every: int = 250, log_level: str = "INFO"):
    load_dotenv()
    logging.root.setLevel(log_level.upper())

    conn_params = {
        "host": os.getenv("PGHOST", "localhost"),
        "port": int(os.getenv("PGPORT", 5432)),
        "dbname": os.getenv("PGDATABASE", "npx"),
        "user": os.getenv("PGUSER", "postgres"),
        "password": os.getenv("PGPASSWORD", "postgres"),
        "sslmode": "require",  # best practice for RDS
    }
    if not conn_params["password"] or conn_params["password"] in {"postgres", ""}:
        raise SystemExit("ðŸ’¥ Refusing to run with default / empty DB password")

    loader = NpxLoader(conn_params, flush_every=flush_every)
    loader.run(Path(folder))

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python npx_loader_updated_corrected.py <folder> [flush] [loglevel]")
        sys.exit(1)
    main(
        folder=sys.argv[1],
        flush_every=int(sys.argv[2]) if len(sys.argv) > 2 else 250,
        log_level=sys.argv[3] if len(sys.argv) > 3 else "INFO",
    )




