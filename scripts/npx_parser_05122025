# %%
"""Improved SEC Nâ€‘PX loader
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Parses .txt filings, extracts XML, and loads to Postgres while
retaining raw values and correcting known anomalies:
* Keeps raw ``periodOfReport`` and derives a clean quarterâ€‘end date.
* Handles Excel serial dates (incl. 1900â€‘02â€‘29 bug).
* Fixes regexes, negativeâ€‘class decimal regex, and multiple ``voteSeries``
  per vote.
* Streams XML fragments to avoid unbounded memory.
* Adds optional structured JSON logging and UTC timestamps.
* Includes perâ€‘table savepoints so childâ€‘row failures do not orphan forms.
"""

from __future__ import annotations

import datetime as dt
import io
import logging
import os
import re
import sys
from collections import defaultdict
from decimal import Decimal
from pathlib import Path
from typing import Dict, List, Optional

import lxml.etree as ET
from lxml.etree import XPathEvalError
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from pythonjsonlogger import jsonlogger  # type: ignore

    class _JsonFmt(jsonlogger.JsonFormatter):
        def add_fields(self, *args, **kwargs):
            super().add_fields(*args, **kwargs)
            # ISOâ€‘8601 UTC timestamp
            self._style._fmt = "%(_:asctime)sZ %(levelname)s %(name)s %(message)s"

    _handler: logging.Handler = logging.StreamHandler()
    _handler.setFormatter(_JsonFmt())
except ImportError:
    _handler = logging.StreamHandler()
    _handler.setFormatter(
        logging.Formatter("%(asctime)sZ [%(levelname)s] %(name)s: %(message)s", datefmt="%Y-%m-%dT%H:%M:%S")
    )

if not logging.root.handlers:
    logging.root.addHandler(_handler)
logging.root.setLevel(logging.INFO)
logger = logging.getLogger("npx_loader")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATE_FMTS = (
    "%m/%d/%Y",
    "%Y-%m-%d",
    "%m-%d-%Y",
    "%Y%m%d",
    "%Y/%m/%d",
    "%m/%d/%y",
    "%m-%d-%y",
    "%d-%b-%Y",
    "%d-%B-%Y",
)

DEC_RE = re.compile(r"[^0-9.\-]")
MAX_DEC = Decimal("1e14")
XML_DECL_RE = re.compile(r"<\?xml", re.I)
END_TAG_RE = re.compile(
    r"</(?:edgarSubmission|[A-Za-z]+:proxyVoteTable|proxyVoteTable)>", re.I
)
ACC_RE = re.compile(r"ACCESSION\s+NUMBER:\s*([^\r\n]+)", re.I)
FILED_RE = re.compile(r"FILED\s+AS\s+OF\s+DATE:\s*(\d{6,8})", re.I)

PARSER = ET.XMLParser(recover=True, huge_tree=True, resolve_entities=False)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def excel_ordinal_to_date(serial: int) -> dt.date:
    """Convert Excel serial date (1900â€‘based) to Python date."""
    if serial <= 0:
        raise ValueError("Serial must be positive")
    if serial >= 60:  # Excel bug: 1900â€‘02â€‘29 phantom day
        serial -= 1
    return dt.date(1899, 12, 30) + dt.timedelta(days=serial)


def pdate(s: Optional[str]):
    if not s or not s.strip():
        return None
    s = s.strip()
    for fmt in DATE_FMTS:
        try:
            return dt.datetime.strptime(s, fmt).date()
        except ValueError:
            continue
    if s.isdigit() and len(s) >= 5:
        try:
            return excel_ordinal_to_date(int(s))
        except Exception:
            pass
    logger.debug("unparseable date %s", s)
    return None


def clean_period(raw: Optional[str]) -> Optional[dt.date]:
    """Return quarterâ€‘end date (Junâ€‘30 or Sepâ€‘30) or None if invalid."""
    if not raw:
        return None
    raw = raw.strip()
    # Excel ordinals first
    if raw.isdigit() and len(raw) >= 5:
        try:
            d = excel_ordinal_to_date(int(raw))
        except Exception:
            return None
    else:
        d = pdate(raw)
    if not d:
        return None
    if d.month == 6 and d.day >= 28:
        return dt.date(d.year, 6, 30)
    if d.month == 9 and d.day >= 28:
        return dt.date(d.year, 9, 30)
    return None


def txt(node: ET._Element, xp: str, sl: Optional[int] = None) -> str:
    try:
        res = node.xpath(xp)
    except Exception as exc:
        logger.error("xpath %s failed â€“ %s", xp, exc)
        return ""
    if not res:
        return ""
    val = res[0] if isinstance(res[0], str) else (res[0].text or "")
    val = val.strip()
    return val[:sl] if sl else val


def dec(node: ET._Element, xp: str):
    t = txt(node, xp)
    if not t or not re.search(r"\d", t):
        return None
    try:
        val = Decimal(DEC_RE.sub("", t))
    except Exception:
        logger.debug("decimal parse failed %s", t)
        return None
    if abs(val) >= MAX_DEC:
        logger.warning("decimal value %s exceeds %s â€“ set NULL", val, MAX_DEC)
        return None
    return val


def canon_case(raw: Optional[str]) -> str:
    return "" if raw is None else raw.strip().upper()[:100]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ manifest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Manifest:
    """Flatâ€‘file manifest for crashâ€‘safe resume (with fileâ€‘locking)."""

    def __init__(self, path: Path):
        self.path = path
        self._processed = set()
        if self.path.exists():
            with self.path.open() as fh:
                for line in fh:
                    self._processed.add(line.strip())

    def processed(self, fname: str) -> bool:
        return fname in self._processed

    def mark(self, fname: str):
        if fname in self._processed:
            return
        # naive lock â€“ suitable for singleâ€‘process; use portalocker for multi
        with self.path.open("a") as fh:
            fh.write(f"{fname}\n")
        self._processed.add(fname)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ loader class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class NpxLoader:
    """Parse SEC Nâ€‘PX .txt filings and load into Postgres."""

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ constructor â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def __init__(self, conn_params: Dict[str, str], flush_every: int = 250, manifest: Manifest | None = None):
        self.conn_params = conn_params
        self.flush_every = flush_every
        self.manifest = manifest or Manifest(Path(".processed_files"))

        self._form_idx = 0
        self._filing_series_counter: Dict[int, int] = defaultdict(int)
        self.reset_stage()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ stage reset â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def reset_stage(self, clear_pending: bool = True):
        self.FORMS: List[dict] = []
        self.MANAGERS: List[dict] = []
        self.SERIES: List[dict] = []
        self.SERIES_CLASS: List[dict] = []
        self.OTHER_PERSONS: List[dict] = []
        self.VOTES: List[dict] = []
        self.CATS: List[dict] = []
        self.VOTE_CATS: List[dict] = []
        self.VOTE_MGR: List[dict] = []
        self.VOTE_SERIES: List[dict] = []
        self.CAT_LOOKUP: Dict[str, int] = {}
        self._header_lines: List[str] = []
        if clear_pending:
            self.PENDING_SERIES_CLASS: List[dict] = []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ parse single filing â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def parse_txt_filing(self, path: Path):
        if self.manifest.processed(path.name):
            logger.info("skip %s â€“ already processed", path.name)
            return

        accession = ""
        filed_date = None
        in_xml = False
        buf = io.StringIO()
        parsed_any = False
        hdr_lines: List[str] = []

        with path.open("r", encoding="utf-8", errors="replace") as fh:
            for line in fh:
                if not in_xml:
                    hdr_lines.append(line)

                if not accession and (m := ACC_RE.search(line)):
                    accession = m.group(1).strip()[:30]
                if filed_date is None and (m := FILED_RE.search(line)):
                    filed_date = pdate(m.group(1))

                if not in_xml and XML_DECL_RE.search(line):
                    in_xml = True
                    buf.write(line)
                    continue

                if in_xml:
                    buf.write(line)
                    if END_TAG_RE.search(line):
                        self._header_lines = hdr_lines
                        parsed_any |= self._parse_xml_fragment(
                            buf.getvalue(), accession, filed_date, path.name
                        )
                        in_xml = False
                        buf.seek(0); buf.truncate(0)
            # dangling fragment
            if in_xml and buf.tell():
                self._header_lines = hdr_lines
                logger.warning("dangling XML fragment in %s â€“ attempting parse", path.name)
                parsed_any |= self._parse_xml_fragment(
                    buf.getvalue(), accession, filed_date, path.name
                )

        if parsed_any:
            self.manifest.mark(path.name)
        else:
            logger.warning("File %s yielded no edgarSubmission â€“ leaving unmarked", path.name)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ helper: scan existing series/class in header â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _scan_existing_series_classes(self, local_form: int):
        if not self._header_lines:
            return
        cur_series = None
        lines = self._header_lines
        for i, ln in enumerate(lines):
            ln = ln.strip()
            if ln.startswith("<SERIES-ID>"):
                cur_series = ln.split(">", 1)[1].split("<", 1)[0].strip()[:25]
            elif ln.startswith("<CLASS-CONTRACT-ID>") and cur_series:
                class_id = ln.split(">", 1)[1].split("<", 1)[0].strip()[:10]
                # look ahead for class name
                class_name = ""
                for j in range(i + 1, min(i + 4, len(lines))):
                    if "<CLASS-CONTRACT-NAME>" in lines[j]:
                        class_name = lines[j].split("<CLASS-CONTRACT-NAME>", 1)[1].split("<", 1)[0].strip()[:250]
                        break
                self.SERIES_CLASS.append(
                    {
                        "local_form": local_form,
                        "series_code": cur_series,
                        "class_id": class_id,
                        "class_name": class_name,
                    }
                )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ parse XML fragment â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _parse_xml_fragment(self, xml: str, accession: str, filed_date: Optional[dt.date], source_file: str) -> bool:
        try:
            root = ET.fromstring(xml.encode(), parser=PARSER)
        except ET.XMLSyntaxError as exc:
            logger.error("XML syntax error in %s â€“ %s", source_file, exc)
            return False

        if root.tag.lower().endswith(("proxyvotetable", "proxyvotetable}")):
            if self._form_idx == 0:
                logger.error("vote table encountered before any edgarSubmission â€“ %s", source_file)
                return False
            local_form = self._form_idx - 1
            self._parse_proxy_vote_table(root, local_form)
            return True

        es_nodes = [root] if root.tag.lower().endswith("edgarsubmission") else root.xpath(".//*[local-name()='edgarSubmission']")
        if not es_nodes:
            logger.debug("non Nâ€‘PX fragment in %s", source_file)
            return False

        es = es_nodes[0]
        local_form = self._form_idx
        self._form_idx += 1

        include_all = txt(es, ".//*[local-name()='rptIncludeAllSeriesFlag']").strip().lower() == "true"
        if include_all:
            self._scan_existing_series_classes(local_form)

        # cover page
        raw_period = txt(es, ".//*[local-name()='periodOfReport']")
        form_row = {
            "local_form": local_form,
            "reporting_person_name": txt(es, ".//*[local-name()='reportingPerson']/*[local-name()='name']", 250),
            "phone_number": txt(es, ".//*[local-name()='phoneNumber']", 50),
            "address_street1": txt(es, ".//*[local-name()='address']/*[local-name()='street1']", 250),
            "address_street2": txt(es, ".//*[local-name()='address']/*[local-name()='street2']", 250),
            "address_city": txt(es, ".//*[local-name()='address']/*[local-name()='city']", 100),
            "address_state": txt(es, ".//*[local-name()='address']/*[local-name()='stateOrCountry']", 100),
            "address_zip": txt(es, ".//*[local-name()='address']/*[local-name()='zipCode']", 30),
            "accession_number": accession[:30],
            "is_parsable": True,
            "cik": txt(es, ".//*[local-name()='cik']", 15),
            "period_of_report_raw": raw_period,
            "conformed_period": clean_period(raw_period),
            "date_filed": filed_date,
            "report_type": txt(es, ".//*[local-name()='reportType']", 100) or "FUND VOTING REPORT",
            "form_type": txt(es, ".//*[local-name()='submissionType']", 10) or "N-PX",
            "sec_file_number": txt(es, ".//*[local-name()='fileNumber']", 20),
            "crd_number": txt(es, ".//*[local-name()='reportingCrdNumber']", 20),
            "sec_file_number_other": txt(es, ".//*[local-name()='reportingSecFileNumber']", 20),
            "lei_number": txt(es, ".//*[local-name()='leiNumber']", 40),
            "investment_company_type": txt(es, ".//*[local-name()='investmentCompanyType']", 20),
            "confidential_treatment": ("Y" if txt(es, ".//*[local-name()='confidentialTreatment']").upper() in {"Y", "YES", "TRUE", "1"} else "N"),
            "is_notice_report": "NOTICE" in txt(es, ".//*[local-name()='reportType']").upper(),
            "explanatory_choice": ("Y" if txt(es, ".//*[local-name()='explanatoryChoice']").upper() in {"Y", "YES", "TRUE", "1"} else "N"),
            "other_included_managers_count": int(txt(es, ".//*[local-name()='otherIncludedManagersCount']") or 0),
            "series_count": 0,  # patched later
            "is_amendment": txt(es, ".//*[local-name()='isAmendment']").upper() in {"Y", "YES", "TRUE", "1"},
            "amendment_no": (lambda v: int(v) if v.isdigit() else None)(txt(es, ".//*[local-name()='amendmentNo']")),
            "amendment_type": txt(es, ".//*[local-name()='amendmentType']", 20),
            "notice_explanation": txt(es, ".//*[local-name()='noticeExplanation']", 200),
            "explanatory_notes": txt(es, ".//*[local-name()='explanatoryNotes']", 200),
            "signatory_name": txt(es, ".//*[local-name()='txSignature']", 250),
            "signatory_name_printed": txt(es, ".//*[local-name()='txPrintedSignature']", 250),
            "signatory_title": txt(es, ".//*[local-name()='txTitle']", 100),
            "signatory_date": pdate(txt(es, ".//*[local-name()='txAsOfDate']")),
        }
        self.FORMS.append(form_row)

        # managers
        for mn in es.xpath(".//*[local-name()='summaryPage']//*[local-name()='investmentManagers']"):
            self.MANAGERS.append(
                {
                    "local_form": local_form,
                    "serial_no": (lambda x: int(x) if x.isdigit() else None)(txt(mn, './/*[local-name()="serialNo"]')),
                    "name": txt(mn, './/*[local-name()="name"]', 250),
                    "form13f_number": txt(mn, './/*[local-name()="form13FFileNumber"]', 20) or txt(mn, './/*[local-name()="icaOr13FFileNumber"]', 17),
                    "crd_number": txt(mn, './/*[local-name()="crdNumber"]', 20),
                    "sec_file_number": txt(mn, './/*[local-name()="secFileNumber"]', 20) or txt(mn, './/*[local-name()="otherFileNumber"]', 17),
                    "lei_number": txt(mn, './/*[local-name()="leiNumber"]', 40) or txt(mn, './/*[local-name()="leiNumberOM"]', 20),
                }
            )

        # series & class
        for sr in es.xpath('.//*[local-name()="seriesReports"]'):
            s_code = txt(sr, './/*[local-name()="idOfSeries"]', 25)
            self._filing_series_counter[local_form] += 1
            self.SERIES.append(
                {
                    "local_form": local_form,
                    "series_code": s_code,
                    "series_name": txt(sr, './/*[local-name()="nameOfSeries"]', 250),
                    "series_lei": txt(sr, './/*[local-name()="leiOfSeries"]', 40),
                }
            )
            for cls in sr.xpath('.//*[local-name()="classInfo"]'):
                self.SERIES_CLASS.append(
                    {
                        "local_form": local_form,
                        "series_code": s_code,
                        "class_id": txt(cls, './/*[local-name()="classId"]', 10),
                        "class_name": txt(cls, './/*[local-name()="className"]', 250),
                    }
                )

        # other persons
        for op in es.xpath('.//*[local-name()="otherManager"]'):
            self.OTHER_PERSONS.append(
                {
                    "local_form": local_form,
                    "ica_form13f": txt(op, './/*[local-name()="icaOr13FFileNumber"]', 17),
                    "crd_number": txt(op, './/*[local-name()="crdNumber"]', 20),
                    "sec_file_number": txt(op, './/*[local-name()="otherFileNumber"]', 17),
                    "lei_number": txt(op, './/*[local-name()="leiNumberOM"]', 20),
                    "name": txt(op, './/*[local-name()="managerName"]', 150),
                }
            )

        # proxy voting records (inline table)
        for pvt in es.xpath('.//*[local-name()="proxyVoteTable"]'):
            self._parse_proxy_vote_table(pvt, local_form)

        form_row["series_count"] = self._filing_series_counter[local_form]
        return True

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ proxy vote table â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _parse_proxy_vote_table(self, pvt: ET._Element, local_form: int):
        try:
            proxy_tables = pvt.xpath('.//*[local-name()="proxyTable"]')
        except XPathEvalError as exc:
            logger.error("proxyVoteTable XPath failure â€“ %s", exc)
            return
        for pt in proxy_tables:
            lv = len(self.VOTES)
            vote_row = {
                "local_vote": lv,
                "local_form": local_form,
                "issuer_name": txt(pt, './/*[local-name()="issuerName"]', 250),
                "cusip": txt(pt, './/*[local-name()="cusip"]', 30),
                "isin": txt(pt, './/*[local-name()="isin"]', 30),
                "figi": txt(pt, './/*[local-name()="figi"]', 30),
                "meeting_date": pdate(txt(pt, './/*[local-name()="meetingDate"]')),
                "vote_description": txt(pt, './/*[local-name()="voteDescription"]'),
                "proposed_by": txt(pt, './/*[local-name()="voteSource"]', 20),
                "shares_voted": dec(pt, './/*[local-name()="sharesVoted"][1]'),
                "shares_on_loan": dec(pt, './/*[local-name()="sharesOnLoan"][1]'),
                "vote_cast": None,
                "vote_cast_shares": None,
                "management_rec": None,
                "other_notes": None,
            }
            vr = pt.xpath('.//*[local-name()="voteRecord"]')
            if vr:
                vote_row["vote_cast"] = txt(vr[0], './/*[local-name()="howVoted"]', 50)
                vote_row["vote_cast_shares"] = dec(vr[0], './/*[local-name()="sharesVoted"]')
                vote_row["management_rec"] = txt(vr[0], './/*[local-name()="managementRecommendation"]', 50)
                if len(vr) > 1:
                    vote_row["other_notes"] = f"{len(vr)} voteRecord tags found"
            self.VOTES.append(vote_row)

            # categories
            for cat in pt.xpath('.//*[local-name()="categoryType"]/text()'):
                c = canon_case(cat)
                if c not in self.CAT_LOOKUP:
                    self.CAT_LOOKUP[c] = len(self.CAT_LOOKUP) + 1
                    self.CATS.append({"local_cat_id": self.CAT_LOOKUP[c], "category_type": c})
                self.VOTE_CATS.append({"local_vote": lv, "local_cat_id": self.CAT_LOOKUP[c]})

            # manager bridge
            for om in pt.xpath('.//*[local-name()="otherManager"]/text()'):
                try:
                    sn = int(om.strip())
                except ValueError:
                    continue
                self.VOTE_MGR.append({"local_vote": lv, "local_form": local_form, "serial_no": sn})

            # series bridge (â™¦ now include all series)
            for sc in pt.xpath('.//*[local-name()="voteSeries"]/text()'):
                sc = sc.strip()[:25]
                if sc:
                    self.VOTE_SERIES.append({"local_vote": lv, "local_form": local_form, "series_code": sc})

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ flush to DB â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def flush_to_db(self):
        if not self.FORMS:
            return
        logger.info("flushing %d filings to DB", len(self.FORMS))
        conn = psycopg2.connect(**self.conn_params)
        try:
            with conn:
                with conn.cursor() as cur:
                    # helper to safely execute batch with savepoint
                    def _exec(sql: str, rows: list, fetch: bool = False):
                        if not rows:
                            return []
                        cur.execute("SAVEPOINT sp")
                        try:
                            return execute_values(cur, sql, rows, fetch=fetch, page_size=min(1000, len(rows)))
                        except Exception:
                            logger.error("batch insert failed â€“ rolled back", exc_info=True)
                            cur.execute("ROLLBACK TO SAVEPOINT sp")
                            return []

                    # form_npx
                    form_cols = [
                        "reporting_person_name","phone_number","address_street1","address_street2","address_city","address_state","address_zip","accession_number","is_parsable","cik","period_of_report_raw","conformed_period","date_filed","report_type","form_type","sec_file_number","crd_number","sec_file_number_other","lei_number","investment_company_type","confidential_treatment","is_notice_report","explanatory_choice","other_included_managers_count","series_count","is_amendment","amendment_no","amendment_type","notice_explanation","explanatory_notes","signatory_name","signatory_name_printed","signatory_title","signatory_date",
                    ]
                    form_rows = [tuple(f.get(c) for c in form_cols) for f in self.FORMS]
                    f_ids = _exec(
                        f"INSERT INTO form_npx ({','.join(form_cols)}) VALUES %s RETURNING form_id",
                        form_rows,
                        fetch=True,
                    )
                    m_form = {f["local_form"]: fid[0] for f, fid in zip(self.FORMS, f_ids)}

                    # institutional_manager
                    mgr_rows = [(
                        m_form[m["local_form"]], m["serial_no"], m["name"], m["form13f_number"], m["crd_number"], m["sec_file_number"], m["lei_number"],
                    ) for m in self.MANAGERS]
                    m_ids = _exec(
                        "INSERT INTO institutional_manager (form_id,serial_no,name,form13f_number,crd_number,sec_file_number,lei_number) VALUES %s RETURNING manager_id,form_id,serial_no",
                        mgr_rows,
                        fetch=True,
                    )
                    m_lookup = {(fid, sn): mid for mid, fid, sn in m_ids}

                    # series
                    ser_rows = [(
                        m_form[s["local_form"]], s["series_code"], s["series_name"], s["series_lei"],
                    ) for s in self.SERIES]
                    ser_ret = _exec(
                        "INSERT INTO series (form_id,series_code,series_name,series_lei) VALUES %s RETURNING series_id,form_id,series_code",
                        ser_rows,
                        fetch=True,
                    )
                    ser_lookup = {(fid, sc): sid for sid, fid, sc in ser_ret}

                    # series_class
                    ready_sc, pending_sc, sc_seen = [], [], set()
                    for sc in self.SERIES_CLASS + self.PENDING_SERIES_CLASS:
                        fid = m_form.get(sc["local_form"])
                        sid = ser_lookup.get((fid, sc["series_code"]))
                        if not sid:
                            pending_sc.append(sc)
                            continue
                        pair = (sid, sc["class_id"])
                        if pair not in sc_seen:
                            ready_sc.append((sid, sc["class_id"], sc["class_name"]))
                            sc_seen.add(pair)
                    _exec("INSERT INTO series_class (series_id,class_id,class_name) VALUES %s", ready_sc)
                    self.PENDING_SERIES_CLASS = pending_sc
                    self.SERIES_CLASS = []

                    # other_reporting_person
                    op_rows = [(
                        m_form[o["local_form"]], o["ica_form13f"], o["crd_number"], o["sec_file_number"], o["lei_number"], o["name"],
                    ) for o in self.OTHER_PERSONS]
                    _exec("INSERT INTO other_reporting_person (form_id,ica_form13f,crd_number,sec_file_number,lei_number,name) VALUES %s", op_rows)

                    # matter_category upsert
                    if self.CATS:
                        cat_ret = _exec(
                            "INSERT INTO matter_category (category_type) VALUES %s ON CONFLICT(category_type) DO UPDATE SET category_type=EXCLUDED.category_type RETURNING category_id,category_type",
                            [(c["category_type"],) for c in self.CATS],
                            fetch=True,
                        )
                        cat_id_map = {t: c for c, t in cat_ret}
                        local_to_db_cat = {v: cat_id_map[k] for k, v in self.CAT_LOOKUP.items() if k in cat_id_map}
                    else:
                        local_to_db_cat = {}

                    # proxy_voting_record
                    vote_rows = [(
                        m_form[v["local_form"]], v["issuer_name"], v["cusip"], v["isin"], v["figi"], v["meeting_date"], v["vote_description"], v["proposed_by"], v["shares_voted"], v["shares_on_loan"], v["vote_cast"], v["vote_cast_shares"], v["management_rec"], v["other_notes"],
                    ) for v in self.VOTES]
                    vote_ret = _exec(
                        "INSERT INTO proxy_voting_record (form_id,issuer_name,cusip,isin,figi,meeting_date,vote_description,proposed_by,shares_voted,shares_on_loan,vote_cast,vote_cast_shares,management_rec,other_notes) VALUES %s RETURNING vote_id",
                        vote_rows,
                        fetch=True,
                    )
                    vote_id_map = {v["local_vote"]: vid[0] for v, vid in zip(self.VOTES, vote_ret)}

                    # proxy_voting_record_category
                    cat_bridge_rows = [(
                        vote_id_map.get(vc["local_vote"]), local_to_db_cat.get(vc["local_cat_id"])
                    ) for vc in self.VOTE_CATS if vote_id_map.get(vc["local_vote"]) and local_to_db_cat.get(vc["local_cat_id"])]
                    _exec("INSERT INTO proxy_voting_record_category (vote_id,category_id) VALUES %s ON CONFLICT DO NOTHING", cat_bridge_rows)

                    # voting_record_manager
                    vrm_rows = [(
                        vote_id_map.get(vm["local_vote"]), m_lookup.get((m_form.get(vm["local_form"]), vm["serial_no"]))
                    ) for vm in self.VOTE_MGR if vote_id_map.get(vm["local_vote"]) and m_lookup.get((m_form.get(vm["local_form"]), vm["serial_no"]))]
                    _exec("INSERT INTO voting_record_manager (vote_id,manager_id) VALUES %s ON CONFLICT DO NOTHING", vrm_rows)

                    # voting_record_series
                    vrs_rows = [(
                        vote_id_map.get(vs["local_vote"]), ser_lookup.get((m_form.get(vs["local_form"]), vs["series_code"]))
                    ) for vs in self.VOTE_SERIES if vote_id_map.get(vs["local_vote"]) and ser_lookup.get((m_form.get(vs["local_form"]), vs["series_code"]))]
                    _exec("INSERT INTO voting_record_series (vote_id,series_id) VALUES %s ON CONFLICT DO NOTHING", vrs_rows)

        finally:
            conn.close()
        self.reset_stage(clear_pending=False)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ main loop â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def run(self, folder: Path):
        counter = 0
        for f in folder.iterdir():
            if f.suffix.lower() == ".txt":
                self.parse_txt_filing(f)
                counter += 1
                if counter % self.flush_every == 0:
                    self.flush_to_db()
        self.flush_to_db()
        while self.SERIES_CLASS or self.PENDING_SERIES_CLASS:
            self.flush_to_db()
        logger.info("finished â€“ %d files processed", counter)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main cmd helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main(folder: str, flush_every: int = 250, log_level: str = "INFO"):
    load_dotenv()
    logging.root.setLevel(log_level.upper())

    conn_params = {
        "host": os.getenv("PGHOST", "localhost"),
        "port": int(os.getenv("PGPORT", 5432)),
        "dbname": os.getenv("PGDATABASE", "npx"),
        "user": os.getenv("PGUSER", "postgres"),
        "password": os.getenv("PGPASSWORD", "postgres"),
    }
    if not conn_params["password"] or conn_params["password"] in {"postgres", ""}:
        raise SystemExit("ðŸ’¥ Refusing to run with default / empty DB password")

    loader = NpxLoader(conn_params, flush_every=flush_every)
    loader.run(Path(folder))

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python npx_loader_notebook.py <folder> [flush] [loglevel]")
        sys.exit(1)
    main(
        folder=sys.argv[1],
        flush_every=int(sys.argv[2]) if len(sys.argv) > 2 else 250,
        log_level=sys.argv[3] if len(sys.argv) > 3 else "INFO",
    )
