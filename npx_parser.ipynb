{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# npx_parsing_notebook_hybrid.ipynb\n",
    "# A Jupyter Notebook Example for Parsing N-PX Filings from `npx_filings` folder,\n",
    "# with file size detection and an lxml.etree.iterparse approach for large docs.\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# For the iterparse approach:\n",
    "from lxml import etree\n",
    "\n",
    "NPX_DOWNLOAD_DIR = \"./npx_filings\"\n",
    "FILE_SIZE_THRESHOLD = 50 * 1024 * 1024  # e.g., 50 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 1. Streaming Approach for \"smaller\" files\n",
    "################################################################################\n",
    "\n",
    "def parse_sec_header_streaming(filepath):\n",
    "    \"\"\"\n",
    "    Reads the file line-by-line until we hit the first <DOCUMENT> or run out of lines.\n",
    "    Extracts fields like AMENDMENT_NO, ACCESSION_NUMBER, FILING_DATE, etc.\n",
    "    \"\"\"\n",
    "    header_info = {\n",
    "        'accessionNumber': \"\",\n",
    "        'filingDate': \"\",\n",
    "        'conformedPeriod': \"\",\n",
    "        'headerCik': \"\",\n",
    "        'amendmentNo': \"\"\n",
    "    }\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line_upper = line.upper()\n",
    "            if \"<DOCUMENT>\" in line_upper:\n",
    "                break\n",
    "\n",
    "            if \"AMENDMENT NO:\" in line_upper:\n",
    "                val = line.split(\"NO:\")[-1].strip()\n",
    "                header_info['amendmentNo'] = val\n",
    "            elif \"ACCESSION NUMBER:\" in line_upper:\n",
    "                val = line.split(\"NUMBER:\")[-1].strip()\n",
    "                header_info['accessionNumber'] = val\n",
    "            elif \"FILED AS OF DATE:\" in line_upper:\n",
    "                val = line.split(\"DATE:\")[-1].strip()\n",
    "                header_info['filingDate'] = val\n",
    "            elif \"CONFORMED PERIOD OF REPORT:\" in line_upper:\n",
    "                val = line.split(\"REPORT:\")[-1].strip()\n",
    "                header_info['conformedPeriod'] = val\n",
    "            elif \"CENTRAL INDEX KEY:\" in line_upper:\n",
    "                val = line.split(\"KEY:\")[-1].strip()\n",
    "                header_info['headerCik'] = val\n",
    "\n",
    "    return header_info\n",
    "\n",
    "def stream_documents(filepath):\n",
    "    \"\"\"\n",
    "    Generator that yields (doc_type, doc_content) for each <DOCUMENT> block,\n",
    "    reading line-by-line.\n",
    "    \"\"\"\n",
    "    inside_document = False\n",
    "    doc_lines = []\n",
    "    doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "    type_regex = re.compile(r\"<TYPE>(?P<type>[^\\r\\n<]+)\", re.IGNORECASE)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            upper_line = line.upper()\n",
    "\n",
    "            if \"<DOCUMENT>\" in upper_line:\n",
    "                inside_document = True\n",
    "                doc_lines = [line]\n",
    "                doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "            elif \"</DOCUMENT>\" in upper_line and inside_document:\n",
    "                doc_lines.append(line)\n",
    "                doc_text = \"\".join(doc_lines)\n",
    "                yield (doc_type.upper(), doc_text)\n",
    "\n",
    "                inside_document = False\n",
    "                doc_lines = []\n",
    "                doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "            else:\n",
    "                if inside_document:\n",
    "                    doc_lines.append(line)\n",
    "                    match = type_regex.search(line)\n",
    "                    if match:\n",
    "                        doc_type = match.group(\"type\").strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 2. Helper Extraction Functions (smaller-file approach)\n",
    "################################################################################\n",
    "\n",
    "def extract_primary_doc_xml(document_text):\n",
    "    filename_match = re.search(r\"<FILENAME>(.*?)</FILENAME>\", document_text, re.IGNORECASE)\n",
    "    filename = filename_match.group(1).strip() if filename_match else \"\"\n",
    "    if \"primary_doc.xml\" in filename.lower():\n",
    "        text_match = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match.group(\"xml\") if text_match else None\n",
    "    return None\n",
    "\n",
    "def extract_vote_table_xml(document_text):\n",
    "    type_match = re.search(r\"<TYPE>(?P<type>[^\\r\\n<]+)\", document_text, re.IGNORECASE)\n",
    "    doc_type = type_match.group(\"type\").strip() if type_match else \"\"\n",
    "    if \"PROXY VOTING RECORD\" in doc_type.upper():\n",
    "        text_match = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match.group(\"xml\") if text_match else None\n",
    "\n",
    "    desc_match = re.search(r\"<DESCRIPTION>(?P<desc>[^\\r\\n<]+)\", document_text, re.IGNORECASE)\n",
    "    desc = desc_match.group(\"desc\").strip() if desc_match else \"\"\n",
    "    if \"VOTE TABLE\" in desc.upper():\n",
    "        text_match2 = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match2.group(\"xml\") if text_match2 else None\n",
    "    return None\n",
    "\n",
    "def parse_primary_npx_xml(xml_str):\n",
    "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
    "    edgar_sub = soup.find(\"edgarSubmission\")\n",
    "    if not edgar_sub:\n",
    "        return {}\n",
    "\n",
    "    doc_info = {}\n",
    "    series_id_tag = edgar_sub.find(\"seriesId\")\n",
    "    doc_info[\"seriesId\"] = series_id_tag.text.strip() if series_id_tag else \"\"\n",
    "\n",
    "    period_of_report = edgar_sub.find(\"periodOfReport\")\n",
    "    submission_type = edgar_sub.find(\"submissionType\")\n",
    "    registrant_type = edgar_sub.find(\"registrantType\")\n",
    "    investment_type = edgar_sub.find(\"investmentCompanyType\")\n",
    "\n",
    "    doc_info[\"periodOfReport\"] = period_of_report.text.strip() if period_of_report else \"\"\n",
    "    doc_info[\"submissionType\"] = submission_type.text.strip() if submission_type else \"\"\n",
    "    doc_info[\"registrantType\"] = registrant_type.text.strip() if registrant_type else \"\"\n",
    "    doc_info[\"investmentCompanyType\"] = investment_type.text.strip() if investment_type else \"\"\n",
    "    # you can parse more fields if needed\n",
    "\n",
    "    return doc_info\n",
    "\n",
    "def parse_vote_table_xml(xml_str):\n",
    "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
    "    vote_tables = soup.find_all([\"proxyTable\", \"inf:proxyTable\"])\n",
    "    results = []\n",
    "    for vt in vote_tables:\n",
    "        row = {}\n",
    "        row[\"issuerName\"] = vt.find_text(\"issuerName\")\n",
    "        row[\"cusip\"] = vt.find_text(\"cusip\")\n",
    "        row[\"isin\"] = vt.find_text(\"isin\")\n",
    "        row[\"figi\"] = vt.find_text(\"figi\")\n",
    "        row[\"meetingDate\"] = vt.find_text(\"meetingDate\")\n",
    "        row[\"sharesVoted\"] = vt.find_text(\"sharesVoted\")\n",
    "        row[\"howVoted\"] = vt.find_text(\"howVoted\")\n",
    "        row[\"managementRecommendation\"] = vt.find_text(\"managementRecommendation\")\n",
    "        row[\"voteDescription\"] = vt.find_text(\"voteDescription\")\n",
    "\n",
    "        # forAgainstMgmt\n",
    "        if row[\"howVoted\"] and row[\"managementRecommendation\"]:\n",
    "            if row[\"howVoted\"].upper() == row[\"managementRecommendation\"].upper():\n",
    "                row[\"forAgainstMgmt\"] = \"FOR\"\n",
    "            else:\n",
    "                row[\"forAgainstMgmt\"] = \"AGAINST\"\n",
    "        else:\n",
    "            row[\"forAgainstMgmt\"] = \"\"\n",
    "\n",
    "        results.append(row)\n",
    "    return results\n",
    "\n",
    "def parse_npx_file_streaming(filepath):\n",
    "    header_info = parse_sec_header_streaming(filepath)\n",
    "    doc_info = dict(header_info)\n",
    "    all_votes = []\n",
    "\n",
    "    # Stream the docs\n",
    "    for doc_type, doc_content in stream_documents(filepath):\n",
    "        if \"N-PX\" in doc_type:\n",
    "            possible_xml = extract_primary_doc_xml(doc_content)\n",
    "            if possible_xml:\n",
    "                info = parse_primary_npx_xml(possible_xml)\n",
    "                doc_info.update(info)\n",
    "        if \"PROXY VOTING RECORD\" in doc_type or \"VOTE TABLE\" in doc_type:\n",
    "            vote_xml = extract_vote_table_xml(doc_content)\n",
    "            if vote_xml:\n",
    "                votes = parse_vote_table_xml(vote_xml)\n",
    "                all_votes.extend(votes)\n",
    "    return doc_info, all_votes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 3. lxml.etree.iterparse approach for big single-doc files\n",
    "################################################################################\n",
    "\n",
    "def parse_npx_file_iterparse(filepath):\n",
    "    \"\"\"\n",
    "    Example of using lxml.etree.iterparse if the file is a single large well-formed XML doc.\n",
    "    In reality, EDGAR .txt might not be strictly well-formed if it has multiple <DOCUMENT> blocks.\n",
    "    This is a simplified demonstration.\n",
    "    \"\"\"\n",
    "    # If the entire file is one giant XML doc, we can do an iterparse. We'll assume we skip the\n",
    "    # standard <SEC-HEADER> approach for now, or parse it separately if you like.\n",
    "    # Example: We parse 'edgarSubmission' nodes, searching for seriesId, etc.\n",
    "\n",
    "    # We'll store minimal info in doc_info + gather votes as we see them\n",
    "    doc_info = {}\n",
    "    all_votes = []\n",
    "\n",
    "    # Possibly parse header lines first if you want the same \"header_info\" from the top lines\n",
    "    header_info = parse_sec_header_streaming(filepath)\n",
    "    doc_info.update(header_info)\n",
    "\n",
    "    # iterparse expects a well-formed XML doc\n",
    "    # If the file has multiple <DOCUMENT> blocks, you might do a prior step to isolate the big one.\n",
    "    context = etree.iterparse(filepath, events=('start', 'end'), recover=True, encoding='utf-8')\n",
    "    # We'll track state\n",
    "    current_vote = None\n",
    "\n",
    "    for event, elem in context:\n",
    "        tag_name = etree.QName(elem.tag).localname\n",
    "\n",
    "        if event == 'end':\n",
    "            # If we see e.g. <seriesId> and we haven't set doc_info[\"seriesId\"] yet, do so\n",
    "            if tag_name == \"seriesId\":\n",
    "                doc_info[\"seriesId\"] = (elem.text or \"\").strip()\n",
    "\n",
    "            # If we see <issuerName> or <cusip> etc., perhaps we are in a <proxyTable> or <inf:proxyTable>.\n",
    "            # This requires a more advanced approach if we have repeated tables.\n",
    "            if tag_name in [\"proxyTable\", \"inf:proxyTable\"]:\n",
    "                # We might finalize a record. This example is simplistic:\n",
    "                # We could parse deeper if needed\n",
    "                pass\n",
    "\n",
    "        # Clear the element to free memory\n",
    "        elem.clear()\n",
    "\n",
    "    # This demonstration is incomplete for a full N-PX parse with iterparse, because\n",
    "    # EDGAR files often are not single well-formed XML doc. But it shows how you'd do it\n",
    "    # for a purely giant single doc.\n",
    "\n",
    "    return doc_info, all_votes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 4. Hybrid parse_npx_file that chooses approach based on file size\n",
    "################################################################################\n",
    "\n",
    "def parse_npx_file_hybrid(filepath, size_threshold=FILE_SIZE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    1) Check file size\n",
    "    2) If less than threshold => use parse_npx_file_streaming\n",
    "    3) If bigger => use parse_npx_file_iterparse\n",
    "       (But keep in mind you might need a more robust logic if\n",
    "        the large file has multiple <DOCUMENT> blocks.)\n",
    "    \"\"\"\n",
    "    file_size = os.path.getsize(filepath)\n",
    "\n",
    "    if file_size < size_threshold:\n",
    "        return parse_npx_file_streaming(filepath)\n",
    "    else:\n",
    "        print(f\"File is {file_size} bytes, using iterparse approach for {filepath}\")\n",
    "        return parse_npx_file_iterparse(filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml-xml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m all_files:\n\u001b[0;32m     11\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(NPX_DOWNLOAD_DIR, filename)\n\u001b[1;32m---> 12\u001b[0m     doc_info, all_votes \u001b[38;5;241m=\u001b[39m \u001b[43mparse_npx_file_hybrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m doc_info:\n\u001b[0;32m     15\u001b[0m         doc_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m, in \u001b[0;36mparse_npx_file_hybrid\u001b[1;34m(filepath, size_threshold)\u001b[0m\n\u001b[0;32m     13\u001b[0m file_size \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(filepath)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_size \u001b[38;5;241m<\u001b[39m size_threshold:\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_npx_file_streaming\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes, using iterparse approach for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 93\u001b[0m, in \u001b[0;36mparse_npx_file_streaming\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     91\u001b[0m         vote_xml \u001b[38;5;241m=\u001b[39m extract_vote_table_xml(doc_content)\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vote_xml:\n\u001b[1;32m---> 93\u001b[0m             votes \u001b[38;5;241m=\u001b[39m \u001b[43mparse_vote_table_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvote_xml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m             all_votes\u001b[38;5;241m.\u001b[39mextend(votes)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc_info, all_votes\n",
      "Cell \u001b[1;32mIn[17], line 51\u001b[0m, in \u001b[0;36mparse_vote_table_xml\u001b[1;34m(xml_str)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse_vote_table_xml\u001b[39m(xml_str):\n\u001b[1;32m---> 51\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml-xml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     vote_tables \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxyTable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf:proxyTable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     53\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\skell\\Documents\\Projects\\edgar-fun\\venv\\lib\\site-packages\\bs4\\__init__.py:364\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     possible_builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[0;32m    365\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    367\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features)\n\u001b[0;32m    368\u001b[0m         )\n\u001b[0;32m    369\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m possible_builder_class\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[1;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml-xml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# 5. Putting It All Together\n",
    "################################################################################\n",
    "\n",
    "metadata_records = []\n",
    "votes_records = []\n",
    "\n",
    "all_files = [f for f in os.listdir(NPX_DOWNLOAD_DIR) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "for filename in all_files:\n",
    "    filepath = os.path.join(NPX_DOWNLOAD_DIR, filename)\n",
    "    doc_info, all_votes = parse_npx_file_hybrid(filepath)\n",
    "\n",
    "    if doc_info:\n",
    "        doc_info[\"filename\"] = filename\n",
    "        metadata_records.append(doc_info)\n",
    "\n",
    "    for v in all_votes:\n",
    "        v[\"filename\"] = filename\n",
    "        votes_records.append(v)\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_records)\n",
    "df_votes = pd.DataFrame(votes_records)\n",
    "\n",
    "# Merge them if you want a single CSV\n",
    "if not df_metadata.empty and not df_votes.empty:\n",
    "    df_final = df_votes.merge(df_metadata, on=\"filename\", how=\"inner\")\n",
    "else:\n",
    "    df_final = pd.DataFrame()\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Vote rows: {len(df_votes)}\")\n",
    "print(f\"Final joined rows: {len(df_final)}\")\n",
    "\n",
    "df_metadata.to_csv(\"parsed_npx_metadata.csv\", index=False)\n",
    "df_votes.to_csv(\"parsed_npx_votes.csv\", index=False)\n",
    "df_final.to_csv(\"parsed_npx_final.csv\", index=False)\n",
    "\n",
    "print(\"Hybrid parse (size-based) complete! See CSV outputs for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
