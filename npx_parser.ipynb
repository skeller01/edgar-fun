{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# npx_parsing_notebook_hybrid_fixed.ipynb\n",
    "# A Jupyter Notebook Example for Parsing N-PX Filings from `npx_filings` folder,\n",
    "# with file size detection and an lxml.etree.iterparse approach for large docs.\n",
    "# AND with bug fixes related to .find_text(...) calls.\n",
    "################################################################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree  # for iterparse in large-file scenario\n",
    "\n",
    "NPX_DOWNLOAD_DIR = \"./npx_filings\"\n",
    "FILE_SIZE_THRESHOLD = 50 * 1024 * 1024  # e.g., 50 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 1. Streaming Approach for \"smaller\" files\n",
    "################################################################################\n",
    "\n",
    "def parse_sec_header_streaming(filepath):\n",
    "    \"\"\"\n",
    "    Reads the file line-by-line until we hit the first <DOCUMENT> or run out of lines.\n",
    "    Extracts fields like AMENDMENT_NO, ACCESSION_NUMBER, FILING_DATE, etc.\n",
    "    \"\"\"\n",
    "    header_info = {\n",
    "        'accessionNumber': \"\",\n",
    "        'filingDate': \"\",\n",
    "        'conformedPeriod': \"\",\n",
    "        'headerCik': \"\",\n",
    "        'amendmentNo': \"\"\n",
    "    }\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line_upper = line.upper()\n",
    "            if \"<DOCUMENT>\" in line_upper:\n",
    "                break\n",
    "\n",
    "            if \"AMENDMENT NO:\" in line_upper:\n",
    "                val = line.split(\"NO:\")[-1].strip()\n",
    "                header_info['amendmentNo'] = val\n",
    "            elif \"ACCESSION NUMBER:\" in line_upper:\n",
    "                val = line.split(\"NUMBER:\")[-1].strip()\n",
    "                header_info['accessionNumber'] = val\n",
    "            elif \"FILED AS OF DATE:\" in line_upper:\n",
    "                val = line.split(\"DATE:\")[-1].strip()\n",
    "                header_info['filingDate'] = val\n",
    "            elif \"CONFORMED PERIOD OF REPORT:\" in line_upper:\n",
    "                val = line.split(\"REPORT:\")[-1].strip()\n",
    "                header_info['conformedPeriod'] = val\n",
    "            elif \"CENTRAL INDEX KEY:\" in line_upper:\n",
    "                val = line.split(\"KEY:\")[-1].strip()\n",
    "                header_info['headerCik'] = val\n",
    "\n",
    "    return header_info\n",
    "\n",
    "\n",
    "def stream_documents(filepath):\n",
    "    \"\"\"\n",
    "    Generator that yields (doc_type, doc_content) for each <DOCUMENT> block,\n",
    "    reading line-by-line.\n",
    "    \"\"\"\n",
    "    inside_document = False\n",
    "    doc_lines = []\n",
    "    doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "    type_regex = re.compile(r\"<TYPE>(?P<type>[^\\r\\n<]+)\", re.IGNORECASE)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            upper_line = line.upper()\n",
    "\n",
    "            if \"<DOCUMENT>\" in upper_line:\n",
    "                inside_document = True\n",
    "                doc_lines = [line]\n",
    "                doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "            elif \"</DOCUMENT>\" in upper_line and inside_document:\n",
    "                doc_lines.append(line)\n",
    "                doc_text = \"\".join(doc_lines)\n",
    "                yield (doc_type.upper(), doc_text)\n",
    "\n",
    "                inside_document = False\n",
    "                doc_lines = []\n",
    "                doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "            else:\n",
    "                if inside_document:\n",
    "                    doc_lines.append(line)\n",
    "                    match = type_regex.search(line)\n",
    "                    if match:\n",
    "                        doc_type = match.group(\"type\").strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 2. Helper Extraction Functions (smaller-file approach)\n",
    "################################################################################\n",
    "\n",
    "def extract_primary_doc_xml(document_text):\n",
    "    filename_match = re.search(r\"<FILENAME>(.*?)</FILENAME>\", document_text, re.IGNORECASE)\n",
    "    filename = filename_match.group(1).strip() if filename_match else \"\"\n",
    "    if \"primary_doc.xml\" in filename.lower():\n",
    "        text_match = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match.group(\"xml\") if text_match else None\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_vote_table_xml(document_text):\n",
    "    type_match = re.search(r\"<TYPE>(?P<type>[^\\r\\n<]+)\", document_text, re.IGNORECASE)\n",
    "    doc_type = type_match.group(\"type\").strip() if type_match else \"\"\n",
    "    if \"PROXY VOTING RECORD\" in doc_type.upper():\n",
    "        text_match = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match.group(\"xml\") if text_match else None\n",
    "\n",
    "    desc_match = re.search(r\"<DESCRIPTION>(?P<desc>[^\\r\\n<]+)\", document_text, re.IGNORECASE)\n",
    "    desc = desc_match.group(\"desc\").strip() if desc_match else \"\"\n",
    "    if \"VOTE TABLE\" in desc.upper():\n",
    "        text_match2 = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match2.group(\"xml\") if text_match2 else None\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_primary_npx_xml(xml_str):\n",
    "    # Use lxml-xml parser to avoid \"FeatureNotFound: xml\" error\n",
    "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
    "    edgar_sub = soup.find(\"edgarSubmission\")\n",
    "    if not edgar_sub:\n",
    "        return {}\n",
    "\n",
    "    doc_info = {}\n",
    "    series_id_tag = edgar_sub.find(\"seriesId\")\n",
    "    doc_info[\"seriesId\"] = series_id_tag.get_text(strip=True) if series_id_tag else \"\"\n",
    "\n",
    "    period_of_report = edgar_sub.find(\"periodOfReport\")\n",
    "    submission_type = edgar_sub.find(\"submissionType\")\n",
    "    registrant_type = edgar_sub.find(\"registrantType\")\n",
    "    investment_type = edgar_sub.find(\"investmentCompanyType\")\n",
    "\n",
    "    doc_info[\"periodOfReport\"] = period_of_report.get_text(strip=True) if period_of_report else \"\"\n",
    "    doc_info[\"submissionType\"] = submission_type.get_text(strip=True) if submission_type else \"\"\n",
    "    doc_info[\"registrantType\"] = registrant_type.get_text(strip=True) if registrant_type else \"\"\n",
    "    doc_info[\"investmentCompanyType\"] = investment_type.get_text(strip=True) if investment_type else \"\"\n",
    "    # parse more fields if needed (e.g. <reportType>, <amendmentType>)\n",
    "\n",
    "    return doc_info\n",
    "\n",
    "\n",
    "def parse_vote_table_xml(xml_str):\n",
    "    # Use lxml-xml parser\n",
    "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
    "    vote_tables = soup.find_all([\"proxyTable\", \"inf:proxyTable\"])\n",
    "\n",
    "    results = []\n",
    "    for vt in vote_tables:\n",
    "        row = {}\n",
    "\n",
    "        # Bug fix: Replacing vt.find_text(...) with standard .find(\"tag\") + .get_text()\n",
    "        issuer_tag = vt.find(\"issuerName\")\n",
    "        row[\"issuerName\"] = issuer_tag.get_text(strip=True) if issuer_tag else \"\"\n",
    "\n",
    "        cusip_tag = vt.find(\"cusip\")\n",
    "        row[\"cusip\"] = cusip_tag.get_text(strip=True) if cusip_tag else \"\"\n",
    "\n",
    "        isin_tag = vt.find(\"isin\")\n",
    "        row[\"isin\"] = isin_tag.get_text(strip=True) if isin_tag else \"\"\n",
    "\n",
    "        figi_tag = vt.find(\"figi\")\n",
    "        row[\"figi\"] = figi_tag.get_text(strip=True) if figi_tag else \"\"\n",
    "\n",
    "        meeting_date_tag = vt.find(\"meetingDate\")\n",
    "        row[\"meetingDate\"] = meeting_date_tag.get_text(strip=True) if meeting_date_tag else \"\"\n",
    "\n",
    "        shares_voted_tag = vt.find(\"sharesVoted\")\n",
    "        row[\"sharesVoted\"] = shares_voted_tag.get_text(strip=True) if shares_voted_tag else \"\"\n",
    "\n",
    "        how_voted_tag = vt.find(\"howVoted\")\n",
    "        row[\"howVoted\"] = how_voted_tag.get_text(strip=True) if how_voted_tag else \"\"\n",
    "\n",
    "        mgmt_rec_tag = vt.find(\"managementRecommendation\")\n",
    "        row[\"managementRecommendation\"] = mgmt_rec_tag.get_text(strip=True) if mgmt_rec_tag else \"\"\n",
    "\n",
    "        vote_desc_tag = vt.find(\"voteDescription\")\n",
    "        row[\"voteDescription\"] = vote_desc_tag.get_text(strip=True) if vote_desc_tag else \"\"\n",
    "\n",
    "        # forAgainstMgmt logic\n",
    "        if row[\"howVoted\"] and row[\"managementRecommendation\"]:\n",
    "            row[\"forAgainstMgmt\"] = \"FOR\" if row[\"howVoted\"].upper() == row[\"managementRecommendation\"].upper() else \"AGAINST\"\n",
    "        else:\n",
    "            row[\"forAgainstMgmt\"] = \"\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_npx_file_streaming(filepath):\n",
    "    header_info = parse_sec_header_streaming(filepath)\n",
    "    doc_info = dict(header_info)\n",
    "    all_votes = []\n",
    "\n",
    "    for doc_type, doc_content in stream_documents(filepath):\n",
    "        if \"N-PX\" in doc_type:\n",
    "            possible_xml = extract_primary_doc_xml(doc_content)\n",
    "            if possible_xml:\n",
    "                info = parse_primary_npx_xml(possible_xml)\n",
    "                doc_info.update(info)\n",
    "        if \"PROXY VOTING RECORD\" in doc_type or \"VOTE TABLE\" in doc_type:\n",
    "            vote_xml = extract_vote_table_xml(doc_content)\n",
    "            if vote_xml:\n",
    "                votes = parse_vote_table_xml(vote_xml)\n",
    "                all_votes.extend(votes)\n",
    "\n",
    "    return doc_info, all_votes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 3. lxml.etree.iterparse approach for big single-doc files\n",
    "################################################################################\n",
    "\n",
    "def parse_npx_file_iterparse(filepath):\n",
    "    \"\"\"\n",
    "    Example of using lxml.etree.iterparse if the file is a single large well-formed XML doc.\n",
    "    In reality, EDGAR .txt might not be strictly well-formed if it has multiple <DOCUMENT> blocks.\n",
    "    This is a simplified demonstration.\n",
    "    \"\"\"\n",
    "    doc_info = {}\n",
    "    all_votes = []\n",
    "\n",
    "    # Possibly parse header lines if desired\n",
    "    header_info = parse_sec_header_streaming(filepath)\n",
    "    doc_info.update(header_info)\n",
    "\n",
    "    context = etree.iterparse(filepath, events=('start', 'end'), recover=True, encoding='utf-8')\n",
    "    for event, elem in context:\n",
    "        tag_name = etree.QName(elem.tag).localname\n",
    "\n",
    "        if event == 'end':\n",
    "            if tag_name == \"seriesId\":\n",
    "                doc_info[\"seriesId\"] = (elem.text or \"\").strip()\n",
    "\n",
    "            # In a real scenario, parse <proxyTable> or <inf:proxyTable> to create vote records\n",
    "            if tag_name in [\"proxyTable\", \"inf:proxyTable\"]:\n",
    "                # This is a placeholder approach\n",
    "                pass\n",
    "\n",
    "        elem.clear()\n",
    "\n",
    "    return doc_info, all_votes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# 4. Hybrid parse_npx_file that chooses approach based on file size\n",
    "################################################################################\n",
    "\n",
    "def parse_npx_file_hybrid(filepath, size_threshold=FILE_SIZE_THRESHOLD):\n",
    "    file_size = os.path.getsize(filepath)\n",
    "\n",
    "    if file_size < size_threshold:\n",
    "        return parse_npx_file_streaming(filepath)\n",
    "    else:\n",
    "        print(f\"File is {file_size} bytes, using iterparse approach for {filepath}\")\n",
    "        return parse_npx_file_iterparse(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is 97342826 bytes, using iterparse approach for ./npx_filings\\2024-08-28_N-PX_0001021408-24-005385.txt\n",
      "Metadata rows: 25\n",
      "Vote rows: 10940\n",
      "Final joined rows: 10940\n",
      "Hybrid parse (size-based) complete! See CSV outputs for details.\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# 5. Putting It All Together\n",
    "################################################################################\n",
    "\n",
    "metadata_records = []\n",
    "votes_records = []\n",
    "\n",
    "all_files = [f for f in os.listdir(NPX_DOWNLOAD_DIR) if f.lower().endswith(\".txt\")]\n",
    "\n",
    "for filename in all_files:\n",
    "    filepath = os.path.join(NPX_DOWNLOAD_DIR, filename)\n",
    "\n",
    "    doc_info, all_votes = parse_npx_file_hybrid(filepath)\n",
    "\n",
    "    if doc_info:\n",
    "        doc_info[\"filename\"] = filename\n",
    "        metadata_records.append(doc_info)\n",
    "\n",
    "    for v in all_votes:\n",
    "        v[\"filename\"] = filename\n",
    "        votes_records.append(v)\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_records)\n",
    "df_votes = pd.DataFrame(votes_records)\n",
    "\n",
    "# Merge them if you want a single CSV\n",
    "if not df_metadata.empty and not df_votes.empty:\n",
    "    df_final = df_votes.merge(df_metadata, on=\"filename\", how=\"inner\")\n",
    "else:\n",
    "    df_final = pd.DataFrame()\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Vote rows: {len(df_votes)}\")\n",
    "print(f\"Final joined rows: {len(df_final)}\")\n",
    "\n",
    "df_metadata.to_csv(\"parsed_npx_metadata.csv\", index=False)\n",
    "df_votes.to_csv(\"parsed_npx_votes.csv\", index=False)\n",
    "df_final.to_csv(\"parsed_npx_final.csv\", index=False)\n",
    "\n",
    "print(\"Hybrid parse (size-based) complete! See CSV outputs for details.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
