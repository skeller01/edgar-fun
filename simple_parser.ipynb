{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "npx_parser_simple.ipynb\n",
    "-----------------------\n",
    "A simplified N-PX parsing demonstration focused on the core algorithm:\n",
    "1) Read the SEC header to extract file-level metadata.\n",
    "2) Stream each <DOCUMENT> block to identify \"primary_doc.xml\" or vote tables.\n",
    "3) Parse the relevant XML sections using BeautifulSoup (lxml-xml).\n",
    "4) Output final metadata and vote data as CSV.\n",
    "\n",
    "This removes advanced or redundant logic (e.g., iterparse for huge files,\n",
    "Trino queries, N-14/N-PORT references) to keep the example minimal.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Directory of .txt N-PX filings (already downloaded)\n",
    "NPX_DOWNLOAD_DIR = \"./npx_filings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1. Read/Parse SEC Header\n",
    "# ------------------------------------------------------------------------------\n",
    "def parse_sec_header(filepath):\n",
    "    \"\"\"\n",
    "    Reads file line-by-line until the first <DOCUMENT> is encountered.\n",
    "    Extracts basic header fields (e.g., ACCESSION NUMBER, FILING DATE).\n",
    "    \"\"\"\n",
    "    header_info = {\n",
    "        'accessionNumber': \"\",\n",
    "        'filingDate': \"\",\n",
    "        'conformedPeriod': \"\",\n",
    "        'headerCik': \"\",\n",
    "        'amendmentNo': \"\"\n",
    "    }\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line_upper = line.upper()\n",
    "            if \"<DOCUMENT>\" in line_upper:\n",
    "                break\n",
    "\n",
    "            if \"AMENDMENT NO:\" in line_upper:\n",
    "                val = line.split(\"NO:\")[-1].strip()\n",
    "                header_info['amendmentNo'] = val\n",
    "            elif \"ACCESSION NUMBER:\" in line_upper:\n",
    "                val = line.split(\"NUMBER:\")[-1].strip()\n",
    "                header_info['accessionNumber'] = val\n",
    "            elif \"FILED AS OF DATE:\" in line_upper:\n",
    "                val = line.split(\"DATE:\")[-1].strip()\n",
    "                header_info['filingDate'] = val\n",
    "            elif \"CONFORMED PERIOD OF REPORT:\" in line_upper:\n",
    "                val = line.split(\"REPORT:\")[-1].strip()\n",
    "                header_info['conformedPeriod'] = val\n",
    "            elif \"CENTRAL INDEX KEY:\" in line_upper:\n",
    "                val = line.split(\"KEY:\")[-1].strip()\n",
    "                header_info['headerCik'] = val\n",
    "\n",
    "    return header_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 2. Stream <DOCUMENT> Blocks\n",
    "# ------------------------------------------------------------------------------\n",
    "def stream_documents(filepath):\n",
    "    \"\"\"\n",
    "    Generator: yields (doc_type, doc_content) for each <DOCUMENT> block.\n",
    "    This approach is simpler than reading the entire file into memory\n",
    "    or dealing with lxml.etree.iterparse for huge files.\n",
    "    \"\"\"\n",
    "    inside_document = False\n",
    "    doc_lines = []\n",
    "    doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "    type_regex = re.compile(r\"<TYPE>(?P<type>[^\\r\\n<]+)\", re.IGNORECASE)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            upper_line = line.upper()\n",
    "\n",
    "            if \"<DOCUMENT>\" in upper_line:\n",
    "                inside_document = True\n",
    "                doc_lines = [line]  # start collecting lines\n",
    "                doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "            elif \"</DOCUMENT>\" in upper_line and inside_document:\n",
    "                doc_lines.append(line)\n",
    "                doc_text = \"\".join(doc_lines)\n",
    "                yield (doc_type.upper(), doc_text)\n",
    "\n",
    "                inside_document = False\n",
    "                doc_lines = []\n",
    "                doc_type = \"NO_TYPE_FOUND\"\n",
    "\n",
    "            else:\n",
    "                if inside_document:\n",
    "                    doc_lines.append(line)\n",
    "                    match = type_regex.search(line)\n",
    "                    if match:\n",
    "                        doc_type = match.group(\"type\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 3. Extract & Parse XML Blocks\n",
    "# ------------------------------------------------------------------------------\n",
    "def extract_primary_doc_xml(document_text):\n",
    "    \"\"\"\n",
    "    Identifies whether the <FILENAME> tag references 'primary_doc.xml'.\n",
    "    If so, extracts the <TEXT> content as raw XML.\n",
    "    \"\"\"\n",
    "    filename_match = re.search(r\"<FILENAME>(.*?)</FILENAME>\", document_text, re.IGNORECASE)\n",
    "    if not filename_match:\n",
    "        return None\n",
    "\n",
    "    filename = filename_match.group(1).strip()\n",
    "    if \"primary_doc.xml\" in filename.lower():\n",
    "        text_match = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match.group(\"xml\") if text_match else None\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_vote_table_xml(document_text):\n",
    "    \"\"\"\n",
    "    Identifies if the <TYPE> or <DESCRIPTION> indicates a PROXY VOTING RECORD or VOTE TABLE,\n",
    "    then grabs the <TEXT> content as raw XML.\n",
    "    \"\"\"\n",
    "    # Check <TYPE>\n",
    "    type_match = re.search(r\"<TYPE>(?P<type>[^\\r\\n<]+)\", document_text, re.IGNORECASE)\n",
    "    doc_type = type_match.group(\"type\").strip() if type_match else \"\"\n",
    "\n",
    "    if \"PROXY VOTING RECORD\" in doc_type.upper():\n",
    "        text_match = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match.group(\"xml\") if text_match else None\n",
    "\n",
    "    # Check <DESCRIPTION>\n",
    "    desc_match = re.search(r\"<DESCRIPTION>(?P<desc>[^\\r\\n<]+)\", document_text, re.IGNORECASE)\n",
    "    desc = desc_match.group(\"desc\").strip() if desc_match else \"\"\n",
    "    if \"VOTE TABLE\" in desc.upper():\n",
    "        text_match2 = re.search(r\"<TEXT>(?P<xml>.*?)</TEXT>\", document_text, re.IGNORECASE | re.DOTALL)\n",
    "        return text_match2.group(\"xml\") if text_match2 else None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 4. Parsing the Found XML with BeautifulSoup (lxml-xml)\n",
    "# ------------------------------------------------------------------------------\n",
    "def parse_primary_npx_xml(xml_str):\n",
    "    \"\"\"\n",
    "    Parses the 'primary_doc.xml' block to extract fields like seriesId, periodOfReport, etc.\n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
    "\n",
    "    edgar_sub = soup.find(\"edgarSubmission\")\n",
    "    if not edgar_sub:\n",
    "        return {}\n",
    "\n",
    "    doc_info = {}\n",
    "    # Example tags we commonly see in N-PX\n",
    "    doc_info[\"seriesId\"] = edgar_sub.find_text(\"seriesId\") or \"\"\n",
    "\n",
    "    doc_info[\"periodOfReport\"] = edgar_sub.find_text(\"periodOfReport\") or \"\"\n",
    "    doc_info[\"submissionType\"] = edgar_sub.find_text(\"submissionType\") or \"\"\n",
    "    doc_info[\"registrantType\"] = edgar_sub.find_text(\"registrantType\") or \"\"\n",
    "    doc_info[\"investmentCompanyType\"] = edgar_sub.find_text(\"investmentCompanyType\") or \"\"\n",
    "\n",
    "    return doc_info\n",
    "\n",
    "\n",
    "def parse_vote_table_xml(xml_str):\n",
    "    \"\"\"\n",
    "    Parses the proxy table to extract voting records: issuerName, cusip, howVoted, etc.\n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(xml_str, \"lxml-xml\")\n",
    "\n",
    "    # \"proxyTable\" or \"inf:proxyTable\"\n",
    "    vote_tables = soup.find_all([\"proxyTable\", \"inf:proxyTable\"])\n",
    "\n",
    "    results = []\n",
    "    for vt in vote_tables:\n",
    "        row = {}\n",
    "\n",
    "        issuer_tag = vt.find(\"issuerName\")\n",
    "        row[\"issuerName\"] = issuer_tag.get_text(strip=True) if issuer_tag else \"\"\n",
    "\n",
    "        cusip_tag = vt.find(\"cusip\")\n",
    "        row[\"cusip\"] = cusip_tag.get_text(strip=True) if cusip_tag else \"\"\n",
    "\n",
    "        isin_tag = vt.find(\"isin\")\n",
    "        row[\"isin\"] = isin_tag.get_text(strip=True) if isin_tag else \"\"\n",
    "\n",
    "        figi_tag = vt.find(\"figi\")\n",
    "        row[\"figi\"] = figi_tag.get_text(strip=True) if figi_tag else \"\"\n",
    "\n",
    "        meeting_date_tag = vt.find(\"meetingDate\")\n",
    "        row[\"meetingDate\"] = meeting_date_tag.get_text(strip=True) if meeting_date_tag else \"\"\n",
    "\n",
    "        shares_voted_tag = vt.find(\"sharesVoted\")\n",
    "        row[\"sharesVoted\"] = shares_voted_tag.get_text(strip=True) if shares_voted_tag else \"\"\n",
    "\n",
    "        how_voted_tag = vt.find(\"howVoted\")\n",
    "        row[\"howVoted\"] = how_voted_tag.get_text(strip=True) if how_voted_tag else \"\"\n",
    "\n",
    "        mgmt_rec_tag = vt.find(\"managementRecommendation\")\n",
    "        row[\"managementRecommendation\"] = mgmt_rec_tag.get_text(strip=True) if mgmt_rec_tag else \"\"\n",
    "\n",
    "        vote_desc_tag = vt.find(\"voteDescription\")\n",
    "        row[\"voteDescription\"] = vote_desc_tag.get_text(strip=True) if vote_desc_tag else \"\"\n",
    "\n",
    "        # Simple logic for \"forAgainstMgmt\"\n",
    "        if row[\"howVoted\"] and row[\"managementRecommendation\"]:\n",
    "            if row[\"howVoted\"].upper() == row[\"managementRecommendation\"].upper():\n",
    "                row[\"forAgainstMgmt\"] = \"FOR\"\n",
    "            else:\n",
    "                row[\"forAgainstMgmt\"] = \"AGAINST\"\n",
    "        else:\n",
    "            row[\"forAgainstMgmt\"] = \"\"\n",
    "\n",
    "        results.append(row)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 5. Main Parsing Function\n",
    "# ------------------------------------------------------------------------------\n",
    "def parse_npx_file(filepath):\n",
    "    \"\"\"\n",
    "    Single approach: line-by-line streaming to get doc_type and doc_content,\n",
    "    extract needed XML, parse them. Return (doc_info, all_votes).\n",
    "    \"\"\"\n",
    "    # 1. Basic header info\n",
    "    header_info = parse_sec_header(filepath)\n",
    "    doc_info = dict(header_info)\n",
    "    all_votes = []\n",
    "\n",
    "    # 2. For each <DOCUMENT> block, see if it's primary_doc.xml or a vote table\n",
    "    for doc_type, doc_content in stream_documents(filepath):\n",
    "        doc_type_upper = doc_type.upper()\n",
    "\n",
    "        # If we see \"N-PX\" in <TYPE> or something similar\n",
    "        if \"N-PX\" in doc_type_upper:\n",
    "            # Attempt to parse \"primary_doc.xml\"\n",
    "            primary_xml = extract_primary_doc_xml(doc_content)\n",
    "            if primary_xml:\n",
    "                p_info = parse_primary_npx_xml(primary_xml)\n",
    "                doc_info.update(p_info)\n",
    "\n",
    "        # If we see \"PROXY VOTING RECORD\" or \"VOTE TABLE\"\n",
    "        if (\"PROXY VOTING RECORD\" in doc_type_upper) or (\"VOTE TABLE\" in doc_type_upper):\n",
    "            vote_xml = extract_vote_table_xml(doc_content)\n",
    "            if vote_xml:\n",
    "                votes = parse_vote_table_xml(vote_xml)\n",
    "                all_votes.extend(votes)\n",
    "\n",
    "    return doc_info, all_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata entries: 25\n",
      "Vote entries: 115047\n",
      "Final joined entries: 115047\n",
      "Done! See CSV outputs for the simplified parse approach.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 6. Minimal Main Notebook Flow\n",
    "# ------------------------------------------------------------------------------\n",
    "metadata_list = []\n",
    "votes_list = []\n",
    "\n",
    "all_txt_files = [\n",
    "    f for f in os.listdir(NPX_DOWNLOAD_DIR)\n",
    "    if f.lower().endswith(\".txt\")\n",
    "]\n",
    "\n",
    "for filename in all_txt_files:\n",
    "    filepath = os.path.join(NPX_DOWNLOAD_DIR, filename)\n",
    "    doc_info, doc_votes = parse_npx_file(filepath)\n",
    "\n",
    "    if doc_info:\n",
    "        doc_info[\"filename\"] = filename\n",
    "        metadata_list.append(doc_info)\n",
    "\n",
    "    for vote_item in doc_votes:\n",
    "        vote_item[\"filename\"] = filename\n",
    "        votes_list.append(vote_item)\n",
    "\n",
    "df_meta = pd.DataFrame(metadata_list)\n",
    "df_votes = pd.DataFrame(votes_list)\n",
    "\n",
    "# (Optional) Merge on filename\n",
    "if not df_meta.empty and not df_votes.empty:\n",
    "    df_final = df_votes.merge(df_meta, on=\"filename\", how=\"inner\")\n",
    "else:\n",
    "    df_final = pd.DataFrame()\n",
    "\n",
    "print(f\"Metadata entries: {len(df_meta)}\")\n",
    "print(f\"Vote entries: {len(df_votes)}\")\n",
    "print(f\"Final joined entries: {len(df_final)}\")\n",
    "\n",
    "df_meta.to_csv(\"npx_simplified_metadata.csv\", index=False)\n",
    "df_votes.to_csv(\"npx_simplified_votes.csv\", index=False)\n",
    "df_final.to_csv(\"npx_simplified_final.csv\", index=False)\n",
    "\n",
    "print(\"Done! See CSV outputs for the simplified parse approach.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
